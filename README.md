# nikita-llm

Custom LLM training project scaffolded with [`@theanikrtgiri/create-llm`].

This repo contains a **fully working pipeline** for training a small GPT-like model on local machine (macOS, CPU). The goal of the project is to learn how to **create, train and serve** your own LLM, and then wrap it into an AI agent (for example, a â€œdata scientist agentâ€).

---

## 1. Project goals

- âœ… scaffold LLM project locally (no cloud required)
- âœ… train tokenizer on **own** data
- âœ… prepare dataset and run training loop
- âœ… save checkpoints and best model
- âœ… run text generation / chat on the trained model
- ðŸ›  next: wrap model into API (FastAPI) and build agent on top

---

## 2. Tech stack

- **Python** 3.12
- **PyTorch** 2.x
- **Transformers**
- **Gradio** (for chat UI)
- **create-llm** CLI (project bootstrap)
- OS: **macOS / Apple Silicon (M2)**

---

## 3. Project structure

```text
.
â”œâ”€â”€ data/             # raw and processed data
â”œâ”€â”€ tokenizer/        # tokenizer training script + tokenizer.json
â”œâ”€â”€ training/         # main training loop, callbacks, dashboard
â”œâ”€â”€ evaluation/       # generation and evaluation scripts
â”œâ”€â”€ models/           # model architectures (nano, tiny, small, base)
â”œâ”€â”€ checkpoints/      # saved models (ignored in git)
â”œâ”€â”€ logs/             # training logs (ignored in git)
â”œâ”€â”€ llm.config.js     # main config (model + training)
â””â”€â”€ README.md
```



## 4. How to run

1. Clone / open project
```
git clone https://github.com/NikitaMarshchonok/nikita-llm.git
cd nikita-llm
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

2. Add data
  ```
mkdir -p data/raw
curl https://www.gutenberg.org/files/100/100-0.txt > data/raw/shakespeare.txt
```  


3. Train tokenizer
```
python tokenizer/train.py --data data/raw/
```


4. Prepare dataset
```
python data/prepare.py
```

5. Train model
```
python training/train.py
```

6. Generate text from checkpoint
```
python evaluation/generate.py \
  --checkpoint checkpoints/checkpoint-best.pt \
  --prompt "Hello, I am Nikita and I am building my own LLM"
```


## 5. Current training run (local, CPU)

device: cpu
steps: â‰ˆ 2000 (config was 10000, but early checkpoint is enough)
time: ~1 hour on MacBook Pro (M2)
best checkpoint: checkpoints/checkpoint-best.pt
validation loss: â‰ˆ 0.08
perplexity: â‰ˆ 1.09
trainer warning: â€œPerplexity < 1.1 â†’ overfittingâ€
