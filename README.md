# nikita-llm

Custom LLM training project scaffolded with [`@theanikrtgiri/create-llm`].

This repo contains a **fully working pipeline** for training a small GPT-like model on local machine (macOS, CPU). The goal of the project is to learn how to **create, train and serve** your own LLM, and then wrap it into an AI agent (for example, a â€œdata scientist agentâ€).

---

## 1. Project goals

- âœ… scaffold LLM project locally (no cloud required)
- âœ… train tokenizer on **own** data
- âœ… prepare dataset and run training loop
- âœ… save checkpoints and best model
- âœ… run text generation / chat on the trained model
- ðŸ›  next: wrap model into API (FastAPI) and build agent on top

---

## 2. Tech stack

- **Python** 3.12
- **PyTorch** 2.x
- **Transformers**
- **Gradio** (for chat UI)
- **create-llm** CLI (project bootstrap)
- OS: **macOS / Apple Silicon (M2)**

---

## 3. Project structure

```text
.
â”œâ”€â”€ data/             # raw and processed data
â”œâ”€â”€ tokenizer/        # tokenizer training script + tokenizer.json
â”œâ”€â”€ training/         # main training loop, callbacks, dashboard
â”œâ”€â”€ evaluation/       # generation and evaluation scripts
â”œâ”€â”€ models/           # model architectures (nano, tiny, small, base)
â”œâ”€â”€ checkpoints/      # saved models (ignored in git)
â”œâ”€â”€ logs/             # training logs (ignored in git)
â”œâ”€â”€ llm.config.js     # main config (model + training)
â””â”€â”€ README.md
```



## 4. How to run

1. Clone / open project
```
git clone https://github.com/NikitaMarshchonok/nikita-llm.git
cd nikita-llm
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

2. Add data
  ```
mkdir -p data/raw
curl https://www.gutenberg.org/files/100/100-0.txt > data/raw/shakespeare.txt
```  


3. Train tokenizer
```
python tokenizer/train.py --data data/raw/
```


4. Prepare dataset
```
python data/prepare.py
```

5. Train model
```
python training/train.py
```

6. Generate text from checkpoint
```
python evaluation/generate.py \
  --checkpoint checkpoints/checkpoint-best.pt \
  --prompt "Hello, I am Nikita and I am building my own LLM"
```


## 5. Current training run (local, CPU)

device: cpu

steps: â‰ˆ 2000 (config was 10000, but early checkpoint is enough)

time: ~1 hour on MacBook Pro (M2Pro)

best checkpoint: checkpoints/checkpoint-best.pt

validation loss: â‰ˆ 0.08

perplexity: â‰ˆ 1.09

trainer warning: â€œPerplexity < 1.1 â†’ overfittingâ€



## 6. Why overfitting happened

small dataset

small model that learns fast

long training (2000+ steps)

same domain text

How to fix:

add more data (data/raw/)

increase dropout in llm.config.js from 0.2 â†’ 0.3

reduce training.max_steps (e.g. 3000 instead of 10000)



## 7. Next steps (agent plan)

âœ… train and get checkpoint

ðŸŸ¡ add simple API (FastAPI) around evaluation/generate.py

ðŸŸ¡ add â€œtoolsâ€: load CSV, describe, train simple model

ðŸŸ¡ build loop: LLM decides â†’ tool runs â†’ LLM Ð¾Ð±ÑŠÑÑÐ½ÑÐµÑ‚

ðŸŸ¡ UI (Gradio / React) â†’ â€œNikita DS Agentâ€

So the final goal: â€œmy own small LLM + agent that helps with data-analysis tasksâ€
