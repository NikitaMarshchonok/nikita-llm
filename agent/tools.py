# agent/tools.py
from __future__ import annotations

import os
import json
import uuid
import base64
from io import BytesIO
from typing import Optional, Literal

import numpy as np
import pandas as pd
import re

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import accuracy_score, f1_score, mean_squared_error
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
import joblib

import matplotlib
matplotlib.use("Agg")  # —á—Ç–æ–±—ã —Ä–µ–Ω–¥–µ—Ä–∏—Ç—å –±–µ–∑ GUI
import matplotlib.pyplot as plt


# ---------------------------------------------------------------------
# 1. –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π EDA
# ---------------------------------------------------------------------
def basic_eda(df: pd.DataFrame) -> dict:
    """
    –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π EDA:
    - —Ñ–æ—Ä–º–∞
    - —Ç–∏–ø—ã
    - –ø—Ä–æ–ø—É—Å–∫–∏ (–∫–æ–ª-–≤–æ –∏ –¥–æ–ª—è)
    - –±–∞–∑–æ–≤—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ —á–∏—Å–ª–æ–≤—ã–º
    - –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –∏ –ø–æ—á—Ç–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    - –ø–∞—Ä—ã —Å –≤—ã—Å–æ–∫–æ–π –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–µ–π
    """
    eda: dict = {
        "shape": list(df.shape),
        "dtypes": {c: str(df[c].dtype) for c in df.columns},
    }

    # –ø—Ä–æ–ø—É—Å–∫–∏
    eda["nulls"] = {c: int(df[c].isna().sum()) for c in df.columns}
    eda["null_fractions"] = {c: float(df[c].isna().mean()) for c in df.columns}

    # stats –ø–æ —á–∏—Å–ª–æ–≤—ã–º
    numeric_cols = df.select_dtypes(include=["number"]).columns.tolist()
    stats = {}
    for c in numeric_cols[:30]:
        ser = df[c]
        stats[c] = {
            "mean": float(ser.mean()),
            "std": float(ser.std() or 0),
            "min": float(ser.min()),
            "max": float(ser.max()),
        }
    eda["numeric_stats"] = stats

    # –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã / –ø–æ—á—Ç–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã
    constant_features = []
    quasi_constant_features = []
    for c in df.columns:
        uniq = df[c].nunique(dropna=True)
        if uniq <= 1:
            constant_features.append(c)
        else:
            top_frac = float(df[c].value_counts(normalize=True, dropna=False).iloc[0])
            if top_frac > 0.98:
                quasi_constant_features.append(c)
    eda["constant_features"] = constant_features
    eda["quasi_constant_features"] = quasi_constant_features

    # –≤—ã—Å–æ–∫–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è
    high_corr_pairs = []
    if len(numeric_cols) >= 2:
        corr = df[numeric_cols].corr().abs()
        for i, c1 in enumerate(numeric_cols):
            for j in range(i + 1, len(numeric_cols)):
                c2 = numeric_cols[j]
                val = float(corr.loc[c1, c2])
                if val >= 0.9:
                    high_corr_pairs.append(
                        {"feature_1": c1, "feature_2": c2, "corr": val}
                    )
    eda["high_corr_pairs"] = high_corr_pairs

    return eda


# ---------------------------------------------------------------------
# 2. —É–≥–∞–¥—ã–≤–∞–Ω–∏–µ —Ç–∞—Ä–≥–µ—Ç–∞ –∏ —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏
# ---------------------------------------------------------------------
ID_LIKE = {"id", "ID", "Id", "index", "Rk", "rank"}


def _looks_like_id(colname: str) -> bool:
    return colname in ID_LIKE or re.search(r"id$", colname, re.IGNORECASE) is not None


def _guess_target(
    df: pd.DataFrame,
) -> tuple[Literal["eda", "classification", "regression"], Optional[str]]:
    lower_cols = {c.lower(): c for c in df.columns}

    # –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ
    for cand in ("target", "label", "class", "y"):
        if cand in lower_cols:
            col = lower_cols[cand]
            if df[col].nunique() <= 50:
                return "classification", col
            else:
                return "regression", col

    # –º–∞–ª–µ–Ω—å–∫–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ
    for c in df.columns:
        if _looks_like_id(c):
            continue
        uniq = df[c].nunique(dropna=True)
        if 2 <= uniq <= 30:
            return "classification", c

    # —á–∏—Å–ª–æ–≤—ã–µ
    num_cols = df.select_dtypes(include=["number"]).columns.tolist()
    for c in num_cols:
        if _looks_like_id(c):
            continue
        if df[c].nunique() > 1:
            return "regression", c

    return "eda", None


def detect_task(df: pd.DataFrame, target: Optional[str] = None) -> dict:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ–º –∑–∞–¥–∞—á—É –∏ –∫–æ–ª–æ–Ω–∫—É-—Ç–∞—Ä–≥–µ—Ç."""
    if target is not None and target in df.columns:
        nunique = df[target].nunique()
        if df[target].dtype == "object" or nunique <= 30:
            task = "classification"
        else:
            task = "regression"
        return {"task": task, "target": target}

    task, tgt = _guess_target(df)
    return {"task": task, "target": tgt}


# ---------------------------------------------------------------------
# 3. –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ –∫ —á–∏—Å–ª–∞–º –∏ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥
# ---------------------------------------------------------------------
def _coerce_numeric(df: pd.DataFrame) -> pd.DataFrame:
    """
    –ü—Ä–æ–±—É–µ–º —Å—Ç—Ä–æ–∫–∏, –ø–æ—Ö–æ–∂–∏–µ –Ω–∞ —á–∏—Å–ª–∞, –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ —á–∏—Å–ª–∞–º.
    –ë–µ–∑ FutureWarning.
    """
    new_df = df.copy()
    for col in new_df.columns:
        if new_df[col].dtype == "object":
            ser = new_df[col].astype(str).str.replace(",", "").str.replace(" ", "")
            try:
                converted = pd.to_numeric(ser)
            except Exception:
                continue
            else:
                if converted.dtype != "object":
                    new_df[col] = converted
    return new_df


def build_preprocessor(X: pd.DataFrame) -> ColumnTransformer:
    numeric_features = X.select_dtypes(
        include=["int64", "float64", "int32", "float32"]
    ).columns.tolist()
    categorical_features = [c for c in X.columns if c not in numeric_features]

    numeric_transformer = Pipeline(
        steps=[("imputer", SimpleImputer(strategy="median"))]
    )

    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore")),
        ]
    )

    return ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ]
    )


# ---------------------------------------------------------------------
# 4. –û–±—É—á–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ
# ---------------------------------------------------------------------
def train_baseline(
    df: pd.DataFrame,
    target: str,
    task: str,
    return_model: bool = False,
) -> Optional[dict]:
    """
    –û–±—É—á–∞–µ–º –æ—á–µ–Ω—å –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å.
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º dict —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏.
    –ï—Å–ª–∏ —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫ ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º None, —á—Ç–æ–±—ã /upload –Ω–µ –ø–∞–¥–∞–ª.
    """
    try:
        if target not in df.columns:
            return None

        df = _coerce_numeric(df)

        # –≤—ã–±—Ä–æ—Å–∏–º —Å—Ç—Ä–æ–∫–∏ –±–µ–∑ —Ç–∞—Ä–≥–µ—Ç–∞
        df = df[~df[target].isna()].copy()
        if df.shape[0] < 20:
            return None

        y = df[target]
        X = df.drop(columns=[target])

        if X.shape[1] == 0:
            return None

        preprocessor = build_preprocessor(X)

        # –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø
        if task == "classification":
            if y.nunique() < 2:
                return None

            model = RandomForestClassifier(
                n_estimators=200, random_state=42, n_jobs=-1
            )

            counts = y.value_counts(dropna=False)
            can_stratify = (counts >= 2).all()

            X_train, X_val, y_train, y_val = train_test_split(
                X,
                y,
                test_size=0.2,
                random_state=42,
                stratify=y if (y.nunique() < 50 and can_stratify) else None,
            )

            pipe = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
            pipe.fit(X_train, y_train)
            preds = pipe.predict(X_val)

            acc = float(accuracy_score(y_val, preds))
            f1 = float(f1_score(y_val, preds, average="weighted"))

            res = {
                "model_type": "RandomForestClassifier",
                "accuracy": acc,
                "f1": f1,
            }
            if return_model:
                res["pipeline"] = pipe
            return res

        # –†–ï–ì–†–ï–°–°–ò–Ø
        elif task == "regression":
            model = RandomForestRegressor(
                n_estimators=200, random_state=42, n_jobs=-1
            )
            X_train, X_val, y_train, y_val = train_test_split(
                X, y, test_size=0.2, random_state=42
            )
            pipe = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
            pipe.fit(X_train, y_train)
            preds = pipe.predict(X_val)

            mse = float(mean_squared_error(y_val, preds))
            rmse = mse ** 0.5

            res = {
                "model_type": "RandomForestRegressor",
                "rmse": rmse,
            }
            if return_model:
                res["pipeline"] = pipe
            return res

        # –µ—Å–ª–∏ –∑–∞–¥–∞—á–∞ —Ç–æ–ª—å–∫–æ EDA
        return None

    except Exception:
        return None


# ---------------------------------------------------------------------
# 5. –û—Ç—á—ë—Ç —Ç–µ–∫—Å—Ç–æ–º
# ---------------------------------------------------------------------
def build_report(df: pd.DataFrame, eda: dict, task: dict, model: dict | None) -> str:
    rows, cols = eda["shape"]
    lines: list[str] = []

    lines.append(f"üìä –í –¥–∞—Ç–∞—Å–µ—Ç–µ {rows} —Å—Ç—Ä–æ–∫ –∏ {cols} –∫–æ–ª–æ–Ω–æ–∫.")

    # –ø—Ä–æ–ø—É—Å–∫–∏
    nulls = eda.get("nulls", {})
    top_nulls = {k: v for k, v in nulls.items() if v > 0}
    if top_nulls:
        lines.append("üï≥Ô∏è –ü—Ä–æ–ø—É—Å–∫–∏ (—Ç–æ–ø):")
        for k, v in list(top_nulls.items())[:10]:
            lines.append(f"  ‚Ä¢ {k}: {v}")

    # —á–∏—Å–ª–æ–≤—ã–µ
    num_stats = eda.get("numeric_stats", {})
    if num_stats:
        lines.append("üìê –ß–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (mean / std / min / max):")
        for name, st in list(num_stats.items())[:10]:
            lines.append(
                f"  ‚Ä¢ {name}: {st['mean']:.3f}/{st['std']:.3f}/{st['min']}/{st['max']}"
            )

    # –∑–∞–¥–∞—á–∞
    if task["task"] == "eda" or task["target"] is None:
        lines.append("üß† –ü–æ–¥—Ö–æ–¥—è—â–µ–π —Ü–µ–ª–µ–≤–æ–π –∫–æ–ª–æ–Ω–∫–∏ –Ω–µ –Ω–∞—à–ª–æ—Å—å ‚Äî —Å–¥–µ–ª–∞–Ω —Ç–æ–ª—å–∫–æ EDA.")
    else:
        lines.append(f'üß† –ó–∞–¥–∞—á–∞: {task["task"]} –ø–æ –∫–æ–ª–æ–Ω–∫–µ "{task["target"]}".')

    # –º–æ–¥–µ–ª—å
    if model:
        if "accuracy" in model:
            lines.append(
                f'üß™ –ú–æ–¥–µ–ª—å: {model["model_type"]}, accuracy={model["accuracy"]:.3f}, f1={model["f1"]:.3f}'
            )
        elif "rmse" in model:
            lines.append(
                f'üß™ –ú–æ–¥–µ–ª—å: {model["model_type"]}, RMSE={model["rmse"]:.3f}'
            )
    else:
        lines.append("üì¶ –ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–∞–ª–∞—Å—å.")

    return "\n".join(lines)


# ---------------------------------------------------------------------
# 6. –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–±–ª–µ–º (–¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π)
# ---------------------------------------------------------------------
def analyze_dataset(df: pd.DataFrame, eda: dict, task: dict) -> dict:
    """
    –°–æ–±–∏—Ä–∞–µ–º –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫—É—é –∏–Ω—Ñ—É:
    - –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã / –∫–≤–∞–∑–∏-–∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã
    - –≤—ã—Å–æ–∫–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è
    - –º–Ω–æ–≥–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤
    - NaN –≤ —Ç–∞—Ä–≥–µ—Ç–µ
    - –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤
    - –≤—ã—Å–æ–∫–∞—è –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å
    """
    problems: dict[str, object] = {}

    # 1) –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã –∏ –ø–æ—á—Ç–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã
    constant_cols = []
    quasi_constant_cols = []
    for col in df.columns:
        nunique = df[col].nunique(dropna=True)
        if nunique <= 1:
            constant_cols.append(col)
        else:
            top_frac = df[col].value_counts(normalize=True, dropna=False).iloc[0]
            if top_frac > 0.98:
                quasi_constant_cols.append(col)

    if constant_cols:
        problems["constant_features"] = constant_cols
    if quasi_constant_cols:
        problems["quasi_constant_features"] = quasi_constant_cols

    # 2) –≤—ã—Å–æ–∫–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è
    num_df = df.select_dtypes(include=["number"])
    high_corr_pairs = []
    if num_df.shape[1] >= 2:
        corr = num_df.corr().abs()
        cols = corr.columns.tolist()
        for i in range(len(cols)):
            for j in range(i + 1, len(cols)):
                cval = float(corr.iloc[i, j])
                if cval >= 0.9:
                    high_corr_pairs.append((cols[i], cols[j], cval))
    if high_corr_pairs:
        problems["high_corr_pairs"] = high_corr_pairs

    # 3) –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –±–æ–ª—å—à–∏–º % –ø—Ä–æ–ø—É—Å–∫–æ–≤
    null_perc = (df.isna().sum() / len(df) * 100)
    high_nulls = {col: float(round(p, 1)) for col, p in null_perc.items() if p >= 30.0}
    if high_nulls:
        problems["high_null_features"] = high_nulls

    # 4) NaN –≤ —Ç–∞—Ä–≥–µ—Ç–µ
    target = task.get("target")
    if target and target in df.columns:
        nan_cnt = int(df[target].isna().sum())
        if nan_cnt > 0:
            problems["target_has_nan"] = {
                "column": target,
                "nan_count": nan_cnt,
                "share": float(round(nan_cnt / len(df) * 100, 1)),
            }

    # 5) –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤
    if task.get("task") == "classification" and target and target in df.columns:
        vc = df[target].value_counts(dropna=False)
        if len(vc) >= 2:
            max_c = int(vc.iloc[0])
            min_c = int(vc.iloc[-1])
            ratio = max_c / max(1, min_c)
            if ratio >= 5:
                problems["class_imbalance"] = {
                    "max_class": vc.index[0],
                    "max_count": max_c,
                    "min_class": vc.index[-1],
                    "min_count": min_c,
                    "ratio": float(round(ratio, 1)),
                }

    # 6) –≤—ã—Å–æ–∫–∞—è –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö
    high_cardinality = []
    for col in df.select_dtypes(include=["object"]).columns:
        nunique = df[col].nunique(dropna=True)
        if nunique > 200:
            high_cardinality.append(
                {"column": col, "n_unique": int(nunique)}
            )
    if high_cardinality:
        problems["high_cardinality"] = high_cardinality

    return problems


# ---------------------------------------------------------------------
# 7. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–±–ª–µ–º
# ---------------------------------------------------------------------
def build_recommendations(
    eda: dict,
    task: dict,
    model: dict | None,
    problems: dict,
) -> list[str]:
    """
    –ù–∞ –æ—Å–Ω–æ–≤–µ EDA/–∑–∞–¥–∞—á–∏/–º–æ–¥–µ–ª–∏/–ø—Ä–æ–±–ª–µ–º –≤—ã–¥–∞—ë–º —Å–ø–∏—Å–æ–∫ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π.
    –í–ê–ñ–ù–û: –∫–æ–ª–æ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –∫–∞–∫ ID/–∫–ª—é—á–∏, —Å—Ç–∞—Ä–∞–µ–º—Å—è –Ω–µ —É–ø–æ–º–∏–Ω–∞—Ç—å.
    """
    recs: list[str] = []

    id_like = set(problems.get("id_like", []))

    # –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã
    consts = [c for c in (problems.get("constant_features") or []) if c not in id_like]
    if consts:
        recs.append(
            f"–ï—Å—Ç—å –ø–æ–ª–Ω–æ—Å—Ç—å—é –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {', '.join(consts[:8])} ‚Äî –º–æ–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å –ø–µ—Ä–µ–¥ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ–º."
        )

    quasi = [c for c in (problems.get("quasi_constant_features") or []) if c not in id_like]
    if quasi:
        recs.append(
            f"–ï—Å—Ç—å –ø–æ—á—Ç–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {', '.join(quasi[:8])} ‚Äî —Å—Ç–æ–∏—Ç –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∏—Ö –ø–æ–ª—å–∑—É."
        )

    # –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
    corr_pairs = problems.get("high_corr_pairs") or []
    if corr_pairs:
        short = []
        for a, b, corr in corr_pairs[:6]:
            # –µ—Å–ª–∏ –æ–±–µ ‚Äî ID, —Ç–æ –Ω–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º
            if a in id_like and b in id_like:
                continue
            short.append(f"{a}‚Üî{b} ({corr:.2f})")
        if short:
            recs.append(
                "–ï—Å—Ç—å —Å–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—â–∏–µ –ø–∞—Ä—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: "
                + ", ".join(short)
                + " ‚Äî –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –æ—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–ª–∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é."
            )

    # –ø—Ä–æ–ø—É—Å–∫–∏
    high_nulls = problems.get("high_null_features") or {}
    if high_nulls:
        show = [f"{k} ({v:.1f}%)" for k, v in list(high_nulls.items())[:6] if k not in id_like]
        if show:
            recs.append(
                "–ï—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –±–æ–ª—å—à–∏–º —á–∏—Å–ª–æ–º –ø—Ä–æ–ø—É—Å–∫–æ–≤: "
                + ", ".join(show)
                + " ‚Äî –∑–∞–ø–æ–ª–Ω–∏/—É–¥–∞–ª–∏/—Å–¥–µ–ª–∞–π –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–ª–∞–≥."
            )

    # NaN –≤ —Ç–∞—Ä–≥–µ—Ç–µ
    if problems.get("target_has_nan"):
        info = problems["target_has_nan"]
        recs.append(
            f"–í —Ü–µ–ª–µ–≤–æ–π –∫–æ–ª–æ–Ω–∫–µ {info['column']} –µ—Å—Ç—å –ø—Ä–æ–ø—É—Å–∫–∏ ({info['nan_count']}) ‚Äî –Ω—É–∂–Ω–æ —É–±—Ä–∞—Ç—å –∏—Ö –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º."
        )

    # –¥–∏—Å–±–∞–ª–∞–Ω—Å
    if problems.get("class_imbalance"):
        ci = problems["class_imbalance"]
        recs.append(
            f"–ù–∞–π–¥–µ–Ω –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ ({ci['max_class']}:{ci['min_class']} ‚âà {ci['ratio']:.1f}). "
            "–ò—Å–ø–æ–ª—å–∑—É–π class_weight='balanced', stratify –ø—Ä–∏ train_test_split –∏–ª–∏ oversampling."
        )

    # –≤—ã—Å–æ–∫–∞—è –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å (–Ω–æ –Ω–µ –¥–ª—è ID)
    high_card = problems.get("high_cardinality") or []
    if high_card:
        cols = [f"{x['column']} ({x['n_unique']})" for x in high_card[:4] if x["column"] not in id_like]
        if cols:
            recs.append(
                "–ï—Å—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –±–æ–ª—å—à–∏–º —á–∏—Å–ª–æ–º –∑–Ω–∞—á–µ–Ω–∏–π: "
                + ", ".join(cols)
                + " ‚Äî –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å CatBoost/target encoding/—á–∞—Å—Ç–æ—Ç–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ."
            )

    # –ø–æ –∑–∞–¥–∞—á–µ
    if task.get("task") == "eda":
        recs.append("–¶–µ–ª–µ–≤–æ–π –ø—Ä–∏–∑–Ω–∞–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω ‚Äî –º–æ–∂–Ω–æ —è–≤–Ω–æ —É–∫–∞–∑–∞—Ç—å target –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ.")
    elif task.get("task") == "regression":
        recs.append("–î–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –º–æ–∂–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –±–æ–ª–µ–µ —Å–∏–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ (CatBoostRegressor, LightGBM).")
    elif task.get("task") == "classification":
        recs.append("–î–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–º–µ–µ—Ç —Å–º—ã—Å–ª –ø–æ—Å—á–∏—Ç–∞—Ç—å ROC-AUC –∏ PR-AUC, –æ—Å–æ–±–µ–Ω–Ω–æ –ø—Ä–∏ –¥–∏—Å–±–∞–ª–∞–Ω—Å–µ.")

    # –ø–æ –º–æ–¥–µ–ª–∏
    if model is None:
        recs.append("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–∞–ª–∞—Å—å ‚Äî —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ, –Ω–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ target –∏–ª–∏ –¥–∞–Ω–Ω—ã—Ö —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ.")
    else:
        if model.get("model_type") == "RandomForestClassifier":
            recs.append("–¢–µ–∫—É—â–∞—è –º–æ–¥–µ–ª—å ‚Äî RandomForestClassifier. –ú–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å –±—É—Å—Ç–∏–Ω–≥–æ–º –∏ –ø–æ–¥–±–æ—Ä–æ–º –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.")
        if model.get("model_type") == "RandomForestRegressor":
            recs.append("–¢–µ–∫—É—â–∞—è –º–æ–¥–µ–ª—å ‚Äî RandomForestRegressor. –ú–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å CatBoost/LightGBM.")

    return recs


def analyze_dataset(df: pd.DataFrame, task: dict) -> dict:
    """
    –°–∏–≥–Ω–∞–ª—ã –ø–æ –¥–∞—Ç–∞—Å–µ—Ç—É: –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã, –∫–≤–∞–∑–∏–∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã, –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏,
    –º–Ω–æ–≥–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤, –¥–∏—Å–±–∞–ª–∞–Ω—Å, –≤—ã—Å–æ–∫–∞—è –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å, ID-–∫–æ–ª–æ–Ω–∫–∏.
    """
    problems: dict[str, object] = {}

    # 0) –¥–µ—Ç–µ–∫—Ç ID / –∫–ª—é—á–µ–π
    id_like_cols: list[str] = []
    n_rows = len(df)
    for col in df.columns:
        col_l = col.lower()
        nunique = df[col].nunique(dropna=True)

        name_looks_like_id = (
            col_l == "id"
            or col_l.endswith("_id")
            or col_l in ("index", "idx", "rk", "rank")
        )
        value_looks_like_id = nunique > 0.9 * n_rows  # –ø–æ—á—Ç–∏ —É–Ω–∏–∫–∞–ª—å–Ω–æ

        if name_looks_like_id or value_looks_like_id:
            id_like_cols.append(col)

    if id_like_cols:
        problems["id_like"] = id_like_cols

    # 1) –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã –∏ –ø–æ—á—Ç–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã
    constant_cols = []
    quasi_constant_cols = []
    for col in df.columns:
        nunique = df[col].nunique(dropna=True)
        if nunique <= 1:
            constant_cols.append(col)
        else:
            top_frac = float(df[col].value_counts(normalize=True, dropna=False).iloc[0])
            if top_frac > 0.98:
                quasi_constant_cols.append(col)
    if constant_cols:
        problems["constant_features"] = constant_cols
    if quasi_constant_cols:
        problems["quasi_constant_features"] = quasi_constant_cols

    # 2) –≤—ã—Å–æ–∫–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ø–æ —á–∏—Å–ª–æ–≤—ã–º
    num_df = df.select_dtypes(include=["number"])
    high_corr_pairs = []
    if num_df.shape[1] >= 2:
        corr = num_df.corr().abs()
        cols = corr.columns.tolist()
        for i in range(len(cols)):
            for j in range(i + 1, len(cols)):
                if corr.iloc[i, j] >= 0.9:
                    high_corr_pairs.append((cols[i], cols[j], float(corr.iloc[i, j])))
    if high_corr_pairs:
        problems["high_corr_pairs"] = high_corr_pairs

    # 3) –ø—Ä–æ–ø—É—Å–∫–∏ (–≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö)
    null_perc = (df.isna().sum() / max(1, n_rows) * 100).sort_values(ascending=False)
    high_nulls = null_perc[null_perc > 30].to_dict()
    if high_nulls:
        problems["high_null_features"] = {k: float(v) for k, v in high_nulls.items()}

    # 4) NaN –≤ —Ç–∞—Ä–≥–µ—Ç–µ
    target = task.get("target")
    if target and target in df.columns:
        n_nan_target = int(df[target].isna().sum())
        if n_nan_target > 0:
            problems["target_has_nan"] = {"column": target, "nan_count": n_nan_target}

    # 5) –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤
    if task.get("task") == "classification" and target and target in df.columns:
        vc = df[target].value_counts(dropna=False)
        if len(vc) >= 2:
            max_c = int(vc.iloc[0])
            min_c = int(vc.iloc[-1])
            ratio = max_c / max(1, min_c)
            if ratio >= 5:
                problems["class_imbalance"] = {
                    "max_class": vc.index[0],
                    "max_count": max_c,
                    "min_class": vc.index[-1],
                    "min_count": min_c,
                    "ratio": float(ratio),
                }

    # 6) –æ—á–µ–Ω—å –º–Ω–æ–≥–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    high_cardinality = []
    for col in df.select_dtypes(include=["object"]).columns:
        nunique = df[col].nunique(dropna=True)
        if nunique > 200 and col not in problems.get("id_like", []):
            high_cardinality.append({"column": col, "n_unique": int(nunique)})
    if high_cardinality:
        problems["high_cardinality"] = high_cardinality

    return problems

# ---------------------------------------------------------------------
# 8. –ì—Ä–∞—Ñ–∏–∫–∏ ‚Üí base64
# ---------------------------------------------------------------------
def make_plots_base64(df: pd.DataFrame) -> list[dict]:
    plots: list[dict] = []

    num_cols = df.select_dtypes(include=["number"]).columns.tolist()[:3]
    for col in num_cols:
        fig, ax = plt.subplots(figsize=(4, 3))
        ax.hist(df[col].dropna(), bins=30)
        ax.set_title(f"Distribution of {col}")
        buf = BytesIO()
        plt.tight_layout()
        fig.savefig(buf, format="png")
        plt.close(fig)
        buf.seek(0)
        b64 = base64.b64encode(buf.read()).decode("utf-8")
        plots.append({"name": f"hist_{col}", "image_base64": b64})

    if len(df.select_dtypes(include=["number"]).columns) >= 2:
        corr = df.select_dtypes(include=["number"]).corr()
        fig, ax = plt.subplots(figsize=(4, 3))
        cax = ax.imshow(corr, cmap="viridis")
        ax.set_xticks(range(len(corr.columns)))
        ax.set_yticks(range(len(corr.columns)))
        ax.set_xticklabels(corr.columns, rotation=90, fontsize=6)
        ax.set_yticklabels(corr.columns, fontsize=6)
        fig.colorbar(cax)
        plt.tight_layout()
        buf = BytesIO()
        fig.savefig(buf, format="png")
        plt.close(fig)
        buf.seek(0)
        b64 = base64.b64encode(buf.read()).decode("utf-8")
        plots.append({"name": "correlation", "image_base64": b64})

    return plots


# ---------------------------------------------------------------------
# 9. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–∞–Ω-–∞ –Ω–∞ –¥–∏—Å–∫ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
# ---------------------------------------------------------------------
def save_run(run_data: dict, model_pipeline) -> str:
    run_id = str(uuid.uuid4())
    run_dir = os.path.join("runs", run_id)
    os.makedirs(run_dir, exist_ok=True)

    with open(os.path.join(run_dir, "report.json"), "w", encoding="utf-8") as f:
        json.dump(run_data, f, ensure_ascii=False, indent=2)

    if model_pipeline is not None:
        joblib.dump(model_pipeline, os.path.join(run_dir, "model.joblib"))

    return run_id
