# agent/tools.py
from __future__ import annotations

import os
import re
import json
import uuid
import base64
from io import BytesIO
from typing import Optional, Literal
import numpy as np
import pandas as pd
import re
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import accuracy_score, f1_score, mean_squared_error
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
import joblib

import matplotlib
matplotlib.use("Agg")  # —á—Ç–æ–±—ã —Ä–µ–Ω–¥–µ—Ä–∏—Ç—å –±–µ–∑ GUI
import matplotlib.pyplot as plt


# ---------------------------------------------------------------------
# 1. EDA (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π)
# ---------------------------------------------------------------------
def basic_eda(df: pd.DataFrame) -> dict:
    """
    –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π EDA:
    - —Ñ–æ—Ä–º–∞
    - —Ç–∏–ø—ã
    - –ø—Ä–æ–ø—É—Å–∫–∏ (–∫–æ–ª-–≤–æ –∏ –¥–æ–ª—è)
    - –±–∞–∑–æ–≤—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ —á–∏—Å–ª–æ–≤—ã–º
    - –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –∏ –ø–æ—á—Ç–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    - –ø–∞—Ä—ã —Å –≤—ã—Å–æ–∫–æ–π –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–µ–π
    """
    eda: dict = {
        "shape": list(df.shape),
        "dtypes": {c: str(df[c].dtype) for c in df.columns},
    }

    # –ø—Ä–æ–ø—É—Å–∫–∏
    null_counts = {c: int(df[c].isna().sum()) for c in df.columns}
    null_frac = {
        c: float(df[c].isna().mean()) for c in df.columns
    }
    eda["nulls"] = null_counts
    eda["null_fractions"] = null_frac

    # –±–∞–∑–æ–≤—ã–µ stats
    numeric_cols = df.select_dtypes(include=["number"]).columns.tolist()
    stats = {}
    for c in numeric_cols[:30]:
        ser = df[c]
        stats[c] = {
            "mean": float(ser.mean()),
            "std": float(ser.std() or 0),
            "min": float(ser.min()),
            "max": float(ser.max()),
        }
    eda["numeric_stats"] = stats

    # –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ / –ø–æ—á—Ç–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ
    constant_features = []
    quasi_constant_features = []
    for c in df.columns:
        uniq = df[c].nunique(dropna=True)
        if uniq <= 1:
            constant_features.append(c)
        else:
            # –¥–æ–ª—è —Å–∞–º–æ–≥–æ —á–∞—Å—Ç–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è
            top_frac = float(df[c].value_counts(normalize=True, dropna=False).iloc[0])
            if top_frac > 0.98:
                quasi_constant_features.append(c)
    eda["constant_features"] = constant_features
    eda["quasi_constant_features"] = quasi_constant_features

    # –ø–∞—Ä—ã —Å –≤—ã—Å–æ–∫–æ–π –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–µ–π (—Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ)
    high_corr_pairs = []
    if len(numeric_cols) >= 2:
        corr = df[numeric_cols].corr().abs()
        # —Ç–æ–ª—å–∫–æ –≤–µ—Ä—Ö–Ω–∏–π —Ç—Ä–µ—É–≥–æ–ª—å–Ω–∏–∫
        for i, c1 in enumerate(numeric_cols):
            for j in range(i + 1, len(numeric_cols)):
                c2 = numeric_cols[j]
                val = float(corr.loc[c1, c2])
                if val >= 0.9:
                    high_corr_pairs.append(
                        {"feature_1": c1, "feature_2": c2, "corr": val}
                    )
    eda["high_corr_pairs"] = high_corr_pairs

    return eda


def build_recommendations(
    eda: dict,
    task: dict,
    model: dict | None = None,
) -> list[str]:
    """
    –ù–∞ –æ—Å–Ω–æ–≤–µ EDA/–∑–∞–¥–∞—á–∏/–º–æ–¥–µ–ª–∏ –≤—ã–¥–∞—ë–º —Å–ø–∏—Å–æ–∫ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π.
    –≠—Ç–æ —Ç–æ, —á—Ç–æ –º—ã –ø–æ–∫–∞–∂–µ–º –≤ UI —Å–ø—Ä–∞–≤–∞.
    """
    recs: list[str] = []

    # –ø—Ä–æ–ø—É—Å–∫–∏
    null_fracs = eda.get("null_fractions", {})
    big_nulls = [c for c, f in null_fracs.items() if f > 0.3]
    if big_nulls:
        recs.append(
            f"–í—ã—Å–æ–∫–∞—è –¥–æ–ª—è –ø—Ä–æ–ø—É—Å–∫–æ–≤ –≤ –∫–æ–ª–æ–Ω–∫–∞—Ö: {', '.join(big_nulls)} ‚Äî —Å—Ç–æ–∏—Ç –ª–∏–±–æ –∏–ºPUT–∏—Ä–æ–≤–∞—Ç—å, –ª–∏–±–æ —É–¥–∞–ª–∏—Ç—å."
        )

    # –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã
    consts = eda.get("constant_features", [])
    if consts:
        recs.append(
            f"–ö–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {', '.join(consts)} ‚Äî –∏—Ö –º–æ–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞."
        )

    quasi = eda.get("quasi_constant_features", [])
    if quasi:
        recs.append(
            f"–ü–æ—á—Ç–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {', '.join(quasi)} ‚Äî —Å—Ç–æ–∏—Ç –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –ª–∏ –æ–Ω–∏ –ø–æ–ª–µ–∑–Ω—ã."
        )

    # –≤—ã—Å–æ–∫–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è
    high_corr = eda.get("high_corr_pairs", [])
    if high_corr:
        top_pairs = ", ".join([f"{p['feature_1']}/{p['feature_2']} ({p['corr']:.2f})" for p in high_corr[:5]])
        recs.append(
            f"–ï—Å—Ç—å —Å–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—â–∏–µ –ø–∞—Ä—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {top_pairs} ‚Äî –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –æ—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–ª–∏ regularization."
        )

    # –ø—Ä–æ –∑–∞–¥–∞—á—É
    if task.get("task") == "classification" and task.get("target"):
        # –µ—Å–ª–∏ –º—ã –∑–Ω–∞–µ–º —Ç–∞—Ä–≥–µ—Ç ‚Äî –º–æ–∂–Ω–æ –ø–æ—Å—á–∏—Ç–∞—Ç—å –¥–∏—Å–±–∞–ª–∞–Ω—Å
        # (—Ç—É—Ç –ª—É—á—à–µ —Å—á–∏—Ç–∞—Ç—å –≤ api, –≥–¥–µ –µ—Å—Ç—å —Å–∞–º df, –Ω–æ —Å–¥–µ–ª–∞–µ–º –ø—Ä–æ—Å—Ç–æ–µ –ø—Ä–∞–≤–∏–ª–æ)
        recs.append(
            "–î–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å—Ç–æ–∏—Ç –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ –∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å class_weight/oversampling."
        )

    if model:
        if "accuracy" in model and model["accuracy"] < 0.7:
            recs.append("–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∏–∂–µ 0.7 ‚Äî –ø–æ–ø—Ä–æ–±—É–π –±–æ–ª–µ–µ –º–æ—â–Ω—É—é –º–æ–¥–µ–ª—å –∏–ª–∏ –ª—É—á—à–µ–µ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏—Ä–æ–≤–∞–Ω–∏–µ.")
        if "rmse" in model:
            recs.append("–î–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –º–æ–∂–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—Ä–æ–≤–∞—Ç—å —Ç–∞—Ä–≥–µ—Ç, –µ—Å–ª–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å —Ö–≤–æ—Å—Ç–æ–º.")

    if not recs:
        recs.append("–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –≤—ã–≥–ª—è–¥–∏—Ç –æ–∫, –º–æ–∂–Ω–æ –¥–≤–∏–≥–∞—Ç—å—Å—è –∫ —Ñ–∏—á–µ-–∏–Ω–∂–∏–Ω–∏—Ä–∏–Ω–≥—É –∏ –º–æ–¥–µ–ª–∏.")

    return recs
# ---------------------------------------------------------------------
# 2. —É–≥–∞–¥—ã–≤–∞–Ω–∏–µ —Ç–∞—Ä–≥–µ—Ç–∞ –∏ –∑–∞–¥–∞—á–∏
# ---------------------------------------------------------------------
ID_LIKE = {"id", "ID", "Id", "index", "Rk", "rank"}


def _looks_like_id(colname: str) -> bool:
    return colname in ID_LIKE or re.search(r"id$", colname, re.IGNORECASE) is not None


def _guess_target(df: pd.DataFrame) -> tuple[Literal["eda", "classification", "regression"], Optional[str]]:
    lower_cols = {c.lower(): c for c in df.columns}

    # –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è
    for cand in ("target", "label", "class", "y"):
        if cand in lower_cols:
            col = lower_cols[cand]
            if df[col].nunique() <= 50:
                return "classification", col
            else:
                return "regression", col

    # –º–∞–ª–µ–Ω—å–∫–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ
    for c in df.columns:
        if _looks_like_id(c):
            continue
        uniq = df[c].nunique(dropna=True)
        if 2 <= uniq <= 30:
            return "classification", c

    # —á–∏—Å–ª–æ–≤—ã–µ
    num_cols = df.select_dtypes(include=["number"]).columns.tolist()
    for c in num_cols:
        if _looks_like_id(c):
            continue
        if df[c].nunique() > 1:
            return "regression", c

    return "eda", None


def detect_task(df: pd.DataFrame, target: Optional[str] = None) -> dict:
    if target is not None and target in df.columns:
        nunique = df[target].nunique()
        if df[target].dtype == "object" or nunique <= 30:
            task = "classification"
        else:
            task = "regression"
        return {"task": task, "target": target}

    task, tgt = _guess_target(df)
    return {"task": task, "target": tgt}


# ---------------------------------------------------------------------
# 3. –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ –∫ —á–∏—Å–ª–∞–º –∏ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥
# ---------------------------------------------------------------------
def _coerce_numeric(df: pd.DataFrame) -> pd.DataFrame:
    """
    –ü—Ä–æ–±—É–µ–º —Å—Ç—Ä–æ–∫–∏, –ø–æ—Ö–æ–∂–∏–µ –Ω–∞ —á–∏—Å–ª–∞, –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ —á–∏—Å–ª–∞–º.
    –ë–µ–∑ warnings.
    """
    new_df = df.copy()
    for col in new_df.columns:
        if new_df[col].dtype == "object":
            # —Å–Ω–∞—á–∞–ª–∞ —á–∏—Å—Ç–∏–º —Å—Ç—Ä–æ–∫—É
            ser = new_df[col].astype(str).str.replace(",", "").str.replace(" ", "")
            try:
                converted = pd.to_numeric(ser)
            except Exception:
                # –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å ‚Äî –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –±—ã–ª–æ
                continue
            else:
                # –ø–æ–ª—É—á–∏–ª–æ—Å—å ‚Äî –ø–æ–¥–º–µ–Ω—è–µ–º –∫–æ–ª–æ–Ω–∫—É
                new_df[col] = converted
    return new_df



def build_preprocessor(X: pd.DataFrame) -> ColumnTransformer:
    numeric_features = X.select_dtypes(include=["int64", "float64", "int32", "float32"]).columns.tolist()
    categorical_features = [c for c in X.columns if c not in numeric_features]

    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
        ]
    )

    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore")),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ]
    )
    return preprocessor


# ---------------------------------------------------------------------
# 4. –æ–±—É—á–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (–±–µ–∑ –ø–∞–¥–µ–Ω–∏–π)
# ---------------------------------------------------------------------
from typing import Optional

def train_baseline(
    df: pd.DataFrame,
    target: str,
    task: str,
    return_model: bool = False,
) -> Optional[dict]:
    """
    –û–±—É—á–∞–µ–º –æ—á–µ–Ω—å –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å.
    –í–ê–ñ–ù–û: —Ç—É—Ç —Å—Ç–∞—Ä–∞–µ–º—Å—è –ù–ò–ö–û–ì–î–ê –Ω–µ –∫–∏–¥–∞—Ç—å –∏—Å–∫–ª—é—á–µ–Ω–∏—è, —á—Ç–æ–±—ã /upload –Ω–µ –ø–∞–¥–∞–ª.
    –ï—Å–ª–∏ —á—Ç–æ-—Ç–æ –Ω–µ —Ç–∞–∫ —Å –¥–∞–Ω–Ω—ã–º–∏ ‚Äî –ø—Ä–æ—Å—Ç–æ –≤–µ—Ä–Ω—ë–º None.
    """
    try:
        if target not in df.columns:
            return None

        # –ø—Ä–∏–≤–µ–¥—ë–º —Å—Ç—Ä–æ–∫–∏-–ø–æ—Ö–æ–∂–∏–µ-–Ω–∞-—á–∏—Å–ª–∞
        df = _coerce_numeric(df)

        # –≤—ã–∫–∏–Ω–µ–º —Å—Ç—Ä–æ–∫–∏, –≥–¥–µ –Ω–µ—Ç —Ç–∞—Ä–≥–µ—Ç–∞
        df = df[~df[target].isna()].copy()
        if df.shape[0] < 20:  # —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö
            return None

        y = df[target]
        X = df.drop(columns=[target])

        if X.shape[1] == 0:
            return None

        preprocessor = build_preprocessor(X)

        # –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø
        if task == "classification":
            # –µ—Å–ª–∏ –≤—Å–µ–≥–æ 1 –∫–ª–∞—Å—Å ‚Äî –Ω–µ—á–µ–≥–æ —É—á–∏—Ç—å
            if y.nunique() < 2:
                return None

            model = RandomForestClassifier(
                n_estimators=200,
                random_state=42,
                n_jobs=-1,
            )

            # –º–æ–∂–Ω–æ –ª–∏ —Å—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å
            counts = y.value_counts(dropna=False)
            can_stratify = (counts >= 2).all()

            X_train, X_val, y_train, y_val = train_test_split(
                X,
                y,
                test_size=0.2,
                random_state=42,
                stratify=y if (y.nunique() < 50 and can_stratify) else None,
            )

            pipe = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
            pipe.fit(X_train, y_train)
            preds = pipe.predict(X_val)

            acc = float(accuracy_score(y_val, preds))
            f1 = float(f1_score(y_val, preds, average="weighted"))

            res = {
                "model_type": "RandomForestClassifier",
                "accuracy": acc,
                "f1": f1,
            }
            if return_model:
                res["pipeline"] = pipe
            return res

        # –†–ï–ì–†–ï–°–°–ò–Ø
        elif task == "regression":
            model = RandomForestRegressor(
                n_estimators=200,
                random_state=42,
                n_jobs=-1,
            )

            X_train, X_val, y_train, y_val = train_test_split(
                X, y, test_size=0.2, random_state=42
            )

            pipe = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
            pipe.fit(X_train, y_train)
            preds = pipe.predict(X_val)

            mse = float(mean_squared_error(y_val, preds))
            rmse = mse ** 0.5

            res = {
                "model_type": "RandomForestRegressor",
                "rmse": rmse,
            }
            if return_model:
                res["pipeline"] = pipe
            return res

        # –µ—Å–ª–∏ –∑–∞–¥–∞—á–∞ "eda" ‚Äî –Ω–µ —É—á–∏–º
        else:
            return None

    except Exception:
        # –≤–æ–æ–±—â–µ –Ω–∞ –≤—Å—ë ‚Äî —Ç–∏—à–∏–Ω–∞, –ø—Ä–æ—Å—Ç–æ –±–µ–∑ –º–æ–¥–µ–ª–∏
        return None

# ---------------------------------------------------------------------
# 5. –æ—Ç—á—ë—Ç –≤ –≤–∏–¥–µ —Ç–µ–∫—Å—Ç–∞
# ---------------------------------------------------------------------
def build_report(df: pd.DataFrame, eda: dict, task: dict, model: dict | None) -> str:
    rows, cols = eda["shape"]
    lines: list[str] = []

    lines.append(f"üìä –í –¥–∞—Ç–∞—Å–µ—Ç–µ {rows} —Å—Ç—Ä–æ–∫ –∏ {cols} –∫–æ–ª–æ–Ω–æ–∫.")

    # –ø—Ä–æ–ø—É—Å–∫–∏
    nulls = eda.get("nulls", {})
    top_nulls = {k: v for k, v in nulls.items() if v > 0}
    if top_nulls:
        lines.append("üï≥Ô∏è –ü—Ä–æ–ø—É—Å–∫–∏ (—Ç–æ–ø):")
        for k, v in list(top_nulls.items())[:10]:
            lines.append(f"  ‚Ä¢ {k}: {v}")

    # –Ω–µ–º–Ω–æ–≥–æ –ø—Ä–æ —á–∏—Å–ª–∞
    num_stats = eda.get("numeric_stats", {})
    if num_stats:
        lines.append("üìê –ß–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (mean / std / min / max):")
        for name, st in list(num_stats.items())[:10]:
            lines.append(
                f"  ‚Ä¢ {name}: {st['mean']:.3f}/{st['std']:.3f}/{st['min']}/{st['max']}"
            )

    # –∑–∞–¥–∞—á–∞
    if task["task"] == "eda" or task["target"] is None:
        lines.append("üß† –ü–æ–¥—Ö–æ–¥—è—â–µ–π —Ü–µ–ª–µ–≤–æ–π –∫–æ–ª–æ–Ω–∫–∏ –Ω–µ –Ω–∞—à–ª–æ—Å—å ‚Äî —Å–¥–µ–ª–∞–Ω —Ç–æ–ª—å–∫–æ EDA.")
    else:
        lines.append(f'üß† –ó–∞–¥–∞—á–∞: {task["task"]} –ø–æ –∫–æ–ª–æ–Ω–∫–µ "{task["target"]}".')

    # –º–æ–¥–µ–ª—å
    if model:
        if "accuracy" in model:
            lines.append(
                f'üß™ –ú–æ–¥–µ–ª—å: {model["model_type"]}, accuracy={model["accuracy"]:.3f}, f1={model["f1"]:.3f}'
            )
        elif "rmse" in model:
            lines.append(
                f'üß™ –ú–æ–¥–µ–ª—å: {model["model_type"]}, RMSE={model["rmse"]:.3f}'
            )
    else:
        lines.append("üì¶ –ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–∞–ª–∞—Å—å.")

    return "\n".join(lines)


# ---------------------------------------------------------------------
# 6. –≥—Ä–∞—Ñ–∏–∫–∏ ‚Üí base64
# ---------------------------------------------------------------------
def make_plots_base64(df: pd.DataFrame) -> list[dict]:
    plots: list[dict] = []

    # –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã –ø–æ –ø–µ—Ä–≤—ã–º 3 —á–∏—Å–ª–æ–≤—ã–º
    num_cols = df.select_dtypes(include=["number"]).columns.tolist()[:3]
    for col in num_cols:
        fig, ax = plt.subplots(figsize=(4, 3))
        ax.hist(df[col].dropna(), bins=30)
        ax.set_title(f"Distribution of {col}")
        buf = BytesIO()
        plt.tight_layout()
        fig.savefig(buf, format="png")
        plt.close(fig)
        buf.seek(0)
        b64 = base64.b64encode(buf.read()).decode("utf-8")
        plots.append({"name": f"hist_{col}", "image_base64": b64})

    # –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è
    if len(df.select_dtypes(include=["number"]).columns) >= 2:
        corr = df.select_dtypes(include=["number"]).corr()
        fig, ax = plt.subplots(figsize=(4, 3))
        cax = ax.imshow(corr, cmap="viridis")
        ax.set_xticks(range(len(corr.columns)))
        ax.set_yticks(range(len(corr.columns)))
        ax.set_xticklabels(corr.columns, rotation=90, fontsize=6)
        ax.set_yticklabels(corr.columns, fontsize=6)
        fig.colorbar(cax)
        plt.tight_layout()
        buf = BytesIO()
        fig.savefig(buf, format="png")
        plt.close(fig)
        buf.seek(0)
        b64 = base64.b64encode(buf.read()).decode("utf-8")
        plots.append({"name": "correlation", "image_base64": b64})

    return plots


# ---------------------------------------------------------------------
# 7. —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–∞–Ωa –Ω–∞ –¥–∏—Å–∫ (–µ—Å–ª–∏ –Ω–∞–¥–æ —Å –¥–∏—Å–∫–æ–º —Ä–∞–±–æ—Ç–∞—Ç—å)
# ---------------------------------------------------------------------
def save_run(run_data: dict, model_pipeline) -> str:
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç run –≤ –ø–∞–ø–∫—É runs/<uuid>/ :
      - report.json
      - model.joblib (–µ—Å–ª–∏ –µ—Å—Ç—å –º–æ–¥–µ–ª—å)
    """
    run_id = str(uuid.uuid4())
    run_dir = os.path.join("runs", run_id)
    os.makedirs(run_dir, exist_ok=True)

    with open(os.path.join(run_dir, "report.json"), "w", encoding="utf-8") as f:
        json.dump(run_data, f, ensure_ascii=False, indent=2)

    if model_pipeline is not None:
        joblib.dump(model_pipeline, os.path.join(run_dir, "model.joblib"))

    return run_id


def analyze_dataset(df: pd.DataFrame, eda: dict, task: dict) -> dict:
    """
    –í—ã—á–∏—Å–ª—è–µ–º –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫—É—é –∏–Ω—Ñ—É: –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã, –∫–≤–∞–∑–∏-–∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã, –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏,
    –¥–∏—Å–±–∞–ª–∞–Ω—Å (–µ—Å–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è), NaN –≤ —Ç–∞—Ä–≥–µ—Ç–µ –∏ —Ç.–ø.
    –≠—Ç–æ –æ—Ç–¥–∞—ë–º –≤ API, —á—Ç–æ–±—ã —Ñ—Ä–æ–Ω—Ç –º–æ–≥ –ø–æ–¥—Å–≤–µ—Ç–∏—Ç—å.
    """
    problems: dict[str, object] = {}

    # 1) –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã –∏ –ø–æ—á—Ç–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã
    constant_cols = []
    quasi_constant_cols = []
    for col in df.columns:
        nunique = df[col].nunique(dropna=True)
        if nunique <= 1:
            constant_cols.append(col)
        elif nunique <= max(3, int(0.01 * len(df))):
            quasi_constant_cols.append(col)
    if constant_cols:
        problems["constant_features"] = constant_cols
    if quasi_constant_cols:
        problems["quasi_constant_features"] = quasi_constant_cols

    # 2) –≤—ã—Å–æ–∫–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ø–æ —á–∏—Å–ª–æ–≤—ã–º
    num_df = df.select_dtypes(include=["number"])
    high_corr_pairs = []
    if num_df.shape[1] >= 2:
        corr = num_df.corr().abs()
        cols = corr.columns.tolist()
        for i in range(len(cols)):
            for j in range(i + 1, len(cols)):
                if corr.iloc[i, j] >= 0.9:  # –ø–æ—Ä–æ–≥ –º–æ–∂–Ω–æ –∫—Ä—É—Ç–∏—Ç—å
                    high_corr_pairs.append((cols[i], cols[j], float(corr.iloc[i, j])))
    if high_corr_pairs:
        problems["high_corr_pairs"] = high_corr_pairs

    # 3) –ø—Ä–æ–ø—É—Å–∫–∏ (–≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö)
    null_perc = (df.isna().sum() / len(df) * 100).sort_values(ascending=False)
    high_nulls = null_perc[null_perc > 30].to_dict()  # >30% —Å—á–∏—Ç–∞–µ–º –º–Ω–æ–≥–æ
    if high_nulls:
        problems["high_null_features"] = {k: float(v) for k, v in high_nulls.items()}

    # 4) NaN –≤ —Ç–∞—Ä–≥–µ—Ç–µ
    target = task.get("target")
    if target and target in df.columns:
        n_nan_target = int(df[target].isna().sum())
        if n_nan_target > 0:
            problems["target_has_nan"] = {"column": target, "nan_count": n_nan_target}

    # 5) –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤
    if task.get("task") == "classification" and target and target in df.columns:
        vc = df[target].value_counts(dropna=False)
        if len(vc) >= 2:
            max_c = int(vc.iloc[0])
            min_c = int(vc.iloc[-1])
            ratio = max_c / max(1, min_c)
            if ratio >= 5:  # –¥–∏—Å–±–∞–ª–∞–Ω—Å
                problems["class_imbalance"] = {
                    "max_class": vc.index[0],
                    "max_count": max_c,
                    "min_class": vc.index[-1],
                    "min_count": min_c,
                    "ratio": float(ratio),
                }

    # 6) –æ—á–µ–Ω—å –º–Ω–æ–≥–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    high_cardinality = []
    for col in df.select_dtypes(include=["object"]).columns:
        nunique = df[col].nunique(dropna=True)
        if nunique > 200:  # –º–Ω–æ–≥–∞ –±—É–∫–∞—Ñ
            high_cardinality.append({"column": col, "n_unique": int(nunique)})
    if high_cardinality:
        problems["high_cardinality"] = high_cardinality

    return problems
