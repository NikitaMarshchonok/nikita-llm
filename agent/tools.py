# agent/tools.py
from __future__ import annotations

import os
import re
import json
import uuid
import base64
from io import BytesIO
from typing import Optional, Literal

import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import accuracy_score, f1_score, mean_squared_error
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
import joblib

import matplotlib
matplotlib.use("Agg")  # —á—Ç–æ–±—ã —Ä–µ–Ω–¥–µ—Ä–∏—Ç—å –±–µ–∑ GUI
import matplotlib.pyplot as plt


# ---------------------------------------------------------------------
# 1. EDA
# ---------------------------------------------------------------------
def basic_eda(df: pd.DataFrame) -> dict:
    eda = {
        "shape": list(df.shape),
        "dtypes": {c: str(df[c].dtype) for c in df.columns},
        "nulls": {c: int(df[c].isna().sum()) for c in df.columns},
    }

    numeric_cols = df.select_dtypes(include=["number"]).columns.tolist()
    stats = {}
    for c in numeric_cols[:20]:
        ser = df[c]
        stats[c] = {
            "mean": float(ser.mean()),
            "std": float(ser.std() or 0),
            "min": float(ser.min()),
            "max": float(ser.max()),
        }
    eda["numeric_stats"] = stats
    return eda


# ---------------------------------------------------------------------
# 2. —É–≥–∞–¥—ã–≤–∞–Ω–∏–µ —Ç–∞—Ä–≥–µ—Ç–∞ –∏ –∑–∞–¥–∞—á–∏
# ---------------------------------------------------------------------
ID_LIKE = {"id", "ID", "Id", "index", "Rk", "rank"}


def _looks_like_id(colname: str) -> bool:
    return colname in ID_LIKE or re.search(r"id$", colname, re.IGNORECASE) is not None


def _guess_target(df: pd.DataFrame) -> tuple[Literal["eda", "classification", "regression"], Optional[str]]:
    lower_cols = {c.lower(): c for c in df.columns}

    # –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è
    for cand in ("target", "label", "class", "y"):
        if cand in lower_cols:
            col = lower_cols[cand]
            if df[col].nunique() <= 50:
                return "classification", col
            else:
                return "regression", col

    # –º–∞–ª–µ–Ω—å–∫–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ
    for c in df.columns:
        if _looks_like_id(c):
            continue
        uniq = df[c].nunique(dropna=True)
        if 2 <= uniq <= 30:
            return "classification", c

    # —á–∏—Å–ª–æ–≤—ã–µ
    num_cols = df.select_dtypes(include=["number"]).columns.tolist()
    for c in num_cols:
        if _looks_like_id(c):
            continue
        if df[c].nunique() > 1:
            return "regression", c

    return "eda", None


def detect_task(df: pd.DataFrame, target: Optional[str] = None) -> dict:
    if target is not None and target in df.columns:
        nunique = df[target].nunique()
        if df[target].dtype == "object" or nunique <= 30:
            task = "classification"
        else:
            task = "regression"
        return {"task": task, "target": target}

    task, tgt = _guess_target(df)
    return {"task": task, "target": tgt}


# ---------------------------------------------------------------------
# 3. –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ –∫ —á–∏—Å–ª–∞–º –∏ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥
# ---------------------------------------------------------------------
def _coerce_numeric(df: pd.DataFrame) -> pd.DataFrame:
    """
    –ü—Ä–æ–±—É–µ–º —Å—Ç—Ä–æ–∫–∏, –ø–æ—Ö–æ–∂–∏–µ –Ω–∞ —á–∏—Å–ª–∞, –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ —á–∏—Å–ª–∞–º.
    –ë–µ–∑ warnings.
    """
    new_df = df.copy()
    for col in new_df.columns:
        if new_df[col].dtype == "object":
            # —Å–Ω–∞—á–∞–ª–∞ —á–∏—Å—Ç–∏–º —Å—Ç—Ä–æ–∫—É
            ser = new_df[col].astype(str).str.replace(",", "").str.replace(" ", "")
            try:
                converted = pd.to_numeric(ser)
            except Exception:
                # –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å ‚Äî –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –±—ã–ª–æ
                continue
            else:
                # –ø–æ–ª—É—á–∏–ª–æ—Å—å ‚Äî –ø–æ–¥–º–µ–Ω—è–µ–º –∫–æ–ª–æ–Ω–∫—É
                new_df[col] = converted
    return new_df



def build_preprocessor(X: pd.DataFrame) -> ColumnTransformer:
    numeric_features = X.select_dtypes(include=["int64", "float64", "int32", "float32"]).columns.tolist()
    categorical_features = [c for c in X.columns if c not in numeric_features]

    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
        ]
    )

    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore")),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ]
    )
    return preprocessor


# ---------------------------------------------------------------------
# 4. –æ–±—É—á–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
# ---------------------------------------------------------------------
def train_baseline(
    df: pd.DataFrame,
    target: str,
    task: str,
    return_model: bool = False,
) -> Optional[dict]:
    """
    –û–±—É—á–∞–µ–º –æ—á–µ–Ω—å –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å –ø–æ–≤–µ—Ä—Ö –∞–≤—Ç–æ-–ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞.
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∏ —Ç–∏–ø –º–æ–¥–µ–ª–∏.
    –ï—Å–ª–∏ return_model=True ‚Äî –µ—â—ë –∏ —Å–∞–º sklearn-–ø–∞–π–ø–ª–∞–π–Ω (–¥–ª—è /predict).
    """
    if target not in df.columns:
        return None

    # –ø–æ–ø—Ä–æ–±—É–µ–º –ø—Ä–∏–≤–µ—Å—Ç–∏ —Å—Ç—Ä–æ–∫–æ–≤—ã–µ —á–∏—Å–ª–∞
    df = _coerce_numeric(df)

    y = df[target]
    X = df.drop(columns=[target])

    if X.shape[1] == 0:
        return None

    preprocessor = build_preprocessor(X)

    # ----- –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è -----
    if task == "classification":
        model = RandomForestClassifier(
            n_estimators=200,
            random_state=42,
            n_jobs=-1,
        )

        # –ø—Ä–æ–≤–µ—Ä—è–µ–º, –º–æ–∂–Ω–æ –ª–∏ —Å—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å
        counts = y.value_counts(dropna=False)
        can_stratify = (counts >= 2).all()

        X_train, X_val, y_train, y_val = train_test_split(
            X,
            y,
            test_size=0.2,
            random_state=42,
            stratify=y if (y.nunique() < 50 and can_stratify) else None,
        )

        pipe = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
        pipe.fit(X_train, y_train)
        preds = pipe.predict(X_val)

        acc = float(accuracy_score(y_val, preds))
        f1 = float(f1_score(y_val, preds, average="weighted"))

        res: dict = {
            "model_type": "RandomForestClassifier",
            "accuracy": acc,
            "f1": f1,
        }
        if return_model:
            res["pipeline"] = pipe
        return res

    # ----- —Ä–µ–≥—Ä–µ—Å—Å–∏—è -----
    elif task == "regression":
        model = RandomForestRegressor(
            n_estimators=200,
            random_state=42,
            n_jobs=-1,
        )

        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        pipe = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
        pipe.fit(X_train, y_train)
        preds = pipe.predict(X_val)

        mse = float(mean_squared_error(y_val, preds))
        rmse = mse ** 0.5

        res: dict = {
            "model_type": "RandomForestRegressor",
            "rmse": rmse,
        }
        if return_model:
            res["pipeline"] = pipe
        return res

    else:
        return None


# ---------------------------------------------------------------------
# 5. –æ—Ç—á—ë—Ç –≤ –≤–∏–¥–µ —Ç–µ–∫—Å—Ç–∞
# ---------------------------------------------------------------------
def build_report(df: pd.DataFrame, eda: dict, task: dict, model: dict | None) -> str:
    rows, cols = eda["shape"]
    lines: list[str] = []

    lines.append(f"üìä –í –¥–∞—Ç–∞—Å–µ—Ç–µ {rows} —Å—Ç—Ä–æ–∫ –∏ {cols} –∫–æ–ª–æ–Ω–æ–∫.")

    # –ø—Ä–æ–ø—É—Å–∫–∏
    nulls = eda.get("nulls", {})
    top_nulls = {k: v for k, v in nulls.items() if v > 0}
    if top_nulls:
        lines.append("üï≥Ô∏è –ü—Ä–æ–ø—É—Å–∫–∏ (—Ç–æ–ø):")
        for k, v in list(top_nulls.items())[:10]:
            lines.append(f"  ‚Ä¢ {k}: {v}")

    # –Ω–µ–º–Ω–æ–≥–æ –ø—Ä–æ —á–∏—Å–ª–∞
    num_stats = eda.get("numeric_stats", {})
    if num_stats:
        lines.append("üìê –ß–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (mean / std / min / max):")
        for name, st in list(num_stats.items())[:10]:
            lines.append(
                f"  ‚Ä¢ {name}: {st['mean']:.3f}/{st['std']:.3f}/{st['min']}/{st['max']}"
            )

    # –∑–∞–¥–∞—á–∞
    if task["task"] == "eda" or task["target"] is None:
        lines.append("üß† –ü–æ–¥—Ö–æ–¥—è—â–µ–π —Ü–µ–ª–µ–≤–æ–π –∫–æ–ª–æ–Ω–∫–∏ –Ω–µ –Ω–∞—à–ª–æ—Å—å ‚Äî —Å–¥–µ–ª–∞–Ω —Ç–æ–ª—å–∫–æ EDA.")
    else:
        lines.append(f'üß† –ó–∞–¥–∞—á–∞: {task["task"]} –ø–æ –∫–æ–ª–æ–Ω–∫–µ "{task["target"]}".')

    # –º–æ–¥–µ–ª—å
    if model:
        if "accuracy" in model:
            lines.append(
                f'üß™ –ú–æ–¥–µ–ª—å: {model["model_type"]}, accuracy={model["accuracy"]:.3f}, f1={model["f1"]:.3f}'
            )
        elif "rmse" in model:
            lines.append(
                f'üß™ –ú–æ–¥–µ–ª—å: {model["model_type"]}, RMSE={model["rmse"]:.3f}'
            )
    else:
        lines.append("üì¶ –ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–∞–ª–∞—Å—å.")

    return "\n".join(lines)


# ---------------------------------------------------------------------
# 6. –≥—Ä–∞—Ñ–∏–∫–∏ ‚Üí base64
# ---------------------------------------------------------------------
def make_plots_base64(df: pd.DataFrame) -> list[dict]:
    plots: list[dict] = []

    # –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã –ø–æ –ø–µ—Ä–≤—ã–º 3 —á–∏—Å–ª–æ–≤—ã–º
    num_cols = df.select_dtypes(include=["number"]).columns.tolist()[:3]
    for col in num_cols:
        fig, ax = plt.subplots(figsize=(4, 3))
        ax.hist(df[col].dropna(), bins=30)
        ax.set_title(f"Distribution of {col}")
        buf = BytesIO()
        plt.tight_layout()
        fig.savefig(buf, format="png")
        plt.close(fig)
        buf.seek(0)
        b64 = base64.b64encode(buf.read()).decode("utf-8")
        plots.append({"name": f"hist_{col}", "image_base64": b64})

    # –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è
    if len(df.select_dtypes(include=["number"]).columns) >= 2:
        corr = df.select_dtypes(include=["number"]).corr()
        fig, ax = plt.subplots(figsize=(4, 3))
        cax = ax.imshow(corr, cmap="viridis")
        ax.set_xticks(range(len(corr.columns)))
        ax.set_yticks(range(len(corr.columns)))
        ax.set_xticklabels(corr.columns, rotation=90, fontsize=6)
        ax.set_yticklabels(corr.columns, fontsize=6)
        fig.colorbar(cax)
        plt.tight_layout()
        buf = BytesIO()
        fig.savefig(buf, format="png")
        plt.close(fig)
        buf.seek(0)
        b64 = base64.b64encode(buf.read()).decode("utf-8")
        plots.append({"name": "correlation", "image_base64": b64})

    return plots


# ---------------------------------------------------------------------
# 7. —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–∞–Ωa –Ω–∞ –¥–∏—Å–∫ (–µ—Å–ª–∏ –Ω–∞–¥–æ —Å –¥–∏—Å–∫–æ–º —Ä–∞–±–æ—Ç–∞—Ç—å)
# ---------------------------------------------------------------------
def save_run(run_data: dict, model_pipeline) -> str:
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç run –≤ –ø–∞–ø–∫—É runs/<uuid>/ :
      - report.json
      - model.joblib (–µ—Å–ª–∏ –µ—Å—Ç—å –º–æ–¥–µ–ª—å)
    """
    run_id = str(uuid.uuid4())
    run_dir = os.path.join("runs", run_id)
    os.makedirs(run_dir, exist_ok=True)

    with open(os.path.join(run_dir, "report.json"), "w", encoding="utf-8") as f:
        json.dump(run_data, f, ensure_ascii=False, indent=2)

    if model_pipeline is not None:
        joblib.dump(model_pipeline, os.path.join(run_dir, "model.joblib"))

    return run_id
