# agent/tools.py
from __future__ import annotations

import os
import json
import uuid
import base64
from io import BytesIO
from typing import Optional, Literal

import numpy as np
import pandas as pd
import re

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import accuracy_score, f1_score, mean_squared_error
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
import joblib

import matplotlib
matplotlib.use("Agg")  # —á—Ç–æ–±—ã —Ä–µ–Ω–¥–µ—Ä–∏—Ç—å –±–µ–∑ GUI
import matplotlib.pyplot as plt


# ---------------------------------------------------------------------
# 1. –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π EDA
# ---------------------------------------------------------------------
def basic_eda(df: pd.DataFrame) -> dict:
    """
    –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π EDA:
    - —Ñ–æ—Ä–º–∞
    - —Ç–∏–ø—ã
    - –ø—Ä–æ–ø—É—Å–∫–∏ (–∫–æ–ª-–≤–æ –∏ –¥–æ–ª—è)
    - –±–∞–∑–æ–≤—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ —á–∏—Å–ª–æ–≤—ã–º
    - –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –∏ –ø–æ—á—Ç–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    - –ø–∞—Ä—ã —Å –≤—ã—Å–æ–∫–æ–π –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–µ–π
    """
    eda: dict = {
        "shape": list(df.shape),
        "dtypes": {c: str(df[c].dtype) for c in df.columns},
    }

    # –ø—Ä–æ–ø—É—Å–∫–∏
    eda["nulls"] = {c: int(df[c].isna().sum()) for c in df.columns}
    eda["null_fractions"] = {c: float(df[c].isna().mean()) for c in df.columns}

    # stats –ø–æ —á–∏—Å–ª–æ–≤—ã–º
    numeric_cols = df.select_dtypes(include=["number"]).columns.tolist()
    stats = {}
    for c in numeric_cols[:30]:
        ser = df[c]
        stats[c] = {
            "mean": float(ser.mean()),
            "std": float(ser.std() or 0),
            "min": float(ser.min()),
            "max": float(ser.max()),
        }
    eda["numeric_stats"] = stats

    # –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã / –ø–æ—á—Ç–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã
    constant_features = []
    quasi_constant_features = []
    for c in df.columns:
        uniq = df[c].nunique(dropna=True)
        if uniq <= 1:
            constant_features.append(c)
        else:
            top_frac = float(df[c].value_counts(normalize=True, dropna=False).iloc[0])
            if top_frac > 0.98:
                quasi_constant_features.append(c)
    eda["constant_features"] = constant_features
    eda["quasi_constant_features"] = quasi_constant_features

    # –≤—ã—Å–æ–∫–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è
    high_corr_pairs = []
    if len(numeric_cols) >= 2:
        corr = df[numeric_cols].corr().abs()
        for i, c1 in enumerate(numeric_cols):
            for j in range(i + 1, len(numeric_cols)):
                c2 = numeric_cols[j]
                val = float(corr.loc[c1, c2])
                if val >= 0.9:
                    high_corr_pairs.append(
                        {"feature_1": c1, "feature_2": c2, "corr": val}
                    )
    eda["high_corr_pairs"] = high_corr_pairs

    return eda


# ---------------------------------------------------------------------
# 2. —É–≥–∞–¥—ã–≤–∞–Ω–∏–µ —Ç–∞—Ä–≥–µ—Ç–∞ –∏ —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏
# ---------------------------------------------------------------------
ID_LIKE = {"id", "ID", "Id", "index", "Rk", "rank"}


def _looks_like_id(colname: str) -> bool:
    return colname in ID_LIKE or re.search(r"id$", colname, re.IGNORECASE) is not None


def _guess_target(
    df: pd.DataFrame,
) -> tuple[Literal["eda", "classification", "regression"], Optional[str]]:
    lower_cols = {c.lower(): c for c in df.columns}

    # –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ
    for cand in ("target", "label", "class", "y"):
        if cand in lower_cols:
            col = lower_cols[cand]
            if df[col].nunique() <= 50:
                return "classification", col
            else:
                return "regression", col

    # –º–∞–ª–µ–Ω—å–∫–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ
    for c in df.columns:
        if _looks_like_id(c):
            continue
        uniq = df[c].nunique(dropna=True)
        if 2 <= uniq <= 30:
            return "classification", c

    # —á–∏—Å–ª–æ–≤—ã–µ
    num_cols = df.select_dtypes(include=["number"]).columns.tolist()
    for c in num_cols:
        if _looks_like_id(c):
            continue
        if df[c].nunique() > 1:
            return "regression", c

    return "eda", None


def detect_task(df: pd.DataFrame, target: Optional[str] = None) -> dict:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ–º –∑–∞–¥–∞—á—É –∏ –∫–æ–ª–æ–Ω–∫—É-—Ç–∞—Ä–≥–µ—Ç."""
    if target is not None and target in df.columns:
        nunique = df[target].nunique()
        if df[target].dtype == "object" or nunique <= 30:
            task = "classification"
        else:
            task = "regression"
        return {"task": task, "target": target}

    task, tgt = _guess_target(df)
    return {"task": task, "target": tgt}


# ---------------------------------------------------------------------
# 3. –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ –∫ —á–∏—Å–ª–∞–º –∏ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥
# ---------------------------------------------------------------------
def _coerce_numeric(df: pd.DataFrame) -> pd.DataFrame:
    """
    –ü—Ä–æ–±—É–µ–º —Å—Ç—Ä–æ–∫–∏, –ø–æ—Ö–æ–∂–∏–µ –Ω–∞ —á–∏—Å–ª–∞, –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ —á–∏—Å–ª–∞–º.
    –ë–µ–∑ FutureWarning.
    """
    new_df = df.copy()
    for col in new_df.columns:
        if new_df[col].dtype == "object":
            ser = new_df[col].astype(str).str.replace(",", "").str.replace(" ", "")
            try:
                converted = pd.to_numeric(ser)
            except Exception:
                continue
            else:
                if converted.dtype != "object":
                    new_df[col] = converted
    return new_df


def build_preprocessor(X: pd.DataFrame) -> ColumnTransformer:
    numeric_features = X.select_dtypes(
        include=["int64", "float64", "int32", "float32"]
    ).columns.tolist()
    categorical_features = [c for c in X.columns if c not in numeric_features]

    numeric_transformer = Pipeline(
        steps=[("imputer", SimpleImputer(strategy="median"))]
    )

    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore")),
        ]
    )

    return ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ]
    )


# ---------------------------------------------------------------------
# 4. –û–±—É—á–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ
# ---------------------------------------------------------------------
def train_baseline(
    df: pd.DataFrame,
    target: str,
    task: str,
    return_model: bool = False,
) -> Optional[dict]:
    """
    –û–±—É—á–∞–µ–º –æ—á–µ–Ω—å –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å.
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º dict —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏.
    –ï—Å–ª–∏ —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫ ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º None, —á—Ç–æ–±—ã /upload –Ω–µ –ø–∞–¥–∞–ª.
    """
    try:
        if target not in df.columns:
            return None

        df = _coerce_numeric(df)

        # –≤—ã–±—Ä–æ—Å–∏–º —Å—Ç—Ä–æ–∫–∏ –±–µ–∑ —Ç–∞—Ä–≥–µ—Ç–∞
        df = df[~df[target].isna()].copy()
        if df.shape[0] < 20:
            return None

        y = df[target]
        X = df.drop(columns=[target])

        if X.shape[1] == 0:
            return None

        preprocessor = build_preprocessor(X)

        # –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø
        if task == "classification":
            if y.nunique() < 2:
                return None

            model = RandomForestClassifier(
                n_estimators=200, random_state=42, n_jobs=-1
            )

            counts = y.value_counts(dropna=False)
            can_stratify = (counts >= 2).all()

            X_train, X_val, y_train, y_val = train_test_split(
                X,
                y,
                test_size=0.2,
                random_state=42,
                stratify=y if (y.nunique() < 50 and can_stratify) else None,
            )

            pipe = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
            pipe.fit(X_train, y_train)
            preds = pipe.predict(X_val)

            acc = float(accuracy_score(y_val, preds))
            f1 = float(f1_score(y_val, preds, average="weighted"))

            res = {
                "model_type": "RandomForestClassifier",
                "accuracy": acc,
                "f1": f1,
            }
            if return_model:
                res["pipeline"] = pipe
            return res

        # –†–ï–ì–†–ï–°–°–ò–Ø
        elif task == "regression":
            model = RandomForestRegressor(
                n_estimators=200, random_state=42, n_jobs=-1
            )
            X_train, X_val, y_train, y_val = train_test_split(
                X, y, test_size=0.2, random_state=42
            )
            pipe = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
            pipe.fit(X_train, y_train)
            preds = pipe.predict(X_val)

            mse = float(mean_squared_error(y_val, preds))
            rmse = mse ** 0.5

            res = {
                "model_type": "RandomForestRegressor",
                "rmse": rmse,
            }
            if return_model:
                res["pipeline"] = pipe
            return res

        # –µ—Å–ª–∏ –∑–∞–¥–∞—á–∞ —Ç–æ–ª—å–∫–æ EDA
        return None

    except Exception:
        return None


# ---------------------------------------------------------------------
# 5. –û—Ç—á—ë—Ç —Ç–µ–∫—Å—Ç–æ–º
# ---------------------------------------------------------------------
def build_report(df: pd.DataFrame, eda: dict, task: dict, model: dict | None) -> str:
    rows, cols = eda["shape"]
    lines: list[str] = []

    lines.append(f"üìä –í –¥–∞—Ç–∞—Å–µ—Ç–µ {rows} —Å—Ç—Ä–æ–∫ –∏ {cols} –∫–æ–ª–æ–Ω–æ–∫.")

    # –ø—Ä–æ–ø—É—Å–∫–∏
    nulls = eda.get("nulls", {})
    top_nulls = {k: v for k, v in nulls.items() if v > 0}
    if top_nulls:
        lines.append("üï≥Ô∏è –ü—Ä–æ–ø—É—Å–∫–∏ (—Ç–æ–ø):")
        for k, v in list(top_nulls.items())[:10]:
            lines.append(f"  ‚Ä¢ {k}: {v}")

    # —á–∏—Å–ª–æ–≤—ã–µ
    num_stats = eda.get("numeric_stats", {})
    if num_stats:
        lines.append("üìê –ß–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (mean / std / min / max):")
        for name, st in list(num_stats.items())[:10]:
            lines.append(
                f"  ‚Ä¢ {name}: {st['mean']:.3f}/{st['std']:.3f}/{st['min']}/{st['max']}"
            )

    # –∑–∞–¥–∞—á–∞
    if task["task"] == "eda" or task["target"] is None:
        lines.append("üß† –ü–æ–¥—Ö–æ–¥—è—â–µ–π —Ü–µ–ª–µ–≤–æ–π –∫–æ–ª–æ–Ω–∫–∏ –Ω–µ –Ω–∞—à–ª–æ—Å—å ‚Äî —Å–¥–µ–ª–∞–Ω —Ç–æ–ª—å–∫–æ EDA.")
    else:
        lines.append(f'üß† –ó–∞–¥–∞—á–∞: {task["task"]} –ø–æ –∫–æ–ª–æ–Ω–∫–µ "{task["target"]}".')

    # –º–æ–¥–µ–ª—å
    if model:
        if "accuracy" in model:
            lines.append(
                f'üß™ –ú–æ–¥–µ–ª—å: {model["model_type"]}, accuracy={model["accuracy"]:.3f}, f1={model["f1"]:.3f}'
            )
        elif "rmse" in model:
            lines.append(
                f'üß™ –ú–æ–¥–µ–ª—å: {model["model_type"]}, RMSE={model["rmse"]:.3f}'
            )
    else:
        lines.append("üì¶ –ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–∞–ª–∞—Å—å.")

    return "\n".join(lines)


# ---------------------------------------------------------------------
# 6. –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–±–ª–µ–º (–¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π)
# ---------------------------------------------------------------------
def analyze_dataset(df: pd.DataFrame, eda: dict, task: dict) -> dict:
    """
    –°–æ–±–∏—Ä–∞–µ–º –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫—É—é –∏–Ω—Ñ—É:
    - –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã / –∫–≤–∞–∑–∏-–∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã
    - –≤—ã—Å–æ–∫–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è
    - –º–Ω–æ–≥–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤
    - NaN –≤ —Ç–∞—Ä–≥–µ—Ç–µ
    - –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤
    - –≤—ã—Å–æ–∫–∞—è –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å
    """
    problems: dict[str, object] = {}

    # 1) –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã –∏ –ø–æ—á—Ç–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã
    constant_cols = []
    quasi_constant_cols = []
    for col in df.columns:
        nunique = df[col].nunique(dropna=True)
        if nunique <= 1:
            constant_cols.append(col)
        else:
            top_frac = df[col].value_counts(normalize=True, dropna=False).iloc[0]
            if top_frac > 0.98:
                quasi_constant_cols.append(col)

    if constant_cols:
        problems["constant_features"] = constant_cols
    if quasi_constant_cols:
        problems["quasi_constant_features"] = quasi_constant_cols

    # 2) –≤—ã—Å–æ–∫–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è
    num_df = df.select_dtypes(include=["number"])
    high_corr_pairs = []
    if num_df.shape[1] >= 2:
        corr = num_df.corr().abs()
        cols = corr.columns.tolist()
        for i in range(len(cols)):
            for j in range(i + 1, len(cols)):
                cval = float(corr.iloc[i, j])
                if cval >= 0.9:
                    high_corr_pairs.append((cols[i], cols[j], cval))
    if high_corr_pairs:
        problems["high_corr_pairs"] = high_corr_pairs

    # 3) –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –±–æ–ª—å—à–∏–º % –ø—Ä–æ–ø—É—Å–∫–æ–≤
    null_perc = (df.isna().sum() / len(df) * 100)
    high_nulls = {col: float(round(p, 1)) for col, p in null_perc.items() if p >= 30.0}
    if high_nulls:
        problems["high_null_features"] = high_nulls

    # 4) NaN –≤ —Ç–∞—Ä–≥–µ—Ç–µ
    target = task.get("target")
    if target and target in df.columns:
        nan_cnt = int(df[target].isna().sum())
        if nan_cnt > 0:
            problems["target_has_nan"] = {
                "column": target,
                "nan_count": nan_cnt,
                "share": float(round(nan_cnt / len(df) * 100, 1)),
            }

    # 5) –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤
    if task.get("task") == "classification" and target and target in df.columns:
        vc = df[target].value_counts(dropna=False)
        if len(vc) >= 2:
            max_c = int(vc.iloc[0])
            min_c = int(vc.iloc[-1])
            ratio = max_c / max(1, min_c)
            if ratio >= 5:
                problems["class_imbalance"] = {
                    "max_class": vc.index[0],
                    "max_count": max_c,
                    "min_class": vc.index[-1],
                    "min_count": min_c,
                    "ratio": float(round(ratio, 1)),
                }

    # 6) –≤—ã—Å–æ–∫–∞—è –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö
    high_cardinality = []
    for col in df.select_dtypes(include=["object"]).columns:
        nunique = df[col].nunique(dropna=True)
        if nunique > 200:
            high_cardinality.append(
                {"column": col, "n_unique": int(nunique)}
            )
    if high_cardinality:
        problems["high_cardinality"] = high_cardinality

    return problems


# ---------------------------------------------------------------------
# 7. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–±–ª–µ–º
# ---------------------------------------------------------------------
def build_recommendations(
    df: pd.DataFrame,
    eda: dict,
    task: dict,
    problems: dict,
    model: dict | None,
    max_items: int = 6,
) -> list[str]:
    """
    –ù–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ —Å–æ–±–∏—Ä–∞–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π.
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫ (—á—Ç–æ–±—ã —Ñ—Ä–æ–Ω—Ç –º–æ–≥ –∏—Ö –ø—Ä–æ—Å—Ç–æ –ø–æ–∫–∞–∑–∞—Ç—å).
    """
    rec_objs: list[dict] = []

    # 1. –∫—Ä–∏—Ç–∏—á–Ω—ã–µ —à—Ç—É–∫–∏
    # NaN –≤ target
    if problems.get("target_has_nan"):
        info = problems["target_has_nan"]
        rec_objs.append({
            "priority": 100,
            "text": (
                f"–í —Ü–µ–ª–µ–≤–æ–π –∫–æ–ª–æ–Ω–∫–µ {info['column']} –µ—Å—Ç—å –ø—Ä–æ–ø—É—Å–∫–∏: {info['nan_count']} "
                f"({info['share']}%). –ü–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º –∏—Ö –Ω—É–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å –∏–ª–∏ –∑–∞–∏–º–ø—É—Ç–∏—Ç—å."
            ),
        })

    # –º–Ω–æ–≥–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤ –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö
    high_nulls = problems.get("high_null_features") or {}
    if high_nulls:
        top = list(high_nulls.items())[:5]
        pretty = ", ".join([f"{c} ({p}%)" for c, p in top])
        rec_objs.append({
            "priority": 90,
            "text": (
                f"–ï—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –±–æ–ª—å—à–∏–º —á–∏—Å–ª–æ–º –ø—Ä–æ–ø—É—Å–∫–æ–≤: {pretty}. "
                "–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–¥–∞–ª–∏—Ç—å —Ç–∞–∫–∏–µ —Å—Ç–æ–ª–±—Ü—ã –∏–ª–∏ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –∏–ºputa—Ü–∏—é."
            ),
        })

    # –¥–∏—Å–±–∞–ª–∞–Ω—Å
    if problems.get("class_imbalance"):
        ci = problems["class_imbalance"]
        rec_objs.append({
            "priority": 85,
            "text": (
                f"–û–±–Ω–∞—Ä—É–∂–µ–Ω –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤: {ci['max_class']}={ci['max_count']} vs "
                f"{ci['min_class']}={ci['min_count']} (‚âà{ci['ratio']}:1). "
                "–ò—Å–ø–æ–ª—å–∑—É–π class_weight='balanced', stratify –ø—Ä–∏ train_test_split –∏–ª–∏ oversampling."
            ),
        })

    # 2. –≤–∞–∂–Ω—ã–µ, –Ω–æ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω—ã–µ
    consts = problems.get("constant_features") or []
    if consts:
        rec_objs.append({
            "priority": 70,
            "text": (
                f"–ö–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {', '.join(consts[:8])}. "
                "–ò—Ö –º–æ–∂–Ω–æ —Å–º–µ–ª–æ —É–¥–∞–ª–∏—Ç—å ‚Äî –æ–Ω–∏ –Ω–µ –Ω–µ—Å—É—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."
            ),
        })

    quasi = problems.get("quasi_constant_features") or []
    if quasi:
        rec_objs.append({
            "priority": 60,
            "text": (
                f"–ü–æ—á—Ç–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {', '.join(quasi[:8])}. "
                "–ü—Ä–æ–≤–µ—Ä—å –∏—Ö –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å –ø–µ—Ä–µ–¥ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ–º."
            ),
        })

    corr_pairs = problems.get("high_corr_pairs") or []
    if corr_pairs:
        short = [f"{a}‚Üî{b} ({c:.2f})" for a, b, c in corr_pairs[:6]]
        rec_objs.append({
            "priority": 55,
            "text": (
                "–ï—Å—Ç—å —Å–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—â–∏–µ –ø–∞—Ä—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: "
                + ", ".join(short)
                + ". –ú–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –æ—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–ª–∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è."
            ),
        })

    high_card = problems.get("high_cardinality") or []
    if high_card:
        show = [f"{x['column']} ({x['n_unique']})" for x in high_card[:3]]
        rec_objs.append({
            "priority": 50,
            "text": (
                "–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –æ—á–µ–Ω—å –±–æ–ª—å—à–∏–º —á–∏—Å–ª–æ–º –∑–Ω–∞—á–µ–Ω–∏–π: "
                + ", ".join(show)
                + ". –õ—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å CatBoost/target encoding/—á–∞—Å—Ç–æ—Ç–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ."
            ),
        })

    # 3. –ø–æ –∑–∞–¥–∞—á–µ
    if task.get("task") == "eda" or task.get("target") is None:
        rec_objs.append({
            "priority": 95,
            "text": "–¶–µ–ª–µ–≤–æ–π –ø—Ä–∏–∑–Ω–∞–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω. –£–∫–∞–∂–∏ –µ–≥–æ —è–≤–Ω–æ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ (–ø–æ–ª–µ target).",
        })
    elif task.get("task") == "regression":
        rec_objs.append({
            "priority": 40,
            "text": "–≠—Ç–æ —Ä–µ–≥—Ä–µ—Å—Å–∏—è ‚Äî –º–æ–∂–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –±–æ–ª–µ–µ —Å–∏–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ (CatBoostRegressor, LightGBM) –∏ –ª–æ–≥-—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é —Ç–∞—Ä–≥–µ—Ç–∞, –µ—Å–ª–∏ –µ—Å—Ç—å –ø–µ—Ä–µ–∫–æ—Å.",
        })
    elif task.get("task") == "classification":
        rec_objs.append({
            "priority": 40,
            "text": "–≠—Ç–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è ‚Äî –∫—Ä–æ–º–µ accuracy/f1 –∏–º–µ–µ—Ç —Å–º—ã—Å–ª —Å—á–∏—Ç–∞—Ç—å ROC-AUC –∏ PR-AUC, –æ—Å–æ–±–µ–Ω–Ω–æ –ø—Ä–∏ –¥–∏—Å–±–∞–ª–∞–Ω—Å–µ.",
        })

    # 4. –ø–æ –º–æ–¥–µ–ª–∏
    if model is None:
        rec_objs.append({
            "priority": 30,
            "text": "–ë–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–±—É—á–∏—Ç—å ‚Äî –Ω—É–∂–Ω–æ –ø–æ—á–∏—Å—Ç–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–ª–∏ —É–∫–∞–∑–∞—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π target.",
        })
    else:
        mtype = model.get("model_type")
        if mtype == "RandomForestClassifier":
            rec_objs.append({
                "priority": 35,
                "text": "–¢–µ–∫—É—â–∞—è –º–æ–¥–µ–ª—å: RandomForestClassifier. –î–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–ø—Ä–æ–±—É–π –±—É—Å—Ç–∏–Ω–≥ (CatBoost/XGBoost/LightGBM) –∏ –ø–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.",
            })
        elif mtype == "RandomForestRegressor":
            rec_objs.append({
                "priority": 35,
                "text": "–¢–µ–∫—É—â–∞—è –º–æ–¥–µ–ª—å: RandomForestRegressor. –ú–æ–∂–Ω–æ —É—Å–∏–ª–∏—Ç—å –º–æ–¥–µ–ª—å –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–º –±—É—Å—Ç–∏–Ω–≥–æ–º –∏ —Ñ–∏—á–µ–∏–Ω–∂–∏–Ω–∏—Ä–∏–Ω–≥–æ–º.",
            })

    # 5. —Å–æ—Ä—Ç–∏—Ä—É–µ–º –∏ –æ–±—Ä–µ–∑–∞–µ–º
    rec_objs.sort(key=lambda x: x["priority"], reverse=True)
    recs = [r["text"] for r in rec_objs[:max_items]]

    return recs


# ---------------------------------------------------------------------
# 8. –ì—Ä–∞—Ñ–∏–∫–∏ ‚Üí base64
# ---------------------------------------------------------------------
def make_plots_base64(df: pd.DataFrame) -> list[dict]:
    plots: list[dict] = []

    num_cols = df.select_dtypes(include=["number"]).columns.tolist()[:3]
    for col in num_cols:
        fig, ax = plt.subplots(figsize=(4, 3))
        ax.hist(df[col].dropna(), bins=30)
        ax.set_title(f"Distribution of {col}")
        buf = BytesIO()
        plt.tight_layout()
        fig.savefig(buf, format="png")
        plt.close(fig)
        buf.seek(0)
        b64 = base64.b64encode(buf.read()).decode("utf-8")
        plots.append({"name": f"hist_{col}", "image_base64": b64})

    if len(df.select_dtypes(include=["number"]).columns) >= 2:
        corr = df.select_dtypes(include=["number"]).corr()
        fig, ax = plt.subplots(figsize=(4, 3))
        cax = ax.imshow(corr, cmap="viridis")
        ax.set_xticks(range(len(corr.columns)))
        ax.set_yticks(range(len(corr.columns)))
        ax.set_xticklabels(corr.columns, rotation=90, fontsize=6)
        ax.set_yticklabels(corr.columns, fontsize=6)
        fig.colorbar(cax)
        plt.tight_layout()
        buf = BytesIO()
        fig.savefig(buf, format="png")
        plt.close(fig)
        buf.seek(0)
        b64 = base64.b64encode(buf.read()).decode("utf-8")
        plots.append({"name": "correlation", "image_base64": b64})

    return plots


# ---------------------------------------------------------------------
# 9. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–∞–Ω-–∞ –Ω–∞ –¥–∏—Å–∫ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
# ---------------------------------------------------------------------
def save_run(run_data: dict, model_pipeline) -> str:
    run_id = str(uuid.uuid4())
    run_dir = os.path.join("runs", run_id)
    os.makedirs(run_dir, exist_ok=True)

    with open(os.path.join(run_dir, "report.json"), "w", encoding="utf-8") as f:
        json.dump(run_data, f, ensure_ascii=False, indent=2)

    if model_pipeline is not None:
        joblib.dump(model_pipeline, os.path.join(run_dir, "model.joblib"))

    return run_id
