from __future__ import annotations

import os
import json
import uuid
import base64
from io import BytesIO
from typing import Optional, Literal

import pandas as pd
import numpy as np
import re

from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, cross_validate
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor

from sklearn.metrics import (
    accuracy_score,
    f1_score,
    mean_squared_error,
    roc_auc_score,
    average_precision_score,
)

import joblib

import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt


# ---------------------------------------------------------------------
# 1. –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π EDA
# ---------------------------------------------------------------------
def basic_eda(df: pd.DataFrame) -> dict:
    eda: dict = {
        "shape": list(df.shape),
        "dtypes": {c: str(df[c].dtype) for c in df.columns},
    }

    eda["nulls"] = {c: int(df[c].isna().sum()) for c in df.columns}
    eda["null_fractions"] = {c: float(df[c].isna().mean()) for c in df.columns}

    numeric_cols = df.select_dtypes(include=["number"]).columns.tolist()
    stats = {}
    for c in numeric_cols[:30]:
        ser = df[c]
        stats[c] = {
            "mean": float(ser.mean()),
            "std": float(ser.std() or 0),
            "min": float(ser.min()),
            "max": float(ser.max()),
        }
    eda["numeric_stats"] = stats

    constant_features = []
    quasi_constant_features = []
    for c in df.columns:
        uniq = df[c].nunique(dropna=True)
        if uniq <= 1:
            constant_features.append(c)
        else:
            top_frac = float(df[c].value_counts(normalize=True, dropna=False).iloc[0])
            if top_frac > 0.98:
                quasi_constant_features.append(c)
    eda["constant_features"] = constant_features
    eda["quasi_constant_features"] = quasi_constant_features

    high_corr_pairs = []
    if len(numeric_cols) >= 2:
        corr = df[numeric_cols].corr().abs()
        for i, c1 in enumerate(numeric_cols):
            for j in range(i + 1, len(numeric_cols)):
                c2 = numeric_cols[j]
                val = float(corr.loc[c1, c2])
                if val >= 0.9:
                    high_corr_pairs.append(
                        {"feature_1": c1, "feature_2": c2, "corr": val}
                    )
    eda["high_corr_pairs"] = high_corr_pairs

    return eda


# ---------------------------------------------------------------------
# 2. —É–≥–∞–¥—ã–≤–∞–Ω–∏–µ —Ç–∞—Ä–≥–µ—Ç–∞ –∏ –∑–∞–¥–∞—á–∏
# ---------------------------------------------------------------------
ID_LIKE = {"id", "ID", "Id", "index", "Rk", "rank"}


def _looks_like_id(colname: str) -> bool:
    return colname in ID_LIKE or re.search(r"id$", colname, re.IGNORECASE) is not None


def _guess_target(
    df: pd.DataFrame,
) -> tuple[Literal["eda", "classification", "regression"], Optional[str]]:
    lower_cols = {c.lower(): c for c in df.columns}

    # —è–≤–Ω—ã–π target / label / class / y
    for cand in ("target", "label", "class", "y"):
        if cand in lower_cols:
            col = lower_cols[cand]
            if df[col].nunique() <= 50:
                return "classification", col
            else:
                return "regression", col

    # "–ø—Å–µ–≤–¥–æ-—Ç–∞—Ä–≥–µ—Ç" –ø–æ –º–∞–ª–µ–Ω—å–∫–æ–º—É —á–∏—Å–ª—É —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    for c in df.columns:
        if _looks_like_id(c):
            continue
        uniq = df[c].nunique(dropna=True)
        if 2 <= uniq <= 30:
            return "classification", c

    # fallback ‚Äî —á–∏—Å–ª–æ–≤–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–π —Ç–∞—Ä–≥–µ—Ç
    num_cols = df.select_dtypes(include=["number"]).columns.tolist()
    for c in num_cols:
        if _looks_like_id(c):
            continue
        if df[c].nunique() > 1:
            return "regression", c

    return "eda", None


def detect_task(df: pd.DataFrame, target: Optional[str] = None) -> dict:
    if target is not None and target in df.columns:
        nunique = df[target].nunique()
        if df[target].dtype == "object" or nunique <= 30:
            task = "classification"
        else:
            task = "regression"
        return {"task": task, "target": target}

    task, tgt = _guess_target(df)
    return {"task": task, "target": tgt}


# ---------------------------------------------------------------------
# 2.1. –ü–æ–¥–±–æ—Ä –∫–∞–Ω–¥–∏–¥–∞—Ç–Ω—ã—Ö —Ç–∞—Ä–≥–µ—Ç–æ–≤
# ---------------------------------------------------------------------
def suggest_targets(
    df: pd.DataFrame,
    problems: dict | None = None,
    current_target: str | None = None,
) -> dict:
    """
    –ü–æ–¥–±–∏—Ä–∞–µ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–Ω—ã–µ —Ç–∞—Ä–≥–µ—Ç—ã –∏–∑ –∫–æ–ª–æ–Ω–æ–∫ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å:
    {
      "current_target": "last_season",
      "default_target": "last_season",
      "candidates": [
        {
          "col": "last_season",
          "type": "classification",  # –∏–ª–∏ "regression"
          "n_unique": 13,
          "dtype": "int64",
          "reason": "...",
        },
        ...
      ],
    }
    """
    problems = problems or {}
    id_like = set(problems.get("id_like") or [])

    candidates: list[dict] = []

    for col in df.columns:
        # –Ω–µ –ø—Ä–µ–¥–ª–∞–≥–∞—Ç—å id-–ø–æ–¥–æ–±–Ω—ã–µ
        if col in id_like:
            continue

        s = df[col]
        nunique = s.nunique(dropna=False)
        dtype = str(s.dtype)

        if nunique <= 1:
            continue

        # —á–∏—Å–ª–æ–≤—ã–µ
        if pd.api.types.is_numeric_dtype(s):
            if 2 <= nunique <= 50:
                candidates.append(
                    {
                        "col": col,
                        "type": "classification",
                        "n_unique": int(nunique),
                        "dtype": dtype,
                        "reason": "—á–∏—Å–ª–æ–≤–æ–π –ø—Ä–∏–∑–Ω–∞–∫ —Å 2‚Äì50 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π ‚Üí –ø–æ–¥—Ö–æ–¥–∏—Ç –∫–∞–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–π —Ç–∞—Ä–≥–µ—Ç",
                    }
                )
            elif nunique > 50:
                candidates.append(
                    {
                        "col": col,
                        "type": "regression",
                        "n_unique": int(nunique),
                        "dtype": dtype,
                        "reason": "—á–∏—Å–ª–æ–≤–æ–π –ø—Ä–∏–∑–Ω–∞–∫ —Å –±–æ–ª—å—à–∏–º —á–∏—Å–ª–æ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π ‚Üí –ø–æ–¥—Ö–æ–¥–∏—Ç –∫–∞–∫ —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–π —Ç–∞—Ä–≥–µ—Ç",
                    }
                )
        # –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ
        else:
            if 2 <= nunique <= 100:
                candidates.append(
                    {
                        "col": col,
                        "type": "classification",
                        "n_unique": int(nunique),
                        "dtype": dtype,
                        "reason": "–∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫ —Å 2‚Äì100 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π ‚Üí –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è",
                    }
                )

    # default_target
    default_target = None
    if current_target and any(c["col"] == current_target for c in candidates):
        default_target = current_target
    else:
        for c in candidates:
            if c["type"] == "classification":
                default_target = c["col"]
                break
        if default_target is None and candidates:
            default_target = candidates[0]["col"]

    return {
        "current_target": current_target,
        "default_target": default_target,
        "candidates": candidates,
    }


# ---------------------------------------------------------------------
# 3. –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ –∫ —á–∏—Å–ª–∞–º –∏ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥
# ---------------------------------------------------------------------
def _coerce_numeric(df: pd.DataFrame) -> pd.DataFrame:
    """
    –ü—ã—Ç–∞–µ–º—Å—è –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å object-–∫–æ–ª–æ–Ω–∫–∏ –≤ —á–∏—Å–ª–∞:
    '1 234,5' ‚Üí 1234.5
    –°–µ–π—á–∞—Å –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –Ω–æ –æ—Å—Ç–∞–≤–ª—è–µ–º –¥–ª—è –±—É–¥—É—â–∏—Ö –∞–ø–¥–µ–π—Ç–æ–≤.
    """
    new_df = df.copy()
    for col in new_df.columns:
        if new_df[col].dtype == "object":
            ser = new_df[col].astype(str).str.replace(",", "").str.replace(" ", "")
            try:
                converted = pd.to_numeric(ser)
            except Exception:
                continue
            else:
                if converted.dtype != "object":
                    new_df[col] = converted
    return new_df


def build_preprocessor(X: pd.DataFrame) -> ColumnTransformer:
    numeric_features = X.select_dtypes(
        include=["int64", "float64", "int32", "float32"]
    ).columns.tolist()
    categorical_features = [c for c in X.columns if c not in numeric_features]

    numeric_transformer = Pipeline(
        steps=[("imputer", SimpleImputer(strategy="median"))]
    )

    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore")),
        ]
    )

    return ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ]
    )


# ---------------------------------------------------------------------
# 4. –æ–±—É—á–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
# ---------------------------------------------------------------------
def train_baseline(
    df: pd.DataFrame,
    target: str,
    task: str,
    problems: dict | None = None,
    return_model: bool = False,
) -> Optional[dict]:
    """
    –û–±—É—á–∞–µ–º RandomForest (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è/—Ä–µ–≥—Ä–µ—Å—Å–∏—è) —Å —É—á—ë—Ç–æ–º:
    - class_weight –ø—Ä–∏ –¥–∏—Å–±–∞–ª–∞–Ω—Å–µ
    - –¥—Ä–æ–ø–∞ id-–ø–æ–¥–æ–±–Ω—ã—Ö –∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
    """
    try:
        if target not in df.columns:
            return {"model_type": "skipped", "reason": f"–ö–æ–ª–æ–Ω–∫–∞ '{target}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞."}

        problems = problems or {}

        # 0. —É–±–∏—Ä–∞–µ–º —Å—Ç—Ä–æ–∫–∏ –±–µ–∑ —Ç–∞—Ä–≥–µ—Ç–∞
        df = df[~df[target].isna()].copy()
        if df.shape[0] < 20:
            return {"model_type": "skipped", "reason": "–ú–∞–ª–æ —Å—Ç—Ä–æ–∫ –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è NaN –≤ —Ç–∞—Ä–≥–µ—Ç–µ."}

        # 1. —á—Ç–æ –¥—Ä–æ–ø–∞–µ–º (id + –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã)
        drop_cols: list[str] = []
        for c in problems.get("id_like", []) or []:
            if c in df.columns and c != target:
                drop_cols.append(c)
        for c in problems.get("constant_features", []) or []:
            if c in df.columns and c != target and c not in drop_cols:
                drop_cols.append(c)

        # 2. X, y
        X = df.drop(columns=[target] + drop_cols)
        y = df[target]

        if X.shape[1] == 0:
            return {"model_type": "skipped", "reason": "–ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤."}

        preprocessor = build_preprocessor(X)

        # ===== –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è =====
        if task == "classification":
            if y.nunique() < 2:
                return {"model_type": "skipped", "reason": "–í —Ç–∞—Ä–≥–µ—Ç–µ –æ–¥–∏–Ω –∫–ª–∞—Å—Å."}

            rf_kwargs = dict(n_estimators=200, random_state=42, n_jobs=-1)

            used_class_weight = False
            if problems.get("class_imbalance"):
                rf_kwargs["class_weight"] = "balanced"
                used_class_weight = True

            model = RandomForestClassifier(**rf_kwargs)

            # –º–æ–∂–Ω–æ –ª–∏ —Å—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å
            counts = y.value_counts(dropna=False)
            can_stratify = (counts >= 2).all()

            X_train, X_val, y_train, y_val = train_test_split(
                X,
                y,
                test_size=0.2,
                random_state=42,
                stratify=y if can_stratify else None,
            )

            pipe = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
            pipe.fit(X_train, y_train)
            preds = pipe.predict(X_val)

            acc = float(accuracy_score(y_val, preds))
            f1 = float(f1_score(y_val, preds, average="weighted"))

            res: dict = {
                "model_type": "RandomForestClassifier",
                "accuracy": acc,
                "f1": f1,
                "training_log": {
                    "dropped_columns": drop_cols,
                    "used_class_weight": used_class_weight,
                    "stratified_split": bool(can_stratify),
                },
            }

            # –±–∏–Ω–∞—Ä–∫–∞ ‚Üí ROC-AUC
            if y_val.nunique() == 2:
                try:
                    proba = pipe.predict_proba(X_val)[:, 1]
                    res["roc_auc"] = float(roc_auc_score(y_val, proba))
                except Exception:
                    pass

            if return_model:
                res["pipeline"] = pipe

            return res

        # ===== —Ä–µ–≥—Ä–µ—Å—Å–∏—è =====
        elif task == "regression":
            model = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)
            X_train, X_val, y_train, y_val = train_test_split(
                X, y, test_size=0.2, random_state=42
            )
            pipe = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
            pipe.fit(X_train, y_train)
            preds = pipe.predict(X_val)

            mse = float(mean_squared_error(y_val, preds))
            rmse = mse ** 0.5

            res = {
                "model_type": "RandomForestRegressor",
                "rmse": rmse,
                "training_log": {
                    "dropped_columns": drop_cols,
                    "used_class_weight": False,
                    "stratified_split": False,
                },
            }

            if return_model:
                res["pipeline"] = pipe

            return res

        # –µ—Å–ª–∏ –∑–∞–¥–∞—á–∞ –Ω–µ–ø–æ–Ω—è—Ç–Ω–∞
        return {"model_type": "skipped", "reason": "–ó–∞–¥–∞—á–∞ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞."}

    except Exception as e:
        return {"model_type": "skipped", "reason": f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {e}"}


# ---------------------------------------------------------------------
# 4.1. –ü–æ–∏—Å–∫ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ (auto-modeling)
# ---------------------------------------------------------------------
def auto_model_search(
    df: pd.DataFrame,
    task: dict,
    problems: dict | None = None,
    return_pipeline: bool = True,
) -> Optional[dict]:
    """
    –ü–µ—Ä–µ–±–∏—Ä–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π (–∫–∞–∫ —Å–¥–µ–ª–∞–ª –±—ã –º–∏–¥–ª-DS),
    —Å—á–∏—Ç–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∏ –≤—ã–±–∏—Ä–∞–µ–º –ª—É—á—à—É—é.

    –¢–µ–ø–µ—Ä—å:
    - –µ—Å—Ç—å holdout-–º–µ—Ç—Ä–∏–∫–∏ (train/val —Å–ø–ª–∏—Ç)
    - –µ—Å—Ç—å k-fold cross-validation (3 —Ñ–æ–ª–¥–∞)
    - primary_score:
        * –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ ‚Äî RMSE (—Å–æ –∑–Ω–∞–∫–æ–º –º–∏–Ω—É—Å)
        * –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ ‚Äî f1_weighted
          –∏–ª–∏ PR-AUC, –µ—Å–ª–∏ –µ—Å—Ç—å –¥–∏—Å–±–∞–ª–∞–Ω—Å –∏ 2 –∫–ª–∞—Å—Å–∞
    """
    problems = problems or {}
    task_type = task.get("task")
    target = task.get("target")

    if task_type == "eda" or not target or target not in df.columns:
        return None

    # 0. —É–±–∏—Ä–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN –≤ —Ç–∞—Ä–≥–µ—Ç–µ
    df_clean = df[~df[target].isna()].copy()
    if df_clean.shape[0] < 50:
        return {
            "best_model": {
                "model_type": "skipped",
                "reason": "–ú–∞–ª–æ —Å—Ç—Ä–æ–∫ –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è NaN –≤ —Ç–∞—Ä–≥–µ—Ç–µ (<50).",
            },
            "leaderboard": [],
            "pipeline": None,
        }

    # 1. –¥—Ä–æ–ø–∞–µ–º id-–ø–æ–¥–æ–±–Ω—ã–µ –∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã
    drop_cols: list[str] = []
    for c in problems.get("id_like", []) or []:
        if c in df_clean.columns and c != target:
            drop_cols.append(c)
    for c in problems.get("constant_features", []) or []:
        if c in df_clean.columns and c != target and c not in drop_cols:
            drop_cols.append(c)

    X = df_clean.drop(columns=[target] + drop_cols)
    y = df_clean[target]

    if X.shape[1] == 0:
        return {
            "best_model": {
                "model_type": "skipped",
                "reason": "–ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.",
            },
            "leaderboard": [],
            "pipeline": None,
        }

    preprocessor = build_preprocessor(X)

    # -------------------------------------------------------------
    # 2. holdout + CV-—Å—Ç—Ä–∞—Ç–µ–≥–∏—è + –≤—ã–±–æ—Ä –æ—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏
    # -------------------------------------------------------------
    primary_metric_name: str

    if task_type == "classification":
        counts = y.value_counts(dropna=False)
        can_stratify = (counts >= 2).all()

        n_classes = int(y.nunique())
        has_imbalance = bool(problems.get("class_imbalance"))

        # –µ—Å–ª–∏ –¥–∏—Å–±–∞–ª–∞–Ω—Å –∏ –±–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è ‚Üí –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º PR-AUC
        if has_imbalance and n_classes == 2:
            primary_metric_name = "pr_auc"
        else:
            primary_metric_name = "f1"

        # holdout-—Å–ø–ª–∏—Ç
        X_train, X_val, y_train, y_val = train_test_split(
            X,
            y,
            test_size=0.2,
            random_state=42,
            stratify=y if can_stratify else None,
        )

        # CV-–æ–±—ä–µ–∫—Ç
        if can_stratify:
            cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
        else:
            cv = KFold(n_splits=3, shuffle=True, random_state=42)

        # –∫–∞–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ —Å—á–∏—Ç–∞–µ–º –Ω–∞ CV
        clf_scoring = {
            "f1_weighted": "f1_weighted",
            "accuracy": "accuracy",
        }
        if n_classes == 2:
            clf_scoring["roc_auc"] = "roc_auc"
            clf_scoring["pr_auc"] = "average_precision"

    else:
        can_stratify = False
        primary_metric_name = "rmse"
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        cv = KFold(n_splits=3, shuffle=True, random_state=42)
        reg_scoring = {"mse": "neg_mean_squared_error"}

    # -------------------------------------------------------------
    # 3. –°–ø–∏—Å–æ–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
    # -------------------------------------------------------------
    models_cfg: list[dict] = []

    if task_type == "classification":
        # –µ—Å–ª–∏ –Ω–∞—à–ª–∏ –¥–∏—Å–±–∞–ª–∞–Ω—Å ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º class_weight='balanced'
        class_weight = "balanced" if problems.get("class_imbalance") else None

        models_cfg.append(
            {
                "name": "LogisticRegression",
                "estimator": LogisticRegression(
                    max_iter=2000,
                    n_jobs=-1,
                    class_weight=class_weight,  # –º–æ–∂–µ—Ç –±—ã—Ç—å None –∏–ª–∏ 'balanced'
                ),
            }
        )

        rf_params = dict(n_estimators=200, random_state=42, n_jobs=-1)
        if class_weight is not None:
            rf_params["class_weight"] = class_weight
        models_cfg.append(
            {
                "name": "RandomForestClassifier",
                "estimator": RandomForestClassifier(**rf_params),
            }
        )

        models_cfg.append(
            {
                "name": "GradientBoostingClassifier",
                "estimator": GradientBoostingClassifier(),
            }
        )

    elif task_type == "regression":
        models_cfg.extend(
            [
                {
                    "name": "LinearRegression",
                    "estimator": LinearRegression(),
                },
                {
                    "name": "RandomForestRegressor",
                    "estimator": RandomForestRegressor(
                        n_estimators=200, random_state=42, 
                    ),
                },
                {
                    "name": "GradientBoostingRegressor",
                    "estimator": GradientBoostingRegressor(),
                },
            ]
        )
    else:
        return {
            "best_model": {
                "model_type": "skipped",
                "reason": f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –∑–∞–¥–∞—á–∏: {task_type}",
            },
            "leaderboard": [],
            "pipeline": None,
        }

    # -------------------------------------------------------------
    # 4. –û–±—É—á–∞–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ + —Å—á–∏—Ç–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
    # -------------------------------------------------------------
    leaderboard: list[dict] = []
    best_score: Optional[float] = None
    best_entry: Optional[dict] = None
    best_pipeline = None

    for cfg in models_cfg:
        est = cfg["estimator"]
        name = cfg["name"]

        pipe = Pipeline(
            steps=[
                ("preprocess", preprocessor),
                ("model", est),
            ]
        )

        try:
            # 1) holdout-–º–µ—Ç—Ä–∏–∫–∏
            pipe.fit(X_train, y_train)
        except Exception as e:
            leaderboard.append(
                {
                    "name": name,
                    "model_type": est.__class__.__name__,
                    "status": "failed",
                    "error": str(e),
                }
            )
            continue

        metrics: dict = {
            "name": name,
            "model_type": est.__class__.__name__,
            "status": "ok",
        }

        # --- holdout ---
        if task_type == "classification":
            y_pred = pipe.predict(X_val)
            acc = float(accuracy_score(y_val, y_pred))
            f1w = float(f1_score(y_val, y_pred, average="weighted"))

            metrics["val_accuracy"] = acc
            metrics["val_f1"] = f1w

            # –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å—á–∏—Ç–∞–µ–º ROC-AUC –∏ PR-AUC
            if y_val.nunique() == 2 and hasattr(pipe, "predict_proba"):
                try:
                    proba = pipe.predict_proba(X_val)[:, 1]
                    roc = float(roc_auc_score(y_val, proba))
                    pr = float(average_precision_score(y_val, proba))
                    metrics["val_roc_auc"] = roc
                    metrics["val_pr_auc"] = pr
                except Exception:
                    pass

            # –≤—Ä–µ–º–µ–Ω–Ω–æ —Å—Ç–∞–≤–∏–º primary = F1, –ø–æ—Ç–æ–º –º–æ–∂–µ–º –ø–µ—Ä–µ–∑–∞–ø–∏—Å–∞—Ç—å –ø–æ—Å–ª–µ CV
            primary_score = f1w

        else:  # regression
            y_pred = pipe.predict(X_val)
            mse = float(mean_squared_error(y_val, y_pred))
            rmse = mse ** 0.5
            metrics["val_rmse"] = rmse
            primary_score = -rmse  # –º–µ–Ω—å—à–µ rmse ‚Üí –ª—É—á—à–µ

        # --- cross-validation ---
        try:
            if task_type == "classification":
                cv_res = cross_validate(
                    pipe,
                    X,
                    y,
                    cv=cv,
                    scoring=clf_scoring,
                    n_jobs=-1,
                )

                if "f1_weighted" in clf_scoring:
                    cv_f1 = cv_res["test_f1_weighted"]
                    metrics["cv_f1_mean"] = float(cv_f1.mean())
                    metrics["cv_f1_std"] = float(cv_f1.std())

                if "accuracy" in clf_scoring:
                    cv_acc = cv_res["test_accuracy"]
                    metrics["cv_accuracy_mean"] = float(cv_acc.mean())
                    metrics["cv_accuracy_std"] = float(cv_acc.std())

                if "pr_auc" in clf_scoring and "test_pr_auc" in cv_res:
                    cv_pr = cv_res["test_pr_auc"]
                    metrics["cv_pr_auc_mean"] = float(cv_pr.mean())
                    metrics["cv_pr_auc_std"] = float(cv_pr.std())

                if "roc_auc" in clf_scoring and "test_roc_auc" in cv_res:
                    cv_roc = cv_res["test_roc_auc"]
                    metrics["cv_roc_auc_mean"] = float(cv_roc.mean())
                    metrics["cv_roc_auc_std"] = float(cv_roc.std())

                # —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –≤—ã–±–æ—Ä primary-–º–µ—Ç—Ä–∏–∫–∏
                if primary_metric_name == "pr_auc" and "cv_pr_auc_mean" in metrics:
                    primary_score = metrics["cv_pr_auc_mean"]
                elif "cv_f1_mean" in metrics:
                    primary_score = metrics["cv_f1_mean"]
                elif "cv_accuracy_mean" in metrics:
                    primary_score = metrics["cv_accuracy_mean"]

            else:  # regression
                cv_res = cross_validate(
                    pipe,
                    X,
                    y,
                    cv=cv,
                    scoring=reg_scoring,
                    n_jobs=-1,
                )
                cv_mse = -cv_res["test_mse"]
                cv_rmse = np.sqrt(cv_mse)
                metrics["cv_rmse_mean"] = float(cv_rmse.mean())
                metrics["cv_rmse_std"] = float(cv_rmse.std())
                primary_score = -metrics["cv_rmse_mean"]
        except Exception as e:
            # –µ—Å–ª–∏ CV —É–ø–∞–ª, –æ—Å—Ç–∞—ë–º—Å—è —Ç–æ–ª—å–∫–æ —Å holdout-–º–µ—Ç—Ä–∏–∫–∞–º–∏
            metrics["cv_error"] = str(e)

        metrics["primary_score"] = primary_score
        leaderboard.append(metrics)

        if (best_score is None) or (primary_score > best_score):
            best_score = primary_score
            best_entry = metrics
            best_pipeline = pipe

    leaderboard_sorted = sorted(
        leaderboard, key=lambda x: x.get("primary_score", float("-inf")), reverse=True
    )

    if best_entry is None:
        return {
            "best_model": {
                "model_type": "skipped",
                "reason": "–í—Å–µ –∫–∞–Ω–¥–∏–¥–∞—Ç—ã —É–ø–∞–ª–∏ –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏.",
            },
            "leaderboard": leaderboard_sorted,
            "pipeline": None,
        }

    best_model = dict(best_entry)
    best_model["training_log"] = {
        "dropped_columns": drop_cols,
        "stratified_split": bool(can_stratify),
        "used_class_weight": bool(
            task_type == "classification" and problems.get("class_imbalance")
        ),
        "primary_metric": primary_metric_name,
    }

    if not return_pipeline:
        best_pipeline = None

    return {
        "best_model": best_model,
        "leaderboard": leaderboard_sorted,
        "pipeline": best_pipeline,
    }


# ---------------------------------------------------------------------
# 5. –æ—Ç—á—ë—Ç (—Ç–µ–∫—Å—Ç –¥–ª—è UI –∏ markdown)
# ---------------------------------------------------------------------
def build_report(
    df: pd.DataFrame,
    eda: dict,
    task: dict,
    model: dict | None,
    problems: dict | None = None,
) -> str:
    problems = problems or {}
    lines: list[str] = []

    rows, cols = eda.get("shape", (len(df), df.shape[1]))
    lines.append("üì¶ –î–∞–Ω–Ω—ã–µ")
    lines.append(f"‚Ä¢ –†–∞–∑–º–µ—Ä: {rows} —Å—Ç—Ä–æ–∫ √ó {cols} –∫–æ–ª–æ–Ω–æ–∫.")

    nulls = eda.get("nulls", {})
    nz = {k: v for k, v in nulls.items() if v > 0}
    if nz:
        lines.append("‚Ä¢ –ü—Ä–æ–ø—É—Å–∫–∏ (—Ç–æ–ø):")
        for k, v in list(nz.items())[:8]:
            lines.append(f"   - {k}: {v}")

    num_stats = eda.get("numeric_stats", {})
    if num_stats:
        lines.append("‚Ä¢ –ß–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (mean / std / min / max):")
        for name, st in list(num_stats.items())[:8]:
            lines.append(
                f"   - {name}: {st['mean']:.3f}/{st['std']:.3f}/{st['min']}/{st['max']}"
            )

    lines.append("")
    lines.append("üß© –ü—Ä–æ–±–ª–µ–º—ã –≤ –¥–∞–Ω–Ω—ã—Ö")
    any_problems = (
        problems.get("constant_features")
        or problems.get("quasi_constant_features")
        or problems.get("high_corr_pairs")
        or problems.get("high_null_features")
        or problems.get("target_has_nan")
        or problems.get("class_imbalance")
        or problems.get("high_cardinality")
    )
    if not any_problems:
        lines.append("‚Ä¢ –Ø–≤–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º –Ω–µ –Ω–∞–π–¥–µ–Ω–æ ‚úÖ")
    else:
        if problems.get("target_has_nan"):
            info = problems["target_has_nan"]
            lines.append(
                f"‚Ä¢ –í —Ç–∞—Ä–≥–µ—Ç–µ {info['column']} –µ—Å—Ç—å {info['nan_count']} –ø—Ä–æ–ø—É—Å–∫–æ–≤ ‚Äî —É–±—Ä–∞—Ç—å –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º."
            )
        consts = problems.get("constant_features") or []
        if consts:
            lines.append("‚Ä¢ –ö–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: " + ", ".join(consts[:8]))
        qconsts = problems.get("quasi_constant_features") or []
        if qconsts:
            lines.append("‚Ä¢ –ü–æ—á—Ç–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ: " + ", ".join(qconsts[:8]))
        corr_pairs = problems.get("high_corr_pairs") or []
        if corr_pairs:
            short = [f"{a}‚Üî{b} ({c:.2f})" for a, b, c in corr_pairs[:6]]
            lines.append("‚Ä¢ –°–∏–ª—å–Ω–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è: " + ", ".join(short))
        high_nulls = problems.get("high_null_features") or {}
        if high_nulls:
            show = [f"{k} ({v:.1f}%)" for k, v in list(high_nulls.items())[:6]]
            lines.append("‚Ä¢ –ú–Ω–æ–≥–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤: " + ", ".join(show))
        if problems.get("class_imbalance"):
            ci = problems["class_imbalance"]
            lines.append(
                f"‚Ä¢ –î–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤: {ci['max_class']}:{ci['min_class']} ‚âà {ci['ratio']:.1f}"
            )
        if problems.get("high_cardinality"):
            cols = [f"{x['column']} ({x['n_unique']})" for x in problems["high_cardinality"][:4]]
            lines.append("‚Ä¢ –í—ã—Å–æ–∫–∞—è –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å: " + ", ".join(cols))

    lines.append("")
    lines.append("ü§ñ –ú–æ–¥–µ–ª—å")
    if task.get("task") == "eda" or not task.get("target"):
        lines.append("‚Ä¢ –¶–µ–ª–µ–≤–æ–π –ø—Ä–∏–∑–Ω–∞–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω ‚Äî –æ–±—É—á–∞—Ç—å –Ω–µ—á–µ–≥–æ.")
    elif model is None:
        lines.append("‚Ä¢ –ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–∞–ª–∞—Å—å (–Ω–µ–≤–µ—Ä–Ω—É–ª—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç).")
    elif model.get("model_type") == "skipped":
        lines.append("‚Ä¢ –ú–æ–¥–µ–ª—å –ø—Ä–æ–ø—É—â–µ–Ω–∞.")
        if model.get("reason"):
            lines.append("‚Ä¢ –ü—Ä–∏—á–∏–Ω–∞: " + model["reason"])
    else:
        lines.append(f"‚Ä¢ –ó–∞–¥–∞—á–∞: {task['task']} –ø–æ –∫–æ–ª–æ–Ω–∫–µ ‚Äú{task['target']}‚Äù.")
        lines.append(f"‚Ä¢ –ú–æ–¥–µ–ª—å: {model.get('model_type')}.")
        if "accuracy" in model:
            lines.append(f"‚Ä¢ accuracy = {model['accuracy']:.3f}")
        if "f1" in model:
            lines.append(f"‚Ä¢ f1 = {model['f1']:.3f}")
        if "roc_auc" in model:
            lines.append(f"‚Ä¢ ROC-AUC = {model['roc_auc']:.3f}")
        if "rmse" in model:
            lines.append(f"‚Ä¢ RMSE = {model['rmse']:.3f}")

    return "\n".join(lines)


def _detect_class_imbalance_advanced(y: pd.Series) -> dict | None:
    """
    –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞:
    - ratio majority/minority
    - severity: none / moderate / heavy / extreme
    - suggest_redefine_target=True –ø—Ä–∏ –æ—á–µ–Ω—å —Ç—è–∂—ë–ª–æ–π –∑–∞–¥–∞—á–µ

    –í–ê–ñ–ù–û: –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏ –Ω–æ–≤—ã–µ –ø–æ–ª—è (majority_class/...), –∏ —Å—Ç–∞—Ä—ã–µ –∞–ª–∏–∞—Å—ã
    (max_class/min_class/...), —á—Ç–æ–±—ã –Ω–µ –ª–æ–º–∞—Ç—å –æ—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥.
    """
    from collections import Counter

    y_arr = np.array(y)
    cnt = Counter(y_arr)

    if len(cnt) <= 1:
        return None

    total = sum(cnt.values())
    majority_class, majority_cnt = max(cnt.items(), key=lambda x: x[1])
    minority_class, minority_cnt = min(cnt.items(), key=lambda x: x[1])

    ratio = majority_cnt / max(1, minority_cnt)

    if ratio < 3:
        severity = "none"
    elif ratio < 10:
        severity = "moderate"
    elif ratio < 30:
        severity = "heavy"
    else:
        severity = "extreme"

    n_classes = len(cnt)
    suggest_redefine = severity in ("heavy", "extreme") and n_classes >= 5

    return {
        # –Ω–æ–≤—ã–µ –ø–æ–ª—è
        "found": ratio >= 3,
        "ratio": float(ratio),
        "majority_class": str(majority_class),
        "majority_count": int(majority_cnt),
        "minority_class": str(minority_class),
        "minority_count": int(minority_cnt),
        "total": int(total),
        "n_classes": int(n_classes),
        "severity": severity,
        "suggest_redefine_target": suggest_redefine,

        # –∞–ª–∏–∞—Å—ã –¥–ª—è —Å—Ç–∞—Ä–æ–≥–æ –∫–æ–¥–∞ (—á—Ç–æ–±—ã –Ω–µ –ø–∞–¥–∞–ª KeyError)
        "max_class": str(majority_class),
        "max_count": int(majority_cnt),
        "min_class": str(minority_class),
        "min_count": int(minority_cnt),
    }


# ---------------------------------------------------------------------
# 6. –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–±–ª–µ–º –¥–∞—Ç–∞—Å–µ—Ç–∞
# ---------------------------------------------------------------------
def analyze_dataset(df: pd.DataFrame, task: dict) -> dict:
    problems: dict[str, object] = {}

    # ID / –∫–ª—é—á–∏
    id_like_cols: list[str] = []
    n_rows = len(df)
    for col in df.columns:
        col_l = col.lower()
        nunique = df[col].nunique(dropna=True)

        name_looks_like_id = (
            col_l == "id"
            or col_l.endswith("_id")
            or col_l in ("index", "idx", "rk", "rank")
        )
        value_looks_like_id = nunique > 0.9 * n_rows

        if name_looks_like_id or value_looks_like_id:
            id_like_cols.append(col)

    if id_like_cols:
        problems["id_like"] = id_like_cols

    # –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã
    constant_cols = []
    quasi_constant_cols = []
    for col in df.columns:
        nunique = df[col].nunique(dropna=True)
        if nunique <= 1:
            constant_cols.append(col)
        else:
            top_frac = float(df[col].value_counts(normalize=True, dropna=False).iloc[0])
            if top_frac > 0.98:
                quasi_constant_cols.append(col)
    if constant_cols:
        problems["constant_features"] = constant_cols
    if quasi_constant_cols:
        problems["quasi_constant_features"] = quasi_constant_cols

    # –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
    num_df = df.select_dtypes(include=["number"])
    high_corr_pairs = []
    if num_df.shape[1] >= 2:
        corr = num_df.corr().abs()
        cols = corr.columns.tolist()
        for i in range(len(cols)):
            for j in range(i + 1, len(cols)):
                if corr.iloc[i, j] >= 0.9:
                    high_corr_pairs.append((cols[i], cols[j], float(corr.iloc[i, j])))
    if high_corr_pairs:
        problems["high_corr_pairs"] = high_corr_pairs

    # –º–Ω–æ–≥–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤
    null_perc = (df.isna().sum() / max(1, n_rows) * 100).sort_values(ascending=False)
    high_nulls = null_perc[null_perc > 30].to_dict()
    if high_nulls:
        problems["high_null_features"] = {k: float(v) for k, v in high_nulls.items()}

    # NaN –≤ —Ç–∞—Ä–≥–µ—Ç–µ
    target = task.get("target")
    if target and target in df.columns:
        n_nan_target = int(df[target].isna().sum())
        if n_nan_target > 0:
            problems["target_has_nan"] = {"column": target, "nan_count": n_nan_target}

    # –¥–∏—Å–±–∞–ª–∞–Ω—Å
    if task.get("task") == "classification" and target and target in df.columns:
        ci = _detect_class_imbalance_advanced(df[target])
        if ci is not None and ci["found"]:
            problems["class_imbalance"] = ci

    # –≤—ã—Å–æ–∫–∞—è –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å
    high_cardinality = []
    for col in df.select_dtypes(include=["object"]).columns:
        nunique = df[col].nunique(dropna=True)
        if nunique > 200 and col not in problems.get("id_like", []):
            high_cardinality.append({"column": col, "n_unique": int(nunique)})
    if high_cardinality:
        problems["high_cardinality"] = high_cardinality

    return problems


# ---------------------------------------------------------------------
# 6.0 —Ä–æ–ª–∏ –∫–æ–ª–æ–Ω–æ–∫
# ---------------------------------------------------------------------
def detect_column_roles(df: pd.DataFrame, task: dict | None = None) -> dict:
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ–º "—Ä–æ–ª—å" –∫–∞–∂–¥–æ–π –∫–æ–ª–æ–Ω–∫–∏:
      id / datetime / bool / numeric / categorical / text
    + —Ñ–ª–∞–≥ –≤–æ–∑–º–æ–∂–Ω–æ–π —É—Ç–µ—á–∫–∏ –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö —Ñ–∏—á, —Å–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—â–∏—Ö —Å —Ç–∞—Ä–≥–µ—Ç–æ–º.
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º dict[col_name] = {...}.
    """
    task = task or {}
    target = task.get("target")
    n_rows = len(df)

    roles: dict[str, dict] = {}

    for col in df.columns:
        ser = df[col]
        info: dict[str, object] = {
            "role": None,
            "n_unique": int(ser.nunique(dropna=True)),
            "has_missing": bool(ser.isna().any()),
        }

        role: str | None = None
        nunique = info["n_unique"]

        # 1) id-–ø–æ–¥–æ–±–Ω—ã–µ
        if _looks_like_id(col) or (n_rows > 0 and nunique > 0.9 * n_rows):
            role = "id"

        # 2) datetime
        if role is None:
            if pd.api.types.is_datetime64_any_dtype(ser):
                role = "datetime"
            elif ser.dtype == "object":
                if re.search(r"(date|time|timestamp)", col, re.IGNORECASE):
                    try:
                        parsed = pd.to_datetime(ser, errors="coerce")
                        if parsed.notna().mean() > 0.7:
                            role = "datetime"
                    except Exception:
                        pass

        # 3) bool
        if role is None:
            if pd.api.types.is_bool_dtype(ser):
                role = "bool"
            else:
                uniques = pd.Series(ser.dropna().unique())
                if 0 < len(uniques) <= 2:
                    role = "bool"

        # 4) numeric / text / categorical
        if role is None:
            if pd.api.types.is_numeric_dtype(ser):
                role = "numeric"
            else:
                try:
                    avg_len = float(
                        ser.dropna().astype(str).str.len().mean() or 0.0
                    )
                except Exception:
                    avg_len = 0.0

                if avg_len > 50 and nunique > 30:
                    role = "text"
                else:
                    role = "categorical"

        info["role"] = role

        # 5) –≤–æ–∑–º–æ–∂–Ω–∞—è —É—Ç–µ—á–∫–∞ (–¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö —Ç–∞—Ä–≥–µ—Ç–æ–≤)
        if target and target in df.columns and col != target:
            tgt = df[target]
            if pd.api.types.is_numeric_dtype(ser) and pd.api.types.is_numeric_dtype(tgt):
                try:
                    corr = float(tgt.corr(ser))
                    if abs(corr) > 0.98:
                        info["possible_leakage"] = True
                        info["leakage_corr"] = corr
                    else:
                        info["possible_leakage"] = False
                except Exception:
                    pass

        roles[col] = info

    return roles


# ---------------------------------------------------------------------
# 6.1 –æ—Ü–µ–Ω–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è –¥–∞—Ç–∞—Å–µ—Ç–∞
# ---------------------------------------------------------------------
def evaluate_dataset_health(eda: dict, problems: dict) -> dict:
    score = 100
    reasons: list[str] = []

    if problems.get("high_null_features"):
        score -= 15
        reasons.append("–º–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏ (>30%)")

    if problems.get("constant_features"):
        score -= 10
        reasons.append("–µ—Å—Ç—å –ø–æ–ª–Ω–æ—Å—Ç—å—é –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏")

    if problems.get("high_corr_pairs"):
        score -= 10
        reasons.append("–µ—Å—Ç—å —Å–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏")

    if problems.get("class_imbalance"):
        score -= 20
        reasons.append("—Å–∏–ª—å–Ω—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤")

    if problems.get("high_cardinality"):
        score -= 5
        reasons.append("–∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ —Å –æ—á–µ–Ω—å –±–æ–ª—å—à–∏–º —á–∏—Å–ª–æ–º –∑–Ω–∞—á–µ–Ω–∏–π")

    score = max(30, min(100, score))

    if score >= 80:
        level = "green"
    elif score >= 55:
        level = "yellow"
    else:
        level = "red"

    return {
        "score": score,
        "level": level,
        "reasons": reasons,
    }


# ---------------------------------------------------------------------
# 6.2 –ø–ª–∞–Ω —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –∫–∞–∫ —É mid+ DS
# ---------------------------------------------------------------------
def build_experiment_plan(
    task: dict | None,
    problems: dict | None,
    dataset_health: dict | None,
    model: dict | None,
    column_roles: dict | None,
) -> list[dict]:
    """
    –°—Ç—Ä–æ–∏–º –ø–ª–∞–Ω —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –∫–∞–∫ —Å–¥–µ–ª–∞–ª –±—ã mid+ DS.
    –§–æ—Ä–º–∞—Ç —à–∞–≥–∞:
    {
      "priority": "now" | "next" | "later",
      "title": "–ö—Ä–∞—Ç–∫–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫",
      "description": "–ß—Ç–æ —Å–¥–µ–ª–∞—Ç—å –∏ –∑–∞—á–µ–º",
      "tags": ["data", "model", "metrics"]
    }
    """
    task = task or {}
    problems = problems or {}
    dataset_health = dataset_health or {}
    column_roles = column_roles or {}

    plan: list[dict] = []

    def add(priority, title, description, tags=None):
        plan.append({
            "priority": priority,
            "title": title,
            "description": description,
            "tags": tags or [],
        })

    # --- NOW: –∫—Ä–∏—Ç–∏—á–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã —Å –¥–∞–Ω–Ω—ã–º–∏ ---

    if problems.get("target_has_nan"):
        info = problems["target_has_nan"]
        add(
            "now",
            "–û—á–∏—Å—Ç–∏—Ç—å —Ç–∞—Ä–≥–µ—Ç –æ—Ç NaN",
            f"–í –∫–æ–ª–æ–Ω–∫–µ {info.get('column')} {info.get('nan_count')} –ø—Ä–æ–ø—É—Å–∫–æ–≤. "
            "–£–¥–∞–ª–∏ —Å—Ç—Ä–æ–∫–∏ —Å NaN –∏–ª–∏ –∑–∞–ø–æ–ª–Ω–∏ –∏—Ö, –∏–Ω–∞—á–µ –º–µ—Ç—Ä–∏–∫–∏ –±—É–¥—É—Ç –∏—Å–∫–∞–∂–µ–Ω—ã.",
            ["data", "target"],
        )

    if problems.get("high_null_features"):
        add(
            "now",
            "–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –±–æ–ª—å—à–∏–º–∏ –ø—Ä–æ–ø—É—Å–∫–∞–º–∏",
            "–ï—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å >30% –ø—Ä–æ–ø—É—Å–∫–æ–≤. –†–µ—à–∏: –¥—Ä–æ–ø–Ω—É—Ç—å –∏—Ö, –∑–∞–∏–º–ø—É—Ç–∏—Ç—å –∏–ª–∏ –¥–æ–±–∞–≤–∏—Ç—å —Ñ–ª–∞–≥–∏ 'is_null'.",
            ["data", "missing"],
        )

    # –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤
    if problems.get("class_imbalance") and task.get("task") == "classification":
        add(
            "now",
            "–°–¥–µ–ª–∞—Ç—å —É—Å—Ç–æ–π—á–∏–≤—ã–π train/test –ø—Ä–∏ –¥–∏—Å–±–∞–ª–∞–Ω—Å–µ",
            "–ò—Å–ø–æ–ª—å–∑—É–π stratify –≤ train_test_split, class_weight='balanced' –∏–ª–∏ oversampling (SMOTE/RandomOverSampler).",
            ["data", "imbalance"],
        )

    # –≤–æ–∑–º–æ–∂–Ω–∞—è —É—Ç–µ—á–∫–∞ —Ñ–∏—á
    leak_cols = [
        c for c, info in column_roles.items()
        if isinstance(info, dict) and info.get("possible_leakage")
    ]
    if leak_cols:
        add(
            "now",
            "–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≤–æ–∑–º–æ–∂–Ω—É—é —É—Ç–µ—á–∫—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤",
            "–ù–∞–π–¥–µ–Ω—ã –ø—Ä–∏–∑–Ω–∞–∫–∏, –ø–æ—á—Ç–∏ –∏–¥–µ–∞–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—â–∏–µ —Å —Ç–∞—Ä–≥–µ—Ç–æ–º: "
            + ", ".join(leak_cols[:5])
            + ". –£–±–µ–¥–∏—Å—å, —á—Ç–æ —ç—Ç–æ –Ω–µ ¬´target –≤ –¥—Ä—É–≥–æ–π —Ñ–æ—Ä–º–µ¬ª.",
            ["data", "leakage"],
        )

    # --- NEXT: —Ñ–∏—á–∏ –∏ –º–æ–¥–µ–ª–∏ ---

    # high cardinality ‚Üí CatBoost / target encoding
    if problems.get("high_cardinality"):
        add(
            "next",
            "–û–±—Ä–∞–±–æ—Ç–∞—Ç—å high-cardinality –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏",
            "–î–ª—è –∫–æ–ª–æ–Ω–æ–∫ —Å –æ—á–µ–Ω—å –±–æ–ª—å—à–∏–º —á–∏—Å–ª–æ–º –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ–ø—Ä–æ–±—É–π CatBoost –∏–ª–∏ target/frequency encoding, "
            "—á—Ç–æ–±—ã –Ω–µ —Ä–∞–∑–¥—É–≤–∞—Ç—å one-hot.",
            ["features", "categorical"],
        )

    # datetime
    has_datetime = any(
        isinstance(info, dict) and info.get("role") == "datetime"
        for info in column_roles.values()
    )
    if has_datetime:
        add(
            "next",
            "–°–¥–µ–ª–∞—Ç—å time-based —Ñ–∏—á–∏ –∏ —Å–ø–ª–∏—Ç",
            "–ò–∑ –¥–∞—Ç –≤—ã–¥–µ–ª–∏ year/month/day/dayofweek, is_weekend. "
            "–î–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–π TimeSeriesSplit –≤–º–µ—Å—Ç–æ –æ–±—ã—á–Ω–æ–≥–æ —Å–ø–ª–∏—Ç–∞.",
            ["features", "datetime"],
        )

    # text
    has_text = any(
        isinstance(info, dict) and info.get("role") == "text"
        for info in column_roles.values()
    )
    if has_text:
        add(
            "next",
            "–û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏",
            "–î–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –ø–æ–ø—Ä–æ–±—É–π TF-IDF + –ª–∏–Ω–µ–π–Ω—É—é –º–æ–¥–µ–ª—å –∏–ª–∏ text-—ç–º–±–µ–¥–¥–∏–Ω–≥–∏. "
            "–°–µ–π—á–∞—Å –æ–Ω–∏, –≤–µ—Ä–æ—è—Ç–Ω–æ, –∏–≥–Ω–æ—Ä–∏—Ä—É—é—Ç—Å—è –∏–ª–∏ –∫–æ–¥–∏—Ä—É—é—Ç—Å—è –≥—Ä—É–±–æ.",
            ["features", "text"],
        )

    # –º–æ–¥–µ–ª—å: –∞–ø–≥—Ä–µ–π–¥ —Å RF
    if model and model.get("model_type") in ("RandomForestClassifier", "RandomForestRegressor"):
        add(
            "next",
            "–ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –±—É—Å—Ç–∏–Ω–≥ –∏ –ø–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤",
            "–¢–µ–∫—É—â–∞—è –º–æ–¥–µ–ª—å ‚Äî RandomForest. –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥ ‚Äî XGBoost/LightGBM/CatBoost "
            "—Å –ø–æ–¥–±–æ—Ä–æ–º –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (GridSearchCV/Optuna).",
            ["model", "tuning"],
        )

    # --- LATER: –º–µ—Ç—Ä–∏–∫–∏ –∏ –ø—Ä–æ–¥–∞–∫—à–µ–Ω ---

    if task.get("task") == "classification":
        add(
            "later",
            "–î–æ–±–∞–≤–∏—Ç—å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–µ—Ç—Ä–∏–∫–∏",
            "–ü–æ–º–∏–º–æ accuracy –∏ F1, –ø–æ—Å—á–∏—Ç–∞–π ROC-AUC –∏ PR-AUC, –æ—Å–æ–±–µ–Ω–Ω–æ –ø—Ä–∏ –¥–∏—Å–±–∞–ª–∞–Ω—Å–µ.",
            ["metrics"],
        )
    elif task.get("task") == "regression":
        add(
            "later",
            "–ü—Ä–æ–≤–µ—Ä–∏—Ç—å R¬≤, MAE –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫",
            "–ü–æ—Å–º–æ—Ç—Ä–∏, –Ω–µ –∑–∞–≤–∞–ª–∏–≤–∞–µ—Ç—Å—è –ª–∏ –º–æ–¥–µ–ª—å –Ω–∞ –∫—Ä–∞–π–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö —Ç–∞—Ä–≥–µ—Ç–∞, –ø–æ—Å—Ç—Ä–æ–π plot y_true vs y_pred.",
            ["metrics"],
        )

    add(
        "later",
        "–ó–∞–¥—É–º–∞—Ç—å—Å—è –æ –ø—Ä–æ–¥–∞–∫—à–µ–Ω-–ø–∞–π–ø–ª–∞–π–Ω–µ",
        "–ö–æ–≥–¥–∞ –∫–∞—á–µ—Å—Ç–≤–æ —É—Å—Ç—Ä–æ–∏—Ç ‚Äî –∑–∞–≤–µ—Ä–Ω–∏ –º–æ–¥–µ–ª—å –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Å–µ—Ä–≤–∏—Å —Å –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ–º, –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º –∏ –ª–æ–≥–∞–º–∏.",
        ["production"],
    )

    return plan


# ---------------------------------------------------------------------
# 7. —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
# ---------------------------------------------------------------------
def build_recommendations(
    df: pd.DataFrame,
    eda: dict,
    task: dict,
    problems: dict,
    model: dict | None,
) -> list[str]:
    recs: list[str] = []

    id_like = set(problems.get("id_like", []))

    consts = [c for c in (problems.get("constant_features") or []) if c not in id_like]
    if consts:
        recs.append(
            f"–ï—Å—Ç—å –ø–æ–ª–Ω–æ—Å—Ç—å—é –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {', '.join(consts[:8])} ‚Äî –º–æ–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å –ø–µ—Ä–µ–¥ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ–º."
        )

    quasi = [c for c in (problems.get("quasi_constant_features") or []) if c not in id_like]
    if quasi:
        recs.append(
            f"–ï—Å—Ç—å –ø–æ—á—Ç–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {', '.join(quasi[:8])} ‚Äî —Å—Ç–æ–∏—Ç –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∏—Ö –ø–æ–ª—å–∑—É."
        )

    corr_pairs = problems.get("high_corr_pairs") or []
    if corr_pairs:
        short = []
        for a, b, corr in corr_pairs[:6]:
            if a in id_like and b in id_like:
                continue
            short.append(f"{a}‚Üî{b} ({corr:.2f})")
        if short:
            recs.append(
                "–ï—Å—Ç—å —Å–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—â–∏–µ –ø–∞—Ä—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: "
                + ", ".join(short)
                + " ‚Äî –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –æ—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–ª–∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é."
            )

    high_nulls = problems.get("high_null_features") or {}
    if high_nulls:
        show = [f"{k} ({v:.1f}%)" for k, v in list(high_nulls.items())[:6] if k not in id_like]
        if show:
            recs.append(
                "–ï—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –±–æ–ª—å—à–∏–º —á–∏—Å–ª–æ–º –ø—Ä–æ–ø—É—Å–∫–æ–≤: "
                + ", ".join(show)
                + " ‚Äî –∑–∞–ø–æ–ª–Ω–∏/—É–¥–∞–ª–∏/—Å–¥–µ–ª–∞–π –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–ª–∞–≥."
            )

    # –¥–∏—Å–±–∞–ª–∞–Ω—Å
    if problems.get("class_imbalance"):
        ci = problems["class_imbalance"]
        recs.append(
            f"–ù–∞–π–¥–µ–Ω –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ ({ci['max_class']}:{ci['min_class']} ‚âà {ci['ratio']:.1f}). "
            "–ò—Å–ø–æ–ª—å–∑—É–π class_weight='balanced', stratify –ø—Ä–∏ train_test_split –∏–ª–∏ oversampling."
        )

    if problems.get("target_has_nan"):
        info = problems["target_has_nan"]
        recs.append(
            f"–í —Ü–µ–ª–µ–≤–æ–π –∫–æ–ª–æ–Ω–∫–µ {info['column']} –µ—Å—Ç—å –ø—Ä–æ–ø—É—Å–∫–∏ ({info['nan_count']}) ‚Äî –Ω—É–∂–Ω–æ —É–±—Ä–∞—Ç—å –∏—Ö –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º."
        )

    high_card = problems.get("high_cardinality") or []
    if high_card:
        cols = [f"{x['column']} ({x['n_unique']})" for x in high_card[:4] if x["column"] not in id_like]
        if cols:
            recs.append(
                "–ï—Å—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –±–æ–ª—å—à–∏–º —á–∏—Å–ª–æ–º –∑–Ω–∞—á–µ–Ω–∏–π: "
                + ", ".join(cols)
                + " ‚Äî –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å CatBoost/target encoding/—á–∞—Å—Ç–æ—Ç–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ."
            )

    if task.get("task") == "eda":
        recs.append("–¶–µ–ª–µ–≤–æ–π –ø—Ä–∏–∑–Ω–∞–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω ‚Äî –º–æ–∂–Ω–æ —è–≤–Ω–æ —É–∫–∞–∑–∞—Ç—å target –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ.")
    elif task.get("task") == "regression":
        recs.append("–î–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –º–æ–∂–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –±–æ–ª–µ–µ —Å–∏–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ (CatBoostRegressor, LightGBM).")
    elif task.get("task") == "classification":
        recs.append("–î–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–º–µ–µ—Ç —Å–º—ã—Å–ª –ø–æ—Å—á–∏—Ç–∞—Ç—å ROC-AUC –∏ PR-AUC, –æ—Å–æ–±–µ–Ω–Ω–æ –ø—Ä–∏ –¥–∏—Å–±–∞–ª–∞–Ω—Å–µ.")

    if model is None:
        recs.append("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–∞–ª–∞—Å—å ‚Äî —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ, –Ω–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ target –∏–ª–∏ –¥–∞–Ω–Ω—ã—Ö —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ.")
    else:
        if model.get("model_type") == "RandomForestClassifier":
            recs.append("–¢–µ–∫—É—â–∞—è –º–æ–¥–µ–ª—å ‚Äî RandomForestClassifier. –ú–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å –±—É—Å—Ç–∏–Ω–≥–æ–º –∏ –ø–æ–¥–±–æ—Ä–æ–º –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.")
        if model.get("model_type") == "RandomForestRegressor":
            recs.append("–¢–µ–∫—É—â–∞—è –º–æ–¥–µ–ª—å ‚Äî RandomForestRegressor. –ú–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å CatBoost/LightGBM.")

    return recs


# ---------------------------------------------------------------------
# 8. –≥—Ä–∞—Ñ–∏–∫–∏ ‚Üí base64
# ---------------------------------------------------------------------
def make_plots_base64(df: pd.DataFrame) -> list[dict]:
    plots: list[dict] = []

    num_cols = df.select_dtypes(include=["number"]).columns.tolist()[:3]
    for col in num_cols:
        fig, ax = plt.subplots(figsize=(4, 3))
        ax.hist(df[col].dropna(), bins=30)
        ax.set_title(f"Distribution of {col}")
        buf = BytesIO()
        plt.tight_layout()
        fig.savefig(buf, format="png")
        plt.close(fig)
        buf.seek(0)
        b64 = base64.b64encode(buf.read()).decode("utf-8")
        plots.append({"name": f"hist_{col}", "image_base64": b64})

    if len(df.select_dtypes(include=["number"]).columns) >= 2:
        corr = df.select_dtypes(include=["number"]).corr()
        fig, ax = plt.subplots(figsize=(4, 3))
        cax = ax.imshow(corr, cmap="viridis")
        ax.set_xticks(range(len(corr.columns)))
        ax.set_yticks(range(len(corr.columns)))
        ax.set_xticklabels(corr.columns, rotation=90, fontsize=6)
        ax.set_yticklabels(corr.columns, fontsize=6)
        fig.colorbar(cax)
        plt.tight_layout()
        buf = BytesIO()
        fig.savefig(buf, format="png")
        plt.close(fig)
        buf.seek(0)
        b64 = base64.b64encode(buf.read()).decode("utf-8")
        plots.append({"name": "correlation", "image_base64": b64})

    return plots


# ---------------------------------------------------------------------
# 9. —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ run –Ω–∞ –¥–∏—Å–∫ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
# ---------------------------------------------------------------------
def save_run(run_data: dict, model_pipeline) -> str:
    run_id = str(uuid.uuid4())
    run_dir = os.path.join("runs", run_id)
    os.makedirs(run_dir, exist_ok=True)

    with open(os.path.join(run_dir, "report.json"), "w", encoding="utf-8") as f:
        json.dump(run_data, f, ensure_ascii=False, indent=2)

    if model_pipeline is not None:
        joblib.dump(model_pipeline, os.path.join(run_dir, "model.joblib"))

    return run_id


# ---------------------------------------------------------------------
# 10. –ö—Ä–∞—Ç–∫–∏–π —Å—Ç–∞—Ç—É—Å –∏ next actions
# ---------------------------------------------------------------------
def build_analysis_status(task: dict, problems: dict, model: dict | None) -> dict:
    problems = problems or {}
    task = task or {}

    status: dict = {
        "task": task.get("task", "eda"),
        "target": task.get("target"),
        "dataset": "ok",
        "model": "ok",
        "notes": [],
    }

    if problems.get("target_has_nan"):
        info = problems["target_has_nan"]
        status["dataset"] = "warning"
        status["notes"].append(
            f"–í —Ç–∞—Ä–≥–µ—Ç–µ {info.get('column', '?')} –µ—Å—Ç—å {info.get('nan_count', 0)} –ø—Ä–æ–ø—É—Å–∫–æ–≤"
        )

    if problems.get("high_null_features"):
        status["dataset"] = "warning"
        status["notes"].append("–ï—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å >30% –ø—Ä–æ–ø—É—Å–∫–æ–≤")

    ci = problems.get("class_imbalance")
    if ci:
        max_cls = ci.get("max_class") or ci.get("most_common_class") or "?"
        min_cls = ci.get("min_class") or ci.get("rarest_class") or "?"
        ratio = ci.get("ratio", "?")
        status["dataset"] = "warning"
        status["notes"].append(f"–î–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤: {max_cls}:{min_cls} ‚âà {ratio}")

    if model is None:
        status["model"] = "not_trained"
        status["notes"].append("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–∞–ª–∞—Å—å")
    elif model.get("model_type") == "skipped":
        status["model"] = "skipped"
        if model.get("reason"):
            status["notes"].append(f"–ú–æ–¥–µ–ª—å –ø—Ä–æ–ø—É—â–µ–Ω–∞: {model['reason']}")
    else:
        status["model"] = "ok"

    return status


def build_next_actions(task: dict, problems: dict, model: dict | None) -> list[str]:
    actions: list[str] = []
    task = task or {}
    problems = problems or {}

    if problems.get("target_has_nan"):
        actions.append("–û—á–∏—Å—Ç–∏ —Ç–∞—Ä–≥–µ—Ç –æ—Ç NaN (—É–¥–∞–ª–∏ —Å—Ç—Ä–æ–∫–∏ –∏–ª–∏ –∑–∞–ø–æ–ª–Ω–∏).")

    if problems.get("high_null_features"):
        actions.append("–û–±—Ä–∞–±–æ—Ç–∞–π –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å >30% –ø—Ä–æ–ø—É—Å–∫–æ–≤: –¥—Ä–æ–ø/–∏–º–ø—É—Ç–∞—Ü–∏—è/—Ñ–ª–∞–≥.")

    if problems.get("class_imbalance") and task.get("task") == "classification":
        actions.append("–ü—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —É–∫–∞–∂–∏ class_weight='balanced' –∏–ª–∏ —Å–¥–µ–ª–∞–π oversampling.")

    if problems.get("high_cardinality"):
        actions.append("–î–ª—è –∫–æ–ª–æ–Ω–æ–∫ —Å –≤—ã—Å–æ–∫–æ–π –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é –∏—Å–ø–æ–ª—å–∑—É–π CatBoost/target encoding.")

    if task.get("task") == "classification":
        actions.append("–ü–æ—Å—á–∏—Ç–∞–π ROC-AUC –∏ PR-AUC, –µ—Å–ª–∏ –≤–∞–∂–Ω—ã —Ä–µ–¥–∫–∏–µ –∫–ª–∞—Å—Å—ã.")
    elif task.get("task") == "regression":
        actions.append("–ü–æ–ø—Ä–æ–±—É–π –±—É—Å—Ç–∏–Ω–≥ (CatBoost/LightGBM) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞.")

    if model is not None and model.get("model_type") == "RandomForestClassifier":
        actions.append("–°–¥–µ–ª–∞–π –ø–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–ª–∏ –ø–æ–ø—Ä–æ–±—É–π –±—É—Å—Ç–∏–Ω–≥ (XGBoost/LightGBM).")

    if not actions:
        actions.append("–î–∞—Ç–∞—Å–µ—Ç –≤—ã–≥–ª—è–¥–∏—Ç –æ–∫ ‚Äî –º–æ–∂–Ω–æ –¥–≤–∏–≥–∞—Ç—å—Å—è –∫ —Ñ–∏—á–∞–º/–ø–æ–¥–±–æ—Ä—É –º–æ–¥–µ–ª–∏.")

    return actions


# ---------------------------------------------------------------------
# 11. –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å–≤–æ–¥–∫–∞ –ø–æ —Ç–∞—Ä–≥–µ—Ç—É (–¥–ª—è LLM –∏ UI)
# ---------------------------------------------------------------------
def summarize_target(df: pd.DataFrame, task: dict) -> dict:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç json-friendly —Å–≤–æ–¥–∫—É –ø–æ —Ç–∞—Ä–≥–µ—Ç—É:
    - –µ—Å—Ç—å –ª–∏ —Ç–∞—Ä–≥–µ—Ç
    - —Å–∫–æ–ª—å–∫–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤ / —Å—Ç—Ä–æ–∫
    - –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: —á–∏—Å–ª–æ –∫–ª–∞—Å—Å–æ–≤ + —Ç–æ–ø –∫–ª–∞—Å—Å—ã
    - –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏: min/max/mean/std
    """
    target = (task or {}).get("target")
    task_type = (task or {}).get("task", "eda")

    if not target or target not in df.columns:
        return {
            "has_target": False,
            "reason": "target_not_found",
        }

    col = df[target]
    n_rows = len(df)
    n_missing = int(col.isna().sum())

    base = {
        "has_target": True,
        "target": target,
        "task": task_type,
        "n_rows": int(n_rows),
        "n_missing": n_missing,
        "missing_frac": float(n_missing / n_rows) if n_rows else 0.0,
    }

    # –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø
    if task_type == "classification":
        vc = col.value_counts(dropna=False)
        n_classes = int(len(vc))

        top_classes = []
        for cls, cnt in vc.head(20).items():
            label = "__NaN__" if pd.isna(cls) else str(cls)
            top_classes.append({
                "label": label,
                "count": int(cnt),
                "share": float(cnt / n_rows) if n_rows else 0.0,
            })

        base.update({
            "n_classes": n_classes,
            "top_classes": top_classes,
        })
        return base

    # –†–ï–ì–†–ï–°–°–ò–Ø / –ß–ò–°–õ–û–í–û–ô –¢–ê–†–ì–ï–¢
    if pd.api.types.is_numeric_dtype(col):
        base.update({
            "min": float(col.min(skipna=True)) if col.notna().any() else 0.0,
            "max": float(col.max(skipna=True)) if col.notna().any() else 0.0,
            "mean": float(col.mean(skipna=True)) if col.notna().any() else 0.0,
            "std": float(col.std(skipna=True)) if col.notna().any() else 0.0,
        })
    else:
        base.update({
            "note": "target_is_not_numeric",
        })

    return base


# ---------------------------------------------------------------------
# 12. –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏ (–¥–ª—è UI –∏ LLM)
# ---------------------------------------------------------------------
def rank_problems(problems: dict) -> list[dict]:
    """
    –ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç dict –∏–∑ analyze_dataset –≤ —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä—É–∫—Ç—É—Ä:
    [{key, severity, message, data}, ...]
    —á—Ç–æ–±—ã —Ñ—Ä–æ–Ω—Ç –º–æ–≥ –ø–æ–∫–∞–∑–∞—Ç—å —Å–Ω–∞—á–∞–ª–∞ high, –ø–æ—Ç–æ–º medium, –ø–æ—Ç–æ–º low.
    """
    problems = problems or {}
    ranked: list[dict] = []

    # 1. NaN –≤ —Ç–∞—Ä–≥–µ—Ç–µ ‚Äî –≤—Å–µ–≥–¥–∞ high
    if problems.get("target_has_nan"):
        info = problems["target_has_nan"]
        ranked.append({
            "key": "target_has_nan",
            "severity": "high",
            "message": f"–í —Ç–∞—Ä–≥–µ—Ç–µ {info.get('column')} –µ—Å—Ç—å {info.get('nan_count')} –ø—Ä–æ–ø—É—Å–∫–æ–≤ ‚Äî —É–±–µ—Ä–∏ –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º.",
            "data": info,
        })

    # 2. –¥–∏—Å–±–∞–ª–∞–Ω—Å
    if problems.get("class_imbalance"):
        ci = problems["class_imbalance"]
        ranked.append({
            "key": "class_imbalance",
            "severity": "high",
            "message": (
                "–ù–∞–π–¥–µ–Ω –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π class_weight='balanced', "
                "stratify –ø—Ä–∏ train_test_split –∏–ª–∏ oversampling."
            ),
            "data": ci,
        })

    # 3. –º–Ω–æ–≥–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤ –≤ —Ñ–∏—á–∞—Ö
    if problems.get("high_null_features"):
        ranked.append({
            "key": "high_null_features",
            "severity": "medium",
            "message": "–ï—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å >30% –ø—Ä–æ–ø—É—Å–∫–æ–≤ ‚Äî –∑–∞–ø–æ–ª–Ω–∏/—É–¥–∞–ª–∏/—Å–¥–µ–ª–∞–π —Ñ–ª–∞–≥.",
            "data": problems["high_null_features"],
        })

    # 4. –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã
    if problems.get("constant_features"):
        ranked.append({
            "key": "constant_features",
            "severity": "medium",
            "message": "–ï—Å—Ç—å –ø–æ–ª–Ω–æ—Å—Ç—å—é –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ‚Äî –º–æ–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å.",
            "data": problems["constant_features"],
        })

    # 5. –ø–æ—á—Ç–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ
    if problems.get("quasi_constant_features"):
        ranked.append({
            "key": "quasi_constant_features",
            "severity": "low",
            "message": "–ï—Å—Ç—å –ø–æ—á—Ç–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ‚Äî –ø—Ä–æ–≤–µ—Ä—å –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å.",
            "data": problems["quasi_constant_features"],
        })

    # 6. –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
    if problems.get("high_corr_pairs"):
        ranked.append({
            "key": "high_corr_pairs",
            "severity": "low",
            "message": "–ï—Å—Ç—å —Å–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ‚Äî –º–æ–∂–Ω–æ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å.",
            "data": problems["high_corr_pairs"][:20],
        })

    # 7. –≤—ã—Å–æ–∫–∞—è –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å
    if problems.get("high_cardinality"):
        ranked.append({
            "key": "high_cardinality",
            "severity": "medium",
            "message": "–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –±–æ–ª—å—à–∏–º —á–∏—Å–ª–æ–º –∑–Ω–∞—á–µ–Ω–∏–π ‚Äî –ª—É—á—à–µ CatBoost/target encoding.",
            "data": problems["high_cardinality"],
        })

    return ranked


# ---------------------------------------------------------------------
# 13. –ò–¥–µ–∏ –Ω–æ–≤—ã—Ö —Ñ–∏—á
# ---------------------------------------------------------------------
def auto_feature_suggestions(df: pd.DataFrame) -> list[str]:
    """
    –û—á–µ–Ω—å –ø—Ä–æ—Å—Ç—ã–µ –ø–æ–¥—Å–∫–∞–∑–∫–∏, –∫–∞–∫–∏–µ —Ñ–∏—á–∏ –º–æ–∂–Ω–æ –¥–æ–∫—Ä—É—Ç–∏—Ç—å.
    –≠—Ç–æ –Ω–µ –º–µ–Ω—è–µ—Ç df, —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ –∏–¥–µ–∏ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è/–∞–≥–µ–Ω—Ç–∞.
    """
    suggestions: list[str] = []

    # –¥–∞—Ç—ã
    for col in df.columns:
        if "date" in col.lower() or "time" in col.lower() or "timestamp" in col.lower():
            suggestions.append(
                f"–ò–∑ –∫–æ–ª–æ–Ω–∫–∏ {col} –º–æ–∂–Ω–æ –≤—ã–Ω–µ—Å—Ç–∏ year/month/day/dayofweek –∏, –≤–æ–∑–º–æ–∂–Ω–æ, –ø—Ä–∏–∑–Ω–∞–∫ 'is_weekend'."
            )

    # –±–æ–ª—å—à–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ
    obj_cols = df.select_dtypes(include=["object"]).columns
    for col in obj_cols:
        nun = df[col].nunique(dropna=True)
        if nun > 200:
            suggestions.append(
                f"–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–∞—è –∫–æ–ª–æ–Ω–∫–∞ {col} –∏–º–µ–µ—Ç –º–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏–π ({nun}) ‚Äî —Å—Ç–æ–∏—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å target/frequency encoding –∏–ª–∏ CatBoost."
            )
        elif 2 < nun <= 50:
            suggestions.append(
                f"–ö–æ–ª–æ–Ω–∫–∞ {col} ‚Äî –∞–∫–∫—É—Ä–∞—Ç–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–∞—è, –º–æ–∂–Ω–æ one-hot (—É —Ç–µ–±—è —ç—Ç–æ —É–∂–µ –µ—Å—Ç—å –≤ –ø–∞–π–ø–ª–∞–π–Ω–µ)."
            )

    # —á–∏—Å–ª–æ–≤—ã–µ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏
    null_frac = df.isna().mean()
    for col, frac in null_frac.items():
        if frac > 0.0 and col not in obj_cols:
            suggestions.append(
                f"–í —á–∏—Å–ª–æ–≤–æ–π –∫–æ–ª–æ–Ω–∫–µ {col} –µ—Å—Ç—å –ø—Ä–æ–ø—É—Å–∫–∏ ({frac:.1%}) ‚Äî –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –±–∏–Ω–∞—Ä–Ω—ã–π —Ñ–ª–∞–≥ 'is_{col}_missing'."
            )

    if not suggestions:
        suggestions.append("–Ø–≤–Ω—ã—Ö –∏–¥–µ–π –ø–æ –Ω–æ–≤—ã–º —Ñ–∏—á–∞–º –Ω–µ—Ç ‚Äî –º–æ–∂–Ω–æ –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç—å –∫ –æ—Ç–±–æ—Ä—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤/–º–æ–¥–µ–ª—è–º.")

    return suggestions


# ---------------------------------------------------------------------
# 14. Feature importance –∏–∑ pipeline
# ---------------------------------------------------------------------
def extract_feature_importance(pipeline) -> list[dict]:
    """
    –î–æ—Å—Ç–∞—ë–º —Ç–æ–ø-–≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –æ–±—É—á–µ–Ω–Ω–æ–≥–æ pipeline, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å —É–º–µ–µ—Ç feature_importances_.
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π: {"feature": ..., "importance": ...}
    –ï—Å–ª–∏ –¥–æ—Å—Ç–∞—Ç—å –Ω–µ–ª—å–∑—è ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫.
    """
    try:
        model = pipeline.named_steps.get("model")
        preprocess = pipeline.named_steps.get("preprocess")

        if model is None or not hasattr(model, "feature_importances_"):
            return []

        importances = model.feature_importances_

        feature_names: list[str] = []
        if hasattr(preprocess, "get_feature_names_out"):
            feature_names = list(preprocess.get_feature_names_out())
        else:
            feature_names = [f"feature_{i}" for i in range(len(importances))]

        items = []
        for name, imp in zip(feature_names, importances):
            items.append({"feature": str(name), "importance": float(imp)})

        items.sort(key=lambda x: x["importance"], reverse=True)
        return items[:50]
    except Exception:
        return []


# ---------------------------------------------------------------------
# 15. –ö–æ–¥-–ø–æ–¥—Å–∫–∞–∑–∫–∏ (snippets) –ø–æ–¥ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã
# ---------------------------------------------------------------------
def build_code_hints(problems: dict, task: dict) -> list[dict]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫ "–∫–æ–¥-–ø–æ–¥—Å–∫–∞–∑–æ–∫", —á—Ç–æ–±—ã —Ñ—Ä–æ–Ω—Ç –º–æ–≥ –ø–æ–∫–∞–∑–∞—Ç—å
    –≥–æ—Ç–æ–≤—ã–µ –∫—É—Å–∫–∏ –∫–æ–¥–∞ –ø–æ–¥ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã.
    –§–æ—Ä–º–∞—Ç —ç–ª–µ–º–µ–Ω—Ç–∞:
    {
        "title": "–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–∏—Å–±–∞–ª–∞–Ω—Å",
        "snippet": "from imblearn.over_sampling import SMOTE\n...",
        "reason": "–ù–∞–π–¥–µ–Ω –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤"
    }
    """
    problems = problems or {}
    task = task or {}
    hints: list[dict] = []

    # 1) –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤
    if problems.get("class_imbalance") and task.get("task") == "classification":
        hints.append({
            "title": "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å class_weight='balanced'",
            "reason": "–ù–∞–π–¥–µ–Ω –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤",
            "snippet": (
                "from sklearn.ensemble import RandomForestClassifier\n"
                "clf = RandomForestClassifier(class_weight='balanced', n_estimators=300, random_state=42)\n"
                "clf.fit(X_train, y_train)\n"
                "preds = clf.predict(X_val)"
            ),
        })
        hints.append({
            "title": "Oversampling —á–µ—Ä–µ–∑ imblearn",
            "reason": "–ù–∞–π–¥–µ–Ω –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤",
            "snippet": (
                "from imblearn.over_sampling import RandomOverSampler\n"
                "ros = RandomOverSampler(random_state=42)\n"
                "X_res, y_res = ros.fit_resample(X_train, y_train)\n"
                "# –¥–∞–ª—å—à–µ –æ–±—É—á–∞–π –º–æ–¥–µ–ª—å –Ω–∞ X_res, y_res"
            ),
        })

    # 2) –º–Ω–æ–≥–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π ‚Üí CatBoost
    if problems.get("high_cardinality"):
        hints.append({
            "title": "CatBoost –¥–ª—è –∫–æ–ª–æ–Ω–æ–∫ —Å –≤—ã—Å–æ–∫–æ–π –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é",
            "reason": "–ï—Å—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –±–æ–ª—å—à–∏–º —á–∏—Å–ª–æ–º –∑–Ω–∞—á–µ–Ω–∏–π",
            "snippet": (
                "from catboost import CatBoostClassifier\n"
                "# –∏–Ω–¥–µ–∫—Å–∞–º–∏ —É–∫–∞–∂–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n"
                "cat_features = [0, 3, 5]\n"
                "model = CatBoostClassifier(depth=6, learning_rate=0.1, loss_function='MultiClass', verbose=False)\n"
                "model.fit(X_train, y_train, cat_features=cat_features, eval_set=(X_val, y_val))"
            ),
        })

    # 3) –ø—Ä–æ–ø—É—Å–∫–∏ –≤ —Ç–∞—Ä–≥–µ—Ç–µ
    if problems.get("target_has_nan"):
        col = problems["target_has_nan"]["column"]
        hints.append({
            "title": "–£–¥–∞–ª–∏—Ç—å —Å—Ç—Ä–æ–∫–∏ —Å NaN –≤ —Ç–∞—Ä–≥–µ—Ç–µ",
            "reason": f"–í —Ü–µ–ª–µ–≤–æ–π –∫–æ–ª–æ–Ω–∫–µ {col} –µ—Å—Ç—å –ø—Ä–æ–ø—É—Å–∫–∏",
            "snippet": (
                f"df = df[~df['{col}'].isna()].copy()\n"
                "# –¥–∞–ª—å—à–µ –¥–µ–ª–∞–π train/test split"
            ),
        })

    # 4) –º–Ω–æ–≥–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤ –≤ —Ñ–∏—á–∞—Ö
    if problems.get("high_null_features"):
        hints.append({
            "title": "–ü–∞–π–ø–ª–∞–π–Ω —Å SimpleImputer",
            "reason": "–ï—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å >30% –ø—Ä–æ–ø—É—Å–∫–æ–≤",
            "snippet": (
                "from sklearn.impute import SimpleImputer\n"
                "from sklearn.pipeline import Pipeline\n"
                "from sklearn.ensemble import RandomForestClassifier\n"
                "pipe = Pipeline([\n"
                "    ('imputer', SimpleImputer(strategy='median')),\n"
                "    ('model', RandomForestClassifier())\n"
                "])\n"
                "pipe.fit(X_train, y_train)"
            ),
        })

    # 5) –ø—Ä–æ—Å—Ç–æ –∑–∞–≥–æ—Ç–æ–≤–∫–∞ –ø–æ–¥ –∑–∞–¥–∞—á—É
    if task.get("task") == "regression":
        hints.append({
            "title": "–ë–∞–∑–æ–≤–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è (RF)",
            "reason": "–ó–∞–¥–∞—á–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ –∫–∞–∫ —Ä–µ–≥—Ä–µ—Å—Å–∏—è",
            "snippet": (
                "from sklearn.ensemble import RandomForestRegressor\n"
                "model = RandomForestRegressor(n_estimators=300, random_state=42)\n"
                "model.fit(X_train, y_train)\n"
                "preds = model.predict(X_val)"
            ),
        })
    elif task.get("task") == "classification":
        hints.append({
            "title": "–ë–∞–∑–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (RF)",
            "reason": "–ó–∞–¥–∞—á–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ –∫–∞–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è",
            "snippet": (
                "from sklearn.ensemble import RandomForestClassifier\n"
                "model = RandomForestClassifier(n_estimators=300, random_state=42)\n"
                "model.fit(X_train, y_train)\n"
                "preds = model.predict(X_val)"
            ),
        })

    return hints
