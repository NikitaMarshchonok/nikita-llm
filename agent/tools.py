# agent/tools.py
from __future__ import annotations

import os
import json
import uuid
import base64
from io import BytesIO
from typing import Optional, Literal

import pandas as pd
import numpy as np
import re

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.metrics import (
    accuracy_score,
    f1_score,
    mean_squared_error,
    roc_auc_score,
)
import joblib

import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt


# ---------------------------------------------------------------------
# 1. –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π EDA
# ---------------------------------------------------------------------
def basic_eda(df: pd.DataFrame) -> dict:
    eda: dict = {
        "shape": list(df.shape),
        "dtypes": {c: str(df[c].dtype) for c in df.columns},
    }

    # –ø—Ä–æ–ø—É—Å–∫–∏
    eda["nulls"] = {c: int(df[c].isna().sum()) for c in df.columns}
    eda["null_fractions"] = {c: float(df[c].isna().mean()) for c in df.columns}

    # —á–∏—Å–ª–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    numeric_cols = df.select_dtypes(include=["number"]).columns.tolist()
    stats = {}
    for c in numeric_cols[:30]:
        ser = df[c]
        stats[c] = {
            "mean": float(ser.mean()),
            "std": float(ser.std() or 0),
            "min": float(ser.min()),
            "max": float(ser.max()),
        }
    eda["numeric_stats"] = stats

    # –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã / –∫–≤–∞–∑–∏-–∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã
    constant_features = []
    quasi_constant_features = []
    for c in df.columns:
        uniq = df[c].nunique(dropna=True)
        if uniq <= 1:
            constant_features.append(c)
        else:
            top_frac = float(df[c].value_counts(normalize=True, dropna=False).iloc[0])
            if top_frac > 0.98:
                quasi_constant_features.append(c)
    eda["constant_features"] = constant_features
    eda["quasi_constant_features"] = quasi_constant_features

    # —Å–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—â–∏–µ –ø–∞—Ä—ã
    high_corr_pairs = []
    if len(numeric_cols) >= 2:
        corr = df[numeric_cols].corr().abs()
        for i, c1 in enumerate(numeric_cols):
            for j in range(i + 1, len(numeric_cols)):
                c2 = numeric_cols[j]
                val = float(corr.loc[c1, c2])
                if val >= 0.9:
                    high_corr_pairs.append(
                        {"feature_1": c1, "feature_2": c2, "corr": val}
                    )
    eda["high_corr_pairs"] = high_corr_pairs

    return eda


# ---------------------------------------------------------------------
# 2. —É–≥–∞–¥—ã–≤–∞–Ω–∏–µ —Ç–∞—Ä–≥–µ—Ç–∞ –∏ –∑–∞–¥–∞—á–∏
# ---------------------------------------------------------------------
ID_LIKE = {"id", "ID", "Id", "index", "Rk", "rank"}


def _looks_like_id(colname: str) -> bool:
    return colname in ID_LIKE or re.search(r"id$", colname, re.IGNORECASE) is not None


def _guess_target(
    df: pd.DataFrame,
) -> tuple[Literal["eda", "classification", "regression"], Optional[str]]:
    lower_cols = {c.lower(): c for c in df.columns}

    for cand in ("target", "label", "class", "y"):
        if cand in lower_cols:
            col = lower_cols[cand]
            if df[col].nunique() <= 50:
                return "classification", col
            else:
                return "regression", col

    for c in df.columns:
        if _looks_like_id(c):
            continue
        uniq = df[c].nunique(dropna=True)
        if 2 <= uniq <= 30:
            return "classification", c

    num_cols = df.select_dtypes(include=["number"]).columns.tolist()
    for c in num_cols:
        if _looks_like_id(c):
            continue
        if df[c].nunique() > 1:
            return "regression", c

    return "eda", None


def detect_task(df: pd.DataFrame, target: Optional[str] = None) -> dict:
    if target is not None and target in df.columns:
        nunique = df[target].nunique()
        if df[target].dtype == "object" or nunique <= 30:
            task = "classification"
        else:
            task = "regression"
        return {"task": task, "target": target}

    task, tgt = _guess_target(df)
    return {"task": task, "target": tgt}


# ---------------------------------------------------------------------
# 3. –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ –∫ —á–∏—Å–ª–∞–º –∏ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥
# ---------------------------------------------------------------------
def _coerce_numeric(df: pd.DataFrame) -> pd.DataFrame:
    new_df = df.copy()
    for col in new_df.columns:
        if new_df[col].dtype == "object":
            ser = new_df[col].astype(str).str.replace(",", "").str.replace(" ", "")
            try:
                converted = pd.to_numeric(ser)
            except Exception:
                continue
            else:
                if converted.dtype != "object":
                    new_df[col] = converted
    return new_df


def build_preprocessor(X: pd.DataFrame) -> ColumnTransformer:
    numeric_features = X.select_dtypes(
        include=["int64", "float64", "int32", "float32"]
    ).columns.tolist()
    categorical_features = [c for c in X.columns if c not in numeric_features]

    numeric_transformer = Pipeline(
        steps=[("imputer", SimpleImputer(strategy="median"))]
    )

    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore")),
        ]
    )

    return ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ]
    )


# ---------------------------------------------------------------------
# 4. –æ–±—É—á–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
# ---------------------------------------------------------------------
def train_baseline(
    df: pd.DataFrame,
    target: str,
    task: str,
    problems: dict | None = None,
    return_model: bool = False,
) -> Optional[dict]:
    try:
        if target not in df.columns:
            return None

        df = _coerce_numeric(df)
        df = df[~df[target].isna()].copy()
        if df.shape[0] < 20:
            return None

        y = df[target]
        X = df.drop(columns=[target])
        if X.shape[1] == 0:
            return None

        preprocessor = build_preprocessor(X)

        # –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        if task == "classification":
            if y.nunique() < 2:
                return None

            rf_kwargs = dict(
                n_estimators=200,
                random_state=42,
                n_jobs=-1,
            )

            # –µ—Å–ª–∏ –Ω–∞—à–ª–∏ –¥–∏—Å–±–∞–ª–∞–Ω—Å ‚Äî –≤–∫–ª—é—á–∞–µ–º –≤–µ—Å–∞
            if problems and problems.get("class_imbalance"):
                rf_kwargs["class_weight"] = "balanced"

            model = RandomForestClassifier(**rf_kwargs)

            counts = y.value_counts(dropna=False)
            can_stratify = (counts >= 2).all()

            X_train, X_val, y_train, y_val = train_test_split(
                X,
                y,
                test_size=0.2,
                random_state=42,
                stratify=y if (y.nunique() < 50 and can_stratify) else None,
            )

            pipe = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
            pipe.fit(X_train, y_train)
            preds = pipe.predict(X_val)

            acc = float(accuracy_score(y_val, preds))
            f1 = float(f1_score(y_val, preds, average="weighted"))

            res: dict = {
                "model_type": "RandomForestClassifier",
                "accuracy": acc,
                "f1": f1,
            }

            # –µ—Å–ª–∏ –±–∏–Ω–∞—Ä–∫–∞ ‚Äî —Å—á–∏—Ç–∞–µ–º AUC
            if y_val.nunique() == 2:
                try:
                    proba = pipe.predict_proba(X_val)[:, 1]
                    auc = float(roc_auc_score(y_val, proba))
                    res["roc_auc"] = auc
                except Exception:
                    pass

            if return_model:
                res["pipeline"] = pipe
            return res

        # —Ä–µ–≥—Ä–µ—Å—Å–∏—è
        elif task == "regression":
            model = RandomForestRegressor(
                n_estimators=200,
                random_state=42,
                n_jobs=-1,
            )
            X_train, X_val, y_train, y_val = train_test_split(
                X, y, test_size=0.2, random_state=42
            )
            pipe = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
            pipe.fit(X_train, y_train)
            preds = pipe.predict(X_val)

            mse = float(mean_squared_error(y_val, preds))
            rmse = mse ** 0.5

            res = {
                "model_type": "RandomForestRegressor",
                "rmse": rmse,
            }
            if return_model:
                res["pipeline"] = pipe
            return res

        return None

    except Exception:
        return None


# ---------------------------------------------------------------------
# 5. –æ—Ç—á—ë—Ç
# ---------------------------------------------------------------------
def build_report(
    df: pd.DataFrame,
    eda: dict,
    task: dict,
    model: dict | None,
    problems: dict | None = None,
) -> str:
    problems = problems or {}
    lines: list[str] = []

    rows, cols = eda.get("shape", (len(df), df.shape[1]))
    lines.append("üì¶ –î–∞–Ω–Ω—ã–µ")
    lines.append(f"‚Ä¢ –†–∞–∑–º–µ—Ä: {rows} —Å—Ç—Ä–æ–∫ √ó {cols} –∫–æ–ª–æ–Ω–æ–∫.")

    nulls = eda.get("nulls", {})
    nz = {k: v for k, v in nulls.items() if v > 0}
    if nz:
        lines.append("‚Ä¢ –ü—Ä–æ–ø—É—Å–∫–∏ (—Ç–æ–ø):")
        for k, v in list(nz.items())[:8]:
            lines.append(f"   - {k}: {v}")

    num_stats = eda.get("numeric_stats", {})
    if num_stats:
        lines.append("‚Ä¢ –ß–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (mean / std / min / max):")
        for name, st in list(num_stats.items())[:8]:
            lines.append(
                f"   - {name}: {st['mean']:.3f}/{st['std']:.3f}/{st['min']}/{st['max']}"
            )

    lines.append("")
    lines.append("üß© –ü—Ä–æ–±–ª–µ–º—ã –≤ –¥–∞–Ω–Ω—ã—Ö")
    any_problems = (
        problems.get("constant_features")
        or problems.get("quasi_constant_features")
        or problems.get("high_corr_pairs")
        or problems.get("high_null_features")
        or problems.get("target_has_nan")
        or problems.get("class_imbalance")
        or problems.get("high_cardinality")
    )
    if not any_problems:
        lines.append("‚Ä¢ –Ø–≤–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º –Ω–µ –Ω–∞–π–¥–µ–Ω–æ ‚úÖ")
    else:
        consts = problems.get("constant_features") or []
        if consts:
            lines.append(
                "‚Ä¢ –ö–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: " + ", ".join(consts[:8]) + " ‚Äî –º–æ–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å."
            )
        qconst = problems.get("quasi_constant_features") or []
        if qconst:
            lines.append(
                "‚Ä¢ –ü–æ—á—Ç–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: "
                + ", ".join(qconst[:8])
                + " ‚Äî –ø—Ä–æ–≤–µ—Ä—å –∏—Ö –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å."
            )
        corr_pairs = problems.get("high_corr_pairs") or []
        if corr_pairs:
            short = [f"{a}‚Üî{b} ({c:.2f})" for a, b, c in corr_pairs[:6]]
            lines.append(
                "‚Ä¢ –°–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—â–∏–µ –ø–∞—Ä—ã: " + ", ".join(short) + " ‚Äî –≤–æ–∑–º–æ–∂–µ–Ω –æ—Ç–±–æ—Ä —Ñ–∏—á."
            )
        high_nulls = problems.get("high_null_features") or {}
        if high_nulls:
            show = [f"{k} ({v:.1f}%)" for k, v in list(high_nulls.items())[:6]]
            lines.append("‚Ä¢ –ú–Ω–æ–≥–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤: " + ", ".join(show))
        if problems.get("target_has_nan"):
            info = problems["target_has_nan"]
            lines.append(
                f"‚Ä¢ –í —Ç–∞—Ä–≥–µ—Ç–µ {info['column']} –µ—Å—Ç—å {info['nan_count']} –ø—Ä–æ–ø—É—Å–∫–æ–≤ ‚Äî —É–±—Ä–∞—Ç—å –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º."
            )
        if problems.get("class_imbalance"):
            ci = problems["class_imbalance"]
            lines.append(
                f"‚Ä¢ –î–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤: {ci['max_class']}:{ci['min_class']} ‚âà {ci['ratio']:.1f} ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π class_weight/oversampling."
            )
        high_card = problems.get("high_cardinality") or []
        if high_card:
            cols_txt = [f"{x['column']} ({x['n_unique']})" for x in high_card[:4]]
            lines.append(
                "‚Ä¢ –í—ã—Å–æ–∫–∞—è –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö: " + ", ".join(cols_txt)
            )

    lines.append("")
    lines.append("ü§ñ –ú–æ–¥–µ–ª—å")
    if task.get("task") == "eda" or not task.get("target"):
        lines.append("‚Ä¢ –¶–µ–ª–µ–≤–æ–π –ø—Ä–∏–∑–Ω–∞–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω ‚Äî –æ–±—É—á–∞—Ç—å –Ω–µ—á–µ–≥–æ.")
    elif model is None:
        lines.append("‚Ä¢ –ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–∞–ª–∞—Å—å ‚Äî –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –æ–¥–∏–Ω –∫–ª–∞—Å—Å.")
    else:
        lines.append(f"‚Ä¢ –ó–∞–¥–∞—á–∞: {task['task']} –ø–æ –∫–æ–ª–æ–Ω–∫–µ ‚Äú{task['target']}‚Äù.")
        lines.append(f"‚Ä¢ –ú–æ–¥–µ–ª—å: {model['model_type']}.")
        if "accuracy" in model:
            lines.append(f"‚Ä¢ accuracy = {model['accuracy']:.3f}")
        if "f1" in model:
            lines.append(f"‚Ä¢ f1 = {model['f1']:.3f}")
        if "roc_auc" in model:
            lines.append(f"‚Ä¢ ROC-AUC = {model['roc_auc']:.3f}")
        if "rmse" in model:
            lines.append(f"‚Ä¢ RMSE = {model['rmse']:.3f}")

    lines.append("")
    lines.append("ü™ú –ß—Ç–æ —Å–¥–µ–ª–∞—Ç—å –¥–∞–ª—å—à–µ")
    lines.append("‚Ä¢ –ü–æ—Å–º–æ—Ç—Ä–∏ –Ω–∞ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã/–∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –∏ —Å–æ–∫—Ä–∞—Ç–∏ —Ñ–∏—á–∏.")
    if task.get("task") == "classification":
        lines.append("‚Ä¢ –î–ª—è –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ ‚Äî class_weight='balanced' –∏–ª–∏ oversampling.")
        lines.append("‚Ä¢ –ü–æ—Å—á–∏—Ç–∞–π ROC-AUC/PR-AUC, –µ—Å–ª–∏ –≤–∞–∂–µ–Ω —Ä–µ–¥–∫–∏–π –∫–ª–∞—Å—Å.")
    if task.get("task") == "regression":
        lines.append("‚Ä¢ –ü–æ–ø—Ä–æ–±—É–π –±—É—Å—Ç–∏–Ω–≥ (CatBoost/LightGBM) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è RMSE.")

    return "\n".join(lines)


# ---------------------------------------------------------------------
# 6. –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–±–ª–µ–º (–æ–¥–∏–Ω –≤–∞—Ä–∏–∞–Ω—Ç!)
# ---------------------------------------------------------------------
def analyze_dataset(df: pd.DataFrame, eda: dict, task: dict) -> dict:
    """
    –°–∏–≥–Ω–∞–ª—ã –ø–æ –¥–∞—Ç–∞—Å–µ—Ç—É: –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã, –∫–≤–∞–∑–∏–∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã, –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏,
    –º–Ω–æ–≥–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤, –¥–∏—Å–±–∞–ª–∞–Ω—Å, –≤—ã—Å–æ–∫–∞—è –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å, ID-–∫–æ–ª–æ–Ω–∫–∏.
    eda –º—ã —Å–µ–π—á–∞—Å –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º, –Ω–æ –ø–µ—Ä–µ–¥–∞—ë–º –¥–ª—è –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–∏—è —Å app.py
    """
    problems: dict[str, object] = {}

    # 0) –¥–µ—Ç–µ–∫—Ç ID / –∫–ª—é—á–µ–π
    id_like_cols: list[str] = []
    n_rows = len(df)
    for col in df.columns:
        col_l = col.lower()
        nunique = df[col].nunique(dropna=True)

        name_looks_like_id = (
            col_l == "id"
            or col_l.endswith("_id")
            or col_l in ("index", "idx", "rk", "rank")
        )
        value_looks_like_id = nunique > 0.9 * n_rows  # –ø–æ—á—Ç–∏ —É–Ω–∏–∫–∞–ª—å–Ω–æ

        if name_looks_like_id or value_looks_like_id:
            id_like_cols.append(col)

    if id_like_cols:
        problems["id_like"] = id_like_cols

    # 1) –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã –∏ –ø–æ—á—Ç–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã
    constant_cols = []
    quasi_constant_cols = []
    for col in df.columns:
        nunique = df[col].nunique(dropna=True)
        if nunique <= 1:
            constant_cols.append(col)
        else:
            top_frac = float(df[col].value_counts(normalize=True, dropna=False).iloc[0])
            if top_frac > 0.98:
                quasi_constant_cols.append(col)
    if constant_cols:
        problems["constant_features"] = constant_cols
    if quasi_constant_cols:
        problems["quasi_constant_features"] = quasi_constant_cols

    # 2) –≤—ã—Å–æ–∫–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ø–æ —á–∏—Å–ª–æ–≤—ã–º
    num_df = df.select_dtypes(include=["number"])
    high_corr_pairs = []
    if num_df.shape[1] >= 2:
        corr = num_df.corr().abs()
        cols = corr.columns.tolist()
        for i in range(len(cols)):
            for j in range(i + 1, len(cols)):
                if corr.iloc[i, j] >= 0.9:
                    high_corr_pairs.append((cols[i], cols[j], float(corr.iloc[i, j])))
    if high_corr_pairs:
        problems["high_corr_pairs"] = high_corr_pairs

    # 3) –ø—Ä–æ–ø—É—Å–∫–∏ (–≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö)
    null_perc = (df.isna().sum() / max(1, n_rows) * 100).sort_values(ascending=False)
    high_nulls = null_perc[null_perc > 30].to_dict()
    if high_nulls:
        problems["high_null_features"] = {k: float(v) for k, v in high_nulls.items()}

    # 4) NaN –≤ —Ç–∞—Ä–≥–µ—Ç–µ
    target = task.get("target")
    if target and target in df.columns:
        n_nan_target = int(df[target].isna().sum())
        if n_nan_target > 0:
            problems["target_has_nan"] = {"column": target, "nan_count": n_nan_target}

    # 5) –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤
    if task.get("task") == "classification" and target and target in df.columns:
        vc = df[target].value_counts(dropna=False)
        if len(vc) >= 2:
            max_c = int(vc.iloc[0])
            min_c = int(vc.iloc[-1])
            ratio = max_c / max(1, min_c)
            if ratio >= 5:
                problems["class_imbalance"] = {
                    "max_class": vc.index[0],
                    "max_count": max_c,
                    "min_class": vc.index[-1],
                    "min_count": min_c,
                    "ratio": float(ratio),
                }

    # 6) –æ—á–µ–Ω—å –º–Ω–æ–≥–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    high_cardinality = []
    for col in df.select_dtypes(include=["object"]).columns:
        nunique = df[col].nunique(dropna=True)
        if nunique > 200 and col not in problems.get("id_like", []):
            high_cardinality.append({"column": col, "n_unique": int(nunique)})
    if high_cardinality:
        problems["high_cardinality"] = high_cardinality

    return problems



# ---------------------------------------------------------------------
# 7. —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ (–ø–æ–¥ —Å–∏–≥–Ω–∞—Ç—É—Ä—É –∏–∑ app.py)
# ---------------------------------------------------------------------
def build_recommendations(
    df: pd.DataFrame,
    eda: dict,
    task: dict,
    problems: dict,
    model: dict | None,
) -> list[str]:
    recs: list[str] = []

    id_like = set(problems.get("id_like", []))

    consts = [c for c in (problems.get("constant_features") or []) if c not in id_like]
    if consts:
        recs.append(
            f"–ï—Å—Ç—å –ø–æ–ª–Ω–æ—Å—Ç—å—é –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {', '.join(consts[:8])} ‚Äî –º–æ–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å –ø–µ—Ä–µ–¥ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ–º."
        )

    quasi = [c for c in (problems.get("quasi_constant_features") or []) if c not in id_like]
    if quasi:
        recs.append(
            f"–ï—Å—Ç—å –ø–æ—á—Ç–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {', '.join(quasi[:8])} ‚Äî —Å—Ç–æ–∏—Ç –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∏—Ö –ø–æ–ª—å–∑—É."
        )

    corr_pairs = problems.get("high_corr_pairs") or []
    if corr_pairs:
        short = []
        for a, b, corr in corr_pairs[:6]:
            if a in id_like and b in id_like:
                continue
            short.append(f"{a}‚Üî{b} ({corr:.2f})")
        if short:
            recs.append(
                "–ï—Å—Ç—å —Å–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—â–∏–µ –ø–∞—Ä—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: "
                + ", ".join(short)
                + " ‚Äî –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –æ—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–ª–∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é."
            )

    high_nulls = problems.get("high_null_features") or {}
    if high_nulls:
        show = [f"{k} ({v:.1f}%)" for k, v in list(high_nulls.items())[:6] if k not in id_like]
        if show:
            recs.append(
                "–ï—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –±–æ–ª—å—à–∏–º —á–∏—Å–ª–æ–º –ø—Ä–æ–ø—É—Å–∫–æ–≤: "
                + ", ".join(show)
                + " ‚Äî –∑–∞–ø–æ–ª–Ω–∏/—É–¥–∞–ª–∏/—Å–¥–µ–ª–∞–π –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–ª–∞–≥."
            )

    if problems.get("target_has_nan"):
        info = problems["target_has_nan"]
        recs.append(
            f"–í —Ü–µ–ª–µ–≤–æ–π –∫–æ–ª–æ–Ω–∫–µ {info['column']} –µ—Å—Ç—å –ø—Ä–æ–ø—É—Å–∫–∏ ({info['nan_count']}) ‚Äî –Ω—É–∂–Ω–æ —É–±—Ä–∞—Ç—å –∏—Ö –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º."
        )

    if problems.get("class_imbalance"):
        ci = problems["class_imbalance"]
        recs.append(
            f"–ù–∞–π–¥–µ–Ω –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ ({ci['max_class']}:{ci['min_class']} ‚âà {ci['ratio']:.1f}). "
            "–ò—Å–ø–æ–ª—å–∑—É–π class_weight='balanced', stratify –ø—Ä–∏ train_test_split –∏–ª–∏ oversampling."
        )

    high_card = problems.get("high_cardinality") or []
    if high_card:
        cols = [f"{x['column']} ({x['n_unique']})" for x in high_card[:4] if x["column"] not in id_like]
        if cols:
            recs.append(
                "–ï—Å—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –±–æ–ª—å—à–∏–º —á–∏—Å–ª–æ–º –∑–Ω–∞—á–µ–Ω–∏–π: "
                + ", ".join(cols)
                + " ‚Äî –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å CatBoost/target encoding/—á–∞—Å—Ç–æ—Ç–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ."
            )

    if task.get("task") == "eda":
        recs.append("–¶–µ–ª–µ–≤–æ–π –ø—Ä–∏–∑–Ω–∞–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω ‚Äî –º–æ–∂–Ω–æ —è–≤–Ω–æ —É–∫–∞–∑–∞—Ç—å target –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ.")
    elif task.get("task") == "regression":
        recs.append("–î–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –º–æ–∂–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –±–æ–ª–µ–µ —Å–∏–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ (CatBoostRegressor, LightGBM).")
    elif task.get("task") == "classification":
        recs.append("–î–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–º–µ–µ—Ç —Å–º—ã—Å–ª –ø–æ—Å—á–∏—Ç–∞—Ç—å ROC-AUC –∏ PR-AUC, –æ—Å–æ–±–µ–Ω–Ω–æ –ø—Ä–∏ –¥–∏—Å–±–∞–ª–∞–Ω—Å–µ.")

    if model is None:
        recs.append("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–∞–ª–∞—Å—å ‚Äî —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ, –Ω–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ target –∏–ª–∏ –¥–∞–Ω–Ω—ã—Ö —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ.")
    else:
        if model.get("model_type") == "RandomForestClassifier":
            recs.append("–¢–µ–∫—É—â–∞—è –º–æ–¥–µ–ª—å ‚Äî RandomForestClassifier. –ú–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å –±—É—Å—Ç–∏–Ω–≥–æ–º –∏ –ø–æ–¥–±–æ—Ä–æ–º –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.")
        if model.get("model_type") == "RandomForestRegressor":
            recs.append("–¢–µ–∫—É—â–∞—è –º–æ–¥–µ–ª—å ‚Äî RandomForestRegressor. –ú–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å CatBoost/LightGBM.")

    return recs


# ---------------------------------------------------------------------
# 8. –≥—Ä–∞—Ñ–∏–∫–∏ ‚Üí base64
# ---------------------------------------------------------------------
def make_plots_base64(df: pd.DataFrame) -> list[dict]:
    plots: list[dict] = []

    num_cols = df.select_dtypes(include=["number"]).columns.tolist()[:3]
    for col in num_cols:
        fig, ax = plt.subplots(figsize=(4, 3))
        ax.hist(df[col].dropna(), bins=30)
        ax.set_title(f"Distribution of {col}")
        buf = BytesIO()
        plt.tight_layout()
        fig.savefig(buf, format="png")
        plt.close(fig)
        buf.seek(0)
        b64 = base64.b64encode(buf.read()).decode("utf-8")
        plots.append({"name": f"hist_{col}", "image_base64": b64})

    if len(df.select_dtypes(include=["number"]).columns) >= 2:
        corr = df.select_dtypes(include=["number"]).corr()
        fig, ax = plt.subplots(figsize=(4, 3))
        cax = ax.imshow(corr, cmap="viridis")
        ax.set_xticks(range(len(corr.columns)))
        ax.set_yticks(range(len(corr.columns)))
        ax.set_xticklabels(corr.columns, rotation=90, fontsize=6)
        ax.set_yticklabels(corr.columns, fontsize=6)
        fig.colorbar(cax)
        plt.tight_layout()
        buf = BytesIO()
        fig.savefig(buf, format="png")
        plt.close(fig)
        buf.seek(0)
        b64 = base64.b64encode(buf.read()).decode("utf-8")
        plots.append({"name": "correlation", "image_base64": b64})

    return plots


# ---------------------------------------------------------------------
# 9. —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–∞ –¥–∏—Å–∫ (–ø–æ –∂–µ–ª–∞–Ω–∏—é)
# ---------------------------------------------------------------------
def save_run(run_data: dict, model_pipeline) -> str:
    run_id = str(uuid.uuid4())
    run_dir = os.path.join("runs", run_id)
    os.makedirs(run_dir, exist_ok=True)

    with open(os.path.join(run_dir, "report.json"), "w", encoding="utf-8") as f:
        json.dump(run_data, f, ensure_ascii=False, indent=2)

    if model_pipeline is not None:
        joblib.dump(model_pipeline, os.path.join(run_dir, "model.joblib"))

    return run_id
