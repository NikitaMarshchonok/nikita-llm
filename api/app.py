# api/app.py
from __future__ import annotations

import os
from io import BytesIO
from uuid import uuid4
from typing import List, Dict, Any, Optional

import numpy as np
import pandas as pd
from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from fastapi.encoders import jsonable_encoder
from fastapi.responses import JSONResponse, FileResponse, PlainTextResponse
from fastapi.middleware.cors import CORSMiddleware

from agent.tools import (
    basic_eda,
    detect_task,
    train_baseline,
    build_report,
    make_plots_base64,
    analyze_dataset,
    build_recommendations,
    evaluate_dataset_health,
    build_analysis_status,
    build_next_actions,
    summarize_target,
    rank_problems,
    auto_feature_suggestions,
    extract_feature_importance,
    build_code_hints,
    detect_column_roles,
    build_experiment_plan,
    auto_model_search,
    suggest_targets,
    apply_auto_fixes_for_inference,
)

# ---------------------------
# App init + CORS
# ---------------------------
app = FastAPI(
    title="Nikita DS Agent",
    description="–ó–∞–≥—Ä—É–∑–∏ CSV/Excel/JSON/Parquet ‚Üí –ø–æ–ª—É—á–∏ EDA, –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å –∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å",
    version="0.3.0",
)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# JSON-safe –¥–∞–Ω–Ω—ã–µ –æ—Ç–¥–µ–ª—å–Ω–æ –æ—Ç –Ω–µ—Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ (pipelines)
RUNS: Dict[str, Dict[str, Any]] = {}
PIPELINES: Dict[str, Any] = {}

# –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞/–æ–±—É—á–µ–Ω–∏—è
MAX_ROWS: int = int(os.getenv("DS_AGENT_MAX_ROWS", "25000"))


# ---------------------------
# Utils: —á—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤
# ---------------------------
def read_csv_safely(file_bytes: bytes) -> pd.DataFrame:
    bio = BytesIO(file_bytes)
    variants = [
        {},
        {"sep": ";"},
        {"encoding": "utf-8-sig"},
        {"encoding": "cp1251"},
        {"sep": ";", "encoding": "cp1251"},
        {"encoding": "latin-1"},
        {"sep": ";", "encoding": "latin-1"},
    ]
    for kwargs in variants:
        try:
            bio.seek(0)
            df = pd.read_csv(bio, on_bad_lines="skip", **kwargs)
            if df.shape[1] > 0:
                return df
        except Exception:
            continue
    raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å CSV –Ω–∏ —Å –æ–¥–Ω–æ–π –∫–æ–º–±–∏–Ω–∞—Ü–∏–µ–π –∫–æ–¥–∏—Ä–æ–≤–∫–∞/—Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å")


def read_any_table(file_bytes: bytes, filename: str | None) -> pd.DataFrame:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∑–∞–≥—Ä—É–∑—á–∏–∫ —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:
    CSV / TXT / Excel / JSON / Parquet.
    –ü—Ä–∏ –ª—é–±–æ–π –æ—à–∏–±–∫–µ –æ—Ç–∫–∞—Ç—ã–≤–∞–µ–º—Å—è –∫ read_csv_safely.
    """
    ext = (os.path.splitext(filename or "")[1] or "").lower()
    bio = BytesIO(file_bytes)

    # 1) CSV / TXT
    if ext in [".csv", ".txt"]:
        return read_csv_safely(file_bytes)

    # 2) Excel
    if ext in [".xlsx", ".xls"]:
        try:
            bio.seek(0)
            df = pd.read_excel(bio)
            if df.shape[1] > 0:
                return df
        except Exception:
            pass  # —É–ø–∞–¥—ë–º –≤ fallback –Ω–∏–∂–µ

    # 3) Parquet
    if ext in [".parquet", ".pq"]:
        try:
            bio.seek(0)
            df = pd.read_parquet(bio)
            if df.shape[1] > 0:
                return df
        except Exception:
            pass

    # 4) JSON / JSONL
    if ext in [".json"]:
        try:
            bio.seek(0)
            # —Å–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è jsonl
            df = pd.read_json(bio, lines=True)
            if df.shape[1] > 0:
                return df
        except Exception:
            try:
                bio.seek(0)
                df = pd.read_json(bio)
                if df.shape[1] > 0:
                    return df
            except Exception:
                pass

    # 5) fallback ‚Äî –∫–∞–∫ CSV
    return read_csv_safely(file_bytes)


def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df.columns = [
        col.strip()
        .replace(" ", "_")
        .replace(".", "_")
        .replace("-", "_")
        .replace("/", "_")
        for col in df.columns
    ]
    return df


def align_features_for_inference(
    df: pd.DataFrame, train_cols: List[str], target: Optional[str]
) -> pd.DataFrame:
    """–í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ñ–∏—á–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –ø–æ–¥ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ —Ñ–∏—á–∏."""
    df = df.copy()
    feat_cols = [c for c in train_cols if (target is None or c != target)]
    if target and target in df.columns:
        df = df.drop(columns=[target])
    for c in feat_cols:
        if c not in df.columns:
            df[c] = pd.NA
    return df[feat_cols]


# ---------------------------
# –°–±–æ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è LLM
# ---------------------------
def build_llm_context(run: Dict[str, Any]) -> str:
    """
    –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ –æ–¥–Ω–æ–º—É run –¥–ª—è LLM:
    –∑–∞–¥–∞—á–∞, –∑–¥–æ—Ä–æ–≤—å–µ –¥–∞—Ç–∞—Å–µ—Ç–∞, –º–µ—Ç—Ä–∏–∫–∏, –≤–∞–∂–Ω—ã–µ —Ñ–∏—á–∏, –ø—Ä–æ–±–ª–µ–º—ã,
    —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏, next actions –∏ –ø–ª–∞–Ω —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤.
    """
    parts: list[str] = []

    task = run.get("task") or {}
    eda = run.get("eda") or {}
    dataset_health = run.get("dataset_health") or {}
    target_summary = run.get("target_summary") or {}
    model = run.get("model") or {}
    feature_importance = run.get("feature_importance") or []
    problem_list = run.get("problem_list") or []
    recommendations = run.get("recommendations") or []
    next_actions = run.get("next_actions") or []
    experiment_plan = run.get("experiment_plan") or []

    # –ó–∞–¥–∞—á–∞
    parts.append("=== –ó–∞–¥–∞—á–∞ –∏ —Ç–∞—Ä–≥–µ—Ç ===")
    parts.append(f"–¢–∏–ø –∑–∞–¥–∞—á–∏: {task.get('task')}")
    parts.append(f"–¢–∞—Ä–≥–µ—Ç: {task.get('target')}")
    parts.append("")

    # –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö
    shape = eda.get("shape")
    if shape and len(shape) == 2:
        parts.append("=== –î–∞—Ç–∞—Å–µ—Ç ===")
        parts.append(f"–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {shape[0]} —Å—Ç—Ä–æ–∫ √ó {shape[1]} –∫–æ–ª–æ–Ω–æ–∫.")
        parts.append("")

    # –ó–¥–æ—Ä–æ–≤—å–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
    if dataset_health:
        parts.append("=== –ó–¥–æ—Ä–æ–≤—å–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ ===")
        parts.append(
            f"–û—Ü–µ–Ω–∫–∞: {dataset_health.get('score')} / —É—Ä–æ–≤–µ–Ω—å: {dataset_health.get('level')}"
        )
        reasons = dataset_health.get("reasons") or []
        if reasons:
            parts.append("–ü—Ä–æ–±–ª–µ–º—ã: " + "; ".join(reasons))
        parts.append("")

    # –°–≤–æ–¥–∫–∞ –ø–æ —Ç–∞—Ä–≥–µ—Ç—É
    if target_summary and (target_summary.get("target") or target_summary.get("has_target")):
        parts.append("=== –¢–∞—Ä–≥–µ—Ç ===")
        parts.append(f"–ò–º—è —Ç–∞—Ä–≥–µ—Ç–∞: {target_summary.get('target')}")
        parts.append(f"–°—Ç—Ä–æ–∫: {target_summary.get('n_rows')}, –ø—Ä–æ–ø—É—Å–∫–æ–≤: {target_summary.get('n_missing')}")
        if target_summary.get("task") == "classification":
            parts.append(f"–ß–∏—Å–ª–æ –∫–ª–∞—Å—Å–æ–≤: {target_summary.get('n_classes')}")
            top_classes = target_summary.get("top_classes") or []
            short = []
            for cls in top_classes[:5]:
                label = cls.get("label")
                share = cls.get("share")
                short.append(f"{label} (~{share:.2f})")
            if short:
                parts.append("–¢–æ–ø –∫–ª–∞—Å—Å—ã: " + ", ".join(short))
        elif target_summary.get("task") == "regression":
            parts.append(
                "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–∞—Ä–≥–µ—Ç–∞: "
                f"min={target_summary.get('min')}, "
                f"max={target_summary.get('max')}, "
                f"mean={target_summary.get('mean')}, "
                f"std={target_summary.get('std')}"
            )
        parts.append("")

    # –ú–æ–¥–µ–ª—å –∏ –º–µ—Ç—Ä–∏–∫–∏
    parts.append("=== –ú–æ–¥–µ–ª—å ===")
    if not model:
        parts.append("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–∞–ª–∞—Å—å –∏–ª–∏ –Ω–µ –≤–µ—Ä–Ω—É–ª–∞—Å—å.")
    elif model.get("model_type") == "skipped":
        parts.append(f"–ú–æ–¥–µ–ª—å –ø—Ä–æ–ø—É—â–µ–Ω–∞: {model.get('reason')}")
    else:
        parts.append(f"–¢–∏–ø –º–æ–¥–µ–ª–∏: {model.get('model_type')}")
        if "accuracy" in model:
            parts.append(f"accuracy = {model['accuracy']:.4f}")
        if "f1" in model:
            parts.append(f"f1 = {model['f1']:.4f}")
        if "roc_auc" in model:
            parts.append(f"roc_auc = {model['roc_auc']:.4f}")
        if "rmse" in model:
            parts.append(f"rmse = {model['rmse']:.4f}")
    parts.append("")

    # –¢–æ–ø –≤–∞–∂–Ω—ã–µ —Ñ–∏—á–∏
    if feature_importance:
        parts.append("=== –¢–æ–ø –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ===")
        top_feats = feature_importance[:5]
        for fi in top_feats:
            parts.append(f"- {fi.get('feature')} (importance={fi.get('importance'):.3f})")
        parts.append("")

    # –ü—Ä–æ–±–ª–µ–º—ã (—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ)
    if problem_list:
        parts.append("=== –ö–ª—é—á–µ–≤—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –≤ –¥–∞–Ω–Ω—ã—Ö ===")
        for p in problem_list[:6]:
            msg = p.get("message") or p.get("code") or p.get("key")
            sev = p.get("severity") or p.get("level")
            parts.append(f"- [{sev}] {msg}")
        parts.append("")

    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    if recommendations:
        parts.append("=== –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ ===")
        for r in recommendations[:6]:
            parts.append(f"- {r}")
        parts.append("")

    # –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏
    if next_actions:
        parts.append("=== –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏ (–∫—Ä–∞—Ç–∫–æ) ===")
        for a in next_actions[:6]:
            parts.append(f"- {a}")
        parts.append("")

    # –ü–ª–∞–Ω —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
    if experiment_plan:
        parts.append("=== –ü–ª–∞–Ω —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ (now / next / later) ===")
        for step in experiment_plan[:10]:
            parts.append(
                f"- [{step.get('priority')}] "
                f"{step.get('title')}: {step.get('description')}"
            )
        parts.append("")

    return "\n".join(parts)


# ---------------------------
# LLM: —Ä–µ–∞–ª—å–Ω—ã–π –≤—ã–∑–æ–≤ + fallback
# ---------------------------
def call_llm(prompt: str) -> str:
    """
    –í—ã–∑–æ–≤ LLM —á–µ—Ä–µ–∑ OpenAI, —Å –±–µ–∑–æ–ø–∞—Å–Ω—ã–º fallback, –µ—Å–ª–∏:
    - –Ω–µ—Ç OPENAI_API_KEY,
    - –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞,
    - –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ.
    """
    def _fallback(reason: str) -> str:
        question_marker = "=== –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è ==="
        if question_marker in prompt:
            user_question = prompt.split(question_marker, 1)[1].strip()
        else:
            user_question = prompt.strip()

        return (
            f"‚ö†Ô∏è LLM –ø–æ–∫–∞ –Ω–µ –ø–æ–¥–∫–ª—é—á—ë–Ω –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ({reason}).\n\n"
            "–ù–æ –≤–æ—Ç –∫–∞–∫ —è –±—ã –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞–ª —Å–∏—Ç—É–∞—Ü–∏—é –∫–∞–∫ DS-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç:\n\n"
            f"–í–æ–ø—Ä–æ—Å: {user_question}\n\n"
            "‚Ä¢ –£ —Ç–µ–±—è —É–∂–µ –µ—Å—Ç—å EDA, –±–µ–π–∑–ª–∞–π–Ω-–º–æ–¥–µ–ª—å, —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–±–ª–µ–º –∏ –ø–ª–∞–Ω —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤.\n"
            "‚Ä¢ –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥ ‚Äî –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –∑–∞–∫—Ä—ã–≤–∞—Ç—å –ø—É–Ω–∫—Ç—ã –∏–∑ –±–ª–æ–∫–æ–≤ "
            "¬´–ß—Ç–æ —Å–¥–µ–ª–∞—Ç—å –¥–∞–ª—å—à–µ¬ª, ¬´–°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏¬ª –∏ –∏–∑ —Å–µ–∫—Ü–∏–∏ now/next –≤ –ø–ª–∞–Ω–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤.\n"
        )

    api_key = os.getenv("OPENAI_API_KEY")
    model_name = os.getenv("OPENAI_MODEL", "gpt-4.1-mini")

    if not api_key:
        return _fallback("–Ω–µ—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è OPENAI_API_KEY")

    try:
        from openai import OpenAI  # type: ignore
    except Exception:
        return _fallback("–±–∏–±–ª–∏–æ—Ç–µ–∫–∞ openai –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ (pip install -U openai)")

    try:
        client = OpenAI(api_key=api_key)
        resp = client.chat.completions.create(
            model=model_name,
            messages=[
                {
                    "role": "system",
                    "content": (
                        "–¢—ã –æ–ø—ã—Ç–Ω—ã–π Data Science –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —É—Ä–æ–≤–Ω—è strong middle/senior. "
                        "–û—Ç–≤–µ—á–∞–π –ø–æ-–¥–µ–ª—É, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–æ, –±–µ–∑ –≤–æ–¥—ã. "
                        "–î–∞–≤–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —à–∞–≥–∏ –ø–æ —Ä–∞–±–æ—Ç–µ —Å –¥–∞–Ω–Ω—ã–º–∏, —Ñ–∏—á–∞–º–∏, –º–æ–¥–µ–ª—è–º–∏ –∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏."
                    ),
                },
                {"role": "user", "content": prompt},
            ],
            temperature=0.2,
        )
        content = resp.choices[0].message.content
        if not content:
            return _fallback("–ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏")
        return content
    except Exception as e:
        return _fallback(f"–æ—à–∏–±–∫–∞ –≤—ã–∑–æ–≤–∞ API: {e}")


# ---------------------------
# Static / health
# ---------------------------
@app.get("/ui")
def ui():
    return FileResponse(os.path.join("api", "static", "frontend.html"))


@app.get("/")
def root():
    return {"msg": "Nikita DS Agent is running"}


# ---------------------------
# Upload & Train
# ---------------------------
@app.post("/upload")
async def upload_dataset(
    file: UploadFile = File(...),
    target: str | None = Form(default=None),
):
    try:
        contents = await file.read()

        # 1) —á–∏—Ç–∞–µ–º –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º
        df = read_any_table(contents, file.filename)
        df = normalize_columns(df)

        # 1.1) –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–æ —Ä–∞–∑–º–µ—Ä—É –¥–∞—Ç–∞—Å–µ—Ç–∞
        original_rows = int(len(df))
        sampling_info: Dict[str, Any] = {
            "applied": False,
            "original_rows": original_rows,
            "used_rows": original_rows,
        }
        if original_rows > MAX_ROWS:
            df = df.sample(n=MAX_ROWS, random_state=42).reset_index(drop=True)
            sampling_info.update(
                {
                    "applied": True,
                    "used_rows": MAX_ROWS,
                    "strategy": "random_sample",
                    "random_state": 42,
                    "note": (
                        f"–î–∞—Ç–∞—Å–µ—Ç –±—ã–ª —Å–ª—É—á–∞–π–Ω–æ –ø–æ–¥—Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω –¥–æ {MAX_ROWS} —Å—Ç—Ä–æ–∫ "
                        "–¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞ –∏ –æ–±—É—á–µ–Ω–∏—è."
                    ),
                }
            )

        # 2) –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º target; –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É —Å—á–∏—Ç–∞–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–º
        if target is not None:
            target = target.strip()
            if target == "":
                target = None
            else:
                target = (
                    target.replace(" ", "_")
                    .replace(".", "_")
                    .replace("-", "_")
                    .replace("/", "_")
                )

        # 3) EDA (–ø–æ –ø–æ–¥—Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É df)
        eda = basic_eda(df)
        # –¥–æ–±–∞–≤–∏–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø–æ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—é –≤ EDA
        eda["original_rows"] = original_rows
        eda["used_rows"] = sampling_info["used_rows"]
        eda["sampling_applied"] = sampling_info["applied"]

        # 4) –∑–∞–¥–∞—á–∞
        task = detect_task(df, target=target)

        # 5) –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
        problems = analyze_dataset(df, task)

        # 5.0) –∫–∞–Ω–¥–∏–¥–∞—Ç—ã —Ç–∞—Ä–≥–µ—Ç–∞ (–Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞)
        target_suggestions = suggest_targets(
            df,
            problems=problems,
            current_target=task.get("target"),
        )

        # 5.1) –∫–æ—Ä–æ—Ç–∫–∞—è —Å–≤–æ–¥–∫–∞ –ø–æ —Ç–∞—Ä–≥–µ—Ç—É
        target_summary = summarize_target(df, task)

        # 5.1.1) —Ä–æ–ª–∏ –∫–æ–ª–æ–Ω–æ–∫ (id / datetime / text / ...)
        column_roles = detect_column_roles(df, task)

        # 5.2) —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–±–ª–µ–º (high/medium/low)
        problem_list = rank_problems(problems)

        # 5.3) –∑–¥–æ—Ä–æ–≤—å–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
        dataset_health = evaluate_dataset_health(eda, problems)

        # 5.4) –∏–¥–µ–∏ —Ñ–∏—á
        feature_suggestions = auto_feature_suggestions(df)

        # 6) –∞–≤—Ç–æ-–ø–æ–∏—Å–∫ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ + –∞–≤—Ç–æ-—Ñ–∏–∫—Å—ã
        model_res: Optional[Dict[str, Any]] = None
        pipeline = None
        model_leaderboard: Optional[list] = None
        feature_importance: List[Dict[str, Any]] = []
        auto_fixes: Optional[Dict[str, Any]] = None
        train_columns_for_alignment: List[str] = list(df.columns)

        if task["task"] != "eda" and task.get("target"):
            # 6.1 auto_model_search
            try:
                auto_res = auto_model_search(df, task, problems)
            except Exception:
                auto_res = None

            if auto_res is not None:
                model_res = auto_res.get("best_model")
                pipeline = auto_res.get("pipeline")
                model_leaderboard = auto_res.get("leaderboard")

                # –¥–æ—Å—Ç–∞—ë–º auto_fixes –∏–∑ training_log
                training_log = (model_res or {}).get("training_log") or {}
                af = training_log.get("auto_fixes")
                if isinstance(af, dict):
                    auto_fixes = af

            # 6.2 fallback: train_baseline, –µ—Å–ª–∏ auto_model_search –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª
            if model_res is None or model_res.get("model_type") == "skipped":
                baseline = train_baseline(
                    df,
                    task["target"],
                    task["task"],
                    problems=problems,
                    return_model=True,
                )
                if baseline:
                    if "pipeline" in baseline:
                        pipeline = baseline.pop("pipeline")
                    model_res = baseline

                    training_log = (model_res or {}).get("training_log") or {}
                    af = training_log.get("auto_fixes")
                    if isinstance(af, dict):
                        auto_fixes = af

        # 6.3) –≤—ã—á–∏—Å–ª—è–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ —Å —É—á—ë—Ç–æ–º –∞–≤—Ç–æ-—Ñ–∏–∫—Å–æ–≤
        try:
            if auto_fixes:
                df_train_align = apply_auto_fixes_for_inference(df, auto_fixes)
                train_columns_for_alignment = list(df_train_align.columns)
            else:
                train_columns_for_alignment = list(df.columns)
        except Exception:
            train_columns_for_alignment = list(df.columns)

        # 6.4) feature importance, –µ—Å–ª–∏ —Å–º–æ–≥–ª–∏ –æ–±—É—á–∏—Ç—å pipeline
        if pipeline is not None:
            try:
                feature_importance = extract_feature_importance(pipeline)
            except Exception:
                feature_importance = []

        # 6.5) –∫–æ–¥-–ø–æ–¥—Å–∫–∞–∑–∫–∏ –ø–æ–¥ –ø—Ä–æ–±–ª–µ–º—ã
        try:
            code_hints = build_code_hints(problems, task)
        except Exception:
            code_hints = []

        # 7) —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á—ë—Ç
        report_text = build_report(df, eda, task, model_res, problems)

        # 8) –≥—Ä–∞—Ñ–∏–∫–∏
        plots = make_plots_base64(df)

        # 9) —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        recs = build_recommendations(df, eda, task, problems, model_res)

        # 10) —Å—Ç–∞—Ç—É—Å + next actions
        status = build_analysis_status(task, problems, model_res)
        next_actions = build_next_actions(task, problems, model_res)

        # 11) –ø–ª–∞–Ω —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ (now/next/later)
        experiment_plan = build_experiment_plan(
            task=task,
            problems=problems,
            dataset_health=dataset_health,
            model=model_res,
            column_roles=column_roles,
        )

        # 12) —Å–æ—Ö—Ä–∞–Ω—è–µ–º run
        run_id = f"run_{uuid4().hex[:8]}"
        RUNS[run_id] = {
            "run_id": run_id,
            "filename": file.filename,
            "eda": eda,
            "task": task,
            "problems": problems,
            "problem_list": problem_list,
            "target_summary": target_summary,
            "dataset_health": dataset_health,
            "column_roles": column_roles,
            "model": model_res,
            "model_leaderboard": model_leaderboard,
            "report": report_text,
            "plots": plots,
            "recommendations": recs,
            "status": status,
            "next_actions": next_actions,
            "feature_suggestions": feature_suggestions,
            "feature_importance": feature_importance,
            "code_hints": code_hints,
            "experiment_plan": experiment_plan,
            "target_suggestions": target_suggestions,
            "columns": train_columns_for_alignment,  # üëà –∫–æ–ª–æ–Ω–∫–∏ –ø–æ—Å–ª–µ –∞–≤—Ç–æ-—Ñ–∏–∫—Å–æ–≤
            "sampling": sampling_info,
            "auto_fixes": auto_fixes,               # üëà —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∞–≤—Ç–æ-—Ñ–∏–∫—Å—ã
        }
        if pipeline is not None:
            PIPELINES[run_id] = pipeline

        payload_keys = [
            "run_id",
            "filename",
            "eda",
            "task",
            "problems",
            "problem_list",
            "target_summary",
            "dataset_health",
            "model",
            "model_leaderboard",
            "report",
            "plots",
            "recommendations",
            "status",
            "next_actions",
            "experiment_plan",
            "feature_suggestions",
            "feature_importance",
            "code_hints",
            "target_suggestions",
            "sampling",
        ]
        payload = {key: RUNS[run_id].get(key) for key in payload_keys}
        return JSONResponse(content=jsonable_encoder(payload))

    except ValueError as e:
        raise HTTPException(
            status_code=400,
            detail={
                "error": "failed_to_process_file",
                "details": str(e),
                "hint": "–ø—Ä–æ–≤–µ—Ä—å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å (',' –∏–ª–∏ ';'), –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫ –∏ target",
            },
        )
    except Exception as e:
        raise HTTPException(
            status_code=400,
            detail={
                "error": "failed_to_process_file",
                "details": str(e),
                "hint": "–ø—Ä–æ–≤–µ—Ä—å —Ñ–∞–π–ª –∏ target",
            },
        )


# ---------------------------
# Inference
# ---------------------------
@app.post("/runs/{run_id}/predict")
async def predict_on_run(
    run_id: str,
    file: UploadFile = File(..., description="CSV/Excel/JSON/Parquet c –Ω–æ–≤—ã–º–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏ (–º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å target)"),
    include_proba: bool = Form(default=True),
    return_rows: int = Form(default=30),
):
    """–ü—Ä–æ–≥–æ–Ω—è–µ—Ç –Ω–æ–≤—ã–π —Ñ–∞–π–ª —á–µ—Ä–µ–∑ pipeline –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ run_id.
    –ï—Å–ª–∏ target –≤ —Ñ–∞–π–ª–µ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –≤–µ—Ä–Ω—ë–º –±—ã—Å—Ç—Ä—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ —ç—Ç–æ–º—É —Ñ–∞–π–ª—É.
    """
    if run_id not in RUNS:
        raise HTTPException(status_code=404, detail="run_id not found")
    if run_id not in PIPELINES:
        raise HTTPException(
            status_code=400,
            detail={"error": "no_pipeline", "details": "–î–ª—è —ç—Ç–æ–≥–æ run –Ω–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–≥–æ pipeline. –ü–µ—Ä–µ–æ–±—É—á–∏ /upload."},
        )

    try:
        contents = await file.read()
        df_new = read_any_table(contents, file.filename)
        df_new = normalize_columns(df_new)

        task = RUNS[run_id]["task"]
        train_cols: List[str] = RUNS[run_id]["columns"]
        auto_fixes: Optional[Dict[str, Any]] = RUNS[run_id].get("auto_fixes")
        target = task.get("target")
        task_type = task.get("task")

        # y_true (–µ—Å–ª–∏ –µ—Å—Ç—å –≤ —Ñ–∞–π–ª–µ)
        y_true = None
        if target and target in df_new.columns:
            y_true = df_new[target].copy()

        # –ø—Ä–∏–º–µ–Ω—è–µ–º —Ç–µ –∂–µ –∞–≤—Ç–æ-—Ñ–∏–∫—Å—ã, —á—Ç–æ –∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
        if auto_fixes:
            df_new_fixed = apply_auto_fixes_for_inference(df_new, auto_fixes)
        else:
            df_new_fixed = df_new

        # –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–º –∫–æ–ª–æ–Ω–∫–∞–º
        X_inf = align_features_for_inference(df_new_fixed, train_cols, target)
        pipe = PIPELINES[run_id]

        # –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        y_pred = pipe.predict(X_inf)
        y_pred_list = np.asarray(y_pred).tolist()

        proba_list = None
        if include_proba and hasattr(pipe, "predict_proba"):
            try:
                proba = pipe.predict_proba(X_inf)
                proba_list = np.max(proba, axis=1).tolist()
            except Exception:
                proba_list = None

        # –º–µ—Ç—Ä–∏–∫–∏, –µ—Å–ª–∏ –µ—Å—Ç—å y_true
        metrics_out = None
        if y_true is not None:
            try:
                from sklearn.metrics import accuracy_score, f1_score, r2_score, mean_squared_error, mean_absolute_error

                if task_type == "classification":
                    acc = float(accuracy_score(y_true, y_pred))
                    f1m = float(f1_score(y_true, y_pred, average="macro"))
                    metrics_out = {"accuracy": acc, "f1_macro": f1m}
                elif task_type == "regression":
                    rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))
                    mae = float(mean_absolute_error(y_true, y_pred))
                    r2 = float(r2_score(y_true, y_pred))
                    metrics_out = {"rmse": rmse, "mae": mae, "r2": r2}
            except Exception:
                metrics_out = None

        # –ø—Ä–µ–≤—å—é (–ø–µ—Ä–≤—ã–µ return_rows —Å—Ç—Ä–æ–∫)
        preview = pd.DataFrame({"prediction": y_pred_list})
        if proba_list is not None:
            preview["proba"] = proba_list
        preview_full = pd.concat(
            [df_new.reset_index(drop=True), preview],
            axis=1,
        ).head(max(1, return_rows))

        return JSONResponse(
            content=jsonable_encoder(
                {
                    "run_id": run_id,
                    "n_rows": int(len(X_inf)),
                    "task": task,
                    "metrics": metrics_out,
                    "predictions": y_pred_list[:1000],
                    "probabilities": proba_list[:1000] if proba_list is not None else None,
                    "preview": preview_full.to_dict(orient="records"),
                }
            )
        )

    except ValueError as e:
        raise HTTPException(status_code=400, detail={"error": "bad_file", "details": str(e)})
    except Exception as e:
        raise HTTPException(status_code=400, detail={"error": "predict_failed", "details": str(e)})

# ---------------------------
# Runs utils
# ---------------------------
@app.get("/runs/{run_id}")
def get_run(run_id: str):
    if run_id not in RUNS:
        raise HTTPException(status_code=404, detail="run_id not found")
    return RUNS[run_id]


@app.get("/runs")
def list_runs():
    items = []
    for run_id, data in RUNS.items():
        items.append(
            {
                "run_id": run_id,
                "filename": data.get("filename"),
                "task": data.get("task"),
                "has_model": data.get("model") is not None,
            }
        )
    return items


@app.get("/runs/{run_id}/report")
def get_run_report(run_id: str):
    if run_id not in RUNS:
        raise HTTPException(status_code=404, detail="run_id not found")
    return RUNS[run_id]


# ---------------------------
# LLM-—Å–ª–æ–π: /ask
# ---------------------------
@app.post("/ask")
async def ask_agent(
    run_id: str = Form(..., description="ID –∑–∞–ø—É—Å–∫–∞ (run_id), –ø–æ –∫–æ—Ç–æ—Ä–æ–º—É —Å–ø—Ä–∞—à–∏–≤–∞–µ–º"),
    question: str = Form(..., description="–í–æ–ø—Ä–æ—Å –∫ DS-–∞–≥–µ–Ω—Ç—É"),
):
    """
    LLM-—Å–ª–æ–π –ø–æ–≤–µ—Ä—Ö DS-–∞–≥–µ–Ω—Ç–∞:
    - –ø–æ run_id –≤—ã—Ç–∞—Å–∫–∏–≤–∞–µ–º –≤–µ—Å—å –∫–æ–Ω—Ç–µ–∫—Å—Ç (EDA, –º–æ–¥–µ–ª—å, –ø—Ä–æ–±–ª–µ–º—ã, —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏),
    - —Å–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç,
    - —Ñ–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ call_llm,
    - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç.
    """
    if run_id not in RUNS:
        raise HTTPException(status_code=404, detail="run_id not found")

    run = RUNS[run_id]
    context_text = build_llm_context(run)

    prompt = (
        "–¢—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π Data Science –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —É—Ä–æ–≤–Ω—è strong middle/senior.\n"
        "–£ —Ç–µ–±—è –µ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞, –±–µ–π–∑–ª–∞–π–Ω-–º–æ–¥–µ–ª—å, —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–±–ª–µ–º, "
        "–æ—Ü–µ–Ω–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ –ø–ª–∞–Ω —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤.\n\n"
        "=== –ö–æ–Ω—Ç–µ–∫—Å—Ç –∞–Ω–∞–ª–∏–∑–∞ ===\n"
        f"{context_text}\n\n"
        "=== –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è ===\n"
        f"{question}\n\n"
        "–û—Ç–≤–µ—Ç—å –ø–æ-—Ä—É—Å—Å–∫–∏ –∏–ª–∏ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º (–∫–∞–∫ —É–¥–æ–±–Ω–µ–µ), —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–æ –∏ –ø–æ —à–∞–≥–∞–º. "
        "–ë–µ–∑ –ª–∏—à–Ω–µ–π –≤–æ–¥—ã, –¥–∞–π 2‚Äì5 –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Å–ª–µ–¥—É—é—â–∏—Ö —à–∞–≥–æ–≤."
    )

    answer = call_llm(prompt)

    return {
        "run_id": run_id,
        "question": question,
        "answer": answer,
    }


# ---------------------------
# –≠–∫—Å–ø–æ—Ä—Ç –æ—Ç—á—ë—Ç–∞ –≤ Markdown
# ---------------------------
@app.get("/runs/{run_id}/download")
def download_run_markdown(run_id: str):
    if run_id not in RUNS:
        raise HTTPException(status_code=404, detail="run_id not found")

    data = RUNS[run_id]
    lines = []
    lines.append(f"# Run {run_id}")
    if data.get("filename"):
        lines.append(f"**–§–∞–π–ª:** {data['filename']}")
    if data.get("task"):
        t = data["task"]
        lines.append(f"**–ó–∞–¥–∞—á–∞:** {t.get('task')}  target: `{t.get('target')}`")
    lines.append("")

    if data.get("report"):
        lines.append("## –û—Ç—á—ë—Ç")
        lines.append("```")
        lines.append(data["report"])
        lines.append("```")
        lines.append("")

    if data.get("problems"):
        lines.append("## –ù–∞–π–¥–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã")
        for key, val in data["problems"].items():
            lines.append(f"- **{key}**: {val}")
        lines.append("")

    if data.get("recommendations"):
        lines.append("## –ß—Ç–æ —Å–¥–µ–ª–∞—Ç—å –¥–∞–ª—å—à–µ")
        for r in data["recommendations"]:
            lines.append(f"- {r}")
        lines.append("")

    md = "\n".join(lines)
    return PlainTextResponse(md)
