/**
 * LLM Training Configuration
 * Generated by create-llm for tiny template
 * 
 * This file controls all aspects of your LLM training.
 * Edit values to customize your training setup.
 */

module.exports = {
  // Model architecture configuration
  model: {
    type: 'gpt',              // Architecture type (gpt, bert, t5)
    size: 'tiny',              // Template size (tiny, small, base, custom)
    vocab_size: 10000,        // Vocabulary size
    max_length: 512,          // Maximum sequence length
    layers: 4,                // Number of transformer layers
    heads: 4,                 // Number of attention heads
    dim: 256,                 // Model dimension
    dropout: 0.2,             // Dropout rate
  },

  // Training hyperparameters
  training: {
    batch_size: 16,           // Training batch size
    learning_rate: 0.0006,      // Learning rate
    warmup_steps: 500,       // Warmup steps
    max_steps: 10000,        // Maximum training steps
    eval_interval: 500,      // Evaluation frequency (steps)
    save_interval: 2000,      // Checkpoint save frequency (steps)
    optimizer: 'adamw',       // Optimizer type (adamw, adam, sgd)
    weight_decay: 0.01,       // Weight decay for regularization
    gradient_clip: 1,        // Gradient clipping threshold
    mixed_precision: false,    // Use mixed precision (FP16) training
    gradient_accumulation_steps: 1, // Gradient accumulation steps
  },

  // Data configuration
  data: {
    train_path: 'data/raw/train.txt',  // Training data path
    val_path: 'data/raw/val.txt',      // Validation data path
    max_length: 512,          // Maximum sequence length
    stride: 256,              // Sliding window stride for data processing
    val_split: 0.1,           // Validation split ratio (if no val_path)
    shuffle: true,            // Shuffle training data
  },

  // Tokenizer configuration
  tokenizer: {
    type: 'bpe',              // Tokenizer type (bpe, wordpiece, unigram)
    vocab_size: 10000,        // Vocabulary size
    min_frequency: 2,         // Minimum token frequency
    special_tokens: ["<pad>","<unk>","<s>","</s>"], // Special tokens
  },

  // Checkpoint management
  checkpoints: {
    save_total_limit: 3,      // Maximum checkpoints to keep (older ones deleted)
    save_on_interrupt: true,  // Save checkpoint on Ctrl+C
    resume_from_checkpoint: null, // Path to checkpoint to resume from
  },

  // Logging configuration
  logging: {
    log_interval: 100,        // Logging frequency (steps)
    log_dir: 'logs',          // Log directory
    tensorboard: true,        // Enable TensorBoard logging
    wandb: false,             // Enable Weights & Biases logging
  },

  // Plugin configuration
  // Uncomment plugins you want to use
  plugins: [
    // 'wandb',               // Weights & Biases integration
    // 'synthex',             // SynthexAI synthetic data generation
    // 'huggingface',         // Hugging Face Hub integration
  ],

  // Deployment configuration
  deployment: {
    huggingface: {
      repo_name: null,        // Hugging Face repo name (e.g., 'username/model-name')
      private: false,         // Make repo private
    },
    replicate: {
      model_name: null,       // Replicate model name
    },
  },
};

/**
 * Configuration Tips:
 * 
 * Template: TINY
 * Parameters: 5M
 * Hardware: None (CPU-friendly)
 * Training Time: 10-30 minutes
 * 
 * Training Tips:
 * - Good balance between size and capability
 * - Requires 1,000+ examples to avoid overfitting
 * - Can handle small to medium datasets (1-50MB)
 * - Use for prototyping before scaling to SMALL
 * - Watch for overfitting: if perplexity < 1.5, add more data
 * - Recommended: 5,000+ training examples for best results
 * 
 * Common Adjustments:
 * - Reduce batch_size if running out of memory
 * - Increase gradient_accumulation_steps to simulate larger batches
 * - Adjust learning_rate if loss is unstable
 * - Increase max_steps for better model quality
 * - Enable mixed_precision to reduce memory usage
 * 
 * For more information, see the README.md file.
 */